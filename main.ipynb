{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "images_arrays = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385088</td>\n",
       "      <td>0.702528</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.364045</td>\n",
       "      <td>0.705285</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.361666</td>\n",
       "      <td>0.700772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525770</td>\n",
       "      <td>0.781881</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>-0.215838</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.165073</td>\n",
       "      <td>0.610637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470336</td>\n",
       "      <td>0.691998</td>\n",
       "      <td>-0.604218</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.705398</td>\n",
       "      <td>-0.624718</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.437711</td>\n",
       "      <td>0.699783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526152</td>\n",
       "      <td>0.778034</td>\n",
       "      <td>0.569747</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.764064</td>\n",
       "      <td>0.639336</td>\n",
       "      <td>0.467184</td>\n",
       "      <td>0.604327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.615995</td>\n",
       "      <td>-0.057232</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.630020</td>\n",
       "      <td>-0.067001</td>\n",
       "      <td>0.988656</td>\n",
       "      <td>0.440118</td>\n",
       "      <td>0.630212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552748</td>\n",
       "      <td>0.769751</td>\n",
       "      <td>0.797690</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.937697</td>\n",
       "      <td>0.743715</td>\n",
       "      <td>0.761387</td>\n",
       "      <td>0.285678</td>\n",
       "      <td>0.625030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.401504</td>\n",
       "      <td>0.383240</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>0.378602</td>\n",
       "      <td>-0.318324</td>\n",
       "      <td>0.989338</td>\n",
       "      <td>0.404689</td>\n",
       "      <td>0.379723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>0.622063</td>\n",
       "      <td>0.713210</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>0.916048</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.763766</td>\n",
       "      <td>-0.045453</td>\n",
       "      <td>0.652138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.683425</td>\n",
       "      <td>-0.067524</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>0.446467</td>\n",
       "      <td>0.690085</td>\n",
       "      <td>-0.090036</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598638</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.911403</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.867453</td>\n",
       "      <td>0.307724</td>\n",
       "      <td>0.658130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOSE_x    NOSE_y    NOSE_z  NOSE_vis  LEFT_EYE_INNER_x  LEFT_EYE_INNER_y  \\\n",
       "0  0.385088  0.702528 -0.004816  0.999651          0.364045          0.705285   \n",
       "1  0.470336  0.691998 -0.604218  0.982091          0.445842          0.705398   \n",
       "2  0.453251  0.615995 -0.057232  0.983838          0.440873          0.630020   \n",
       "3  0.401504  0.383240 -0.309374  0.984927          0.436412          0.378602   \n",
       "4  0.450490  0.683425 -0.067524  0.986200          0.446467          0.690085   \n",
       "\n",
       "   LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  LEFT_EYE_y  ...  \\\n",
       "0         -0.031445            0.999706    0.361666    0.700772  ...   \n",
       "1         -0.624718            0.987435    0.437711    0.699783  ...   \n",
       "2         -0.067001            0.988656    0.440118    0.630212  ...   \n",
       "3         -0.318324            0.989338    0.404689    0.379723  ...   \n",
       "4         -0.090036            0.990196    0.426712    0.690337  ...   \n",
       "\n",
       "   RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  LEFT_FOOT_INDEX_z  \\\n",
       "0        0.525770           0.781881           0.930616          -0.215838   \n",
       "1        0.526152           0.778034           0.569747           0.541724   \n",
       "2        0.552748           0.769751           0.797690           0.004431   \n",
       "3        0.591222           0.622063           0.713210          -0.035523   \n",
       "4        0.598638           0.656271           0.870245           0.077734   \n",
       "\n",
       "   LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  RIGHT_FOOT_INDEX_y  \\\n",
       "0             0.980343            0.763475            0.904605   \n",
       "1             0.935673            0.764064            0.639336   \n",
       "2             0.937697            0.743715            0.761387   \n",
       "3             0.916048            0.569005            0.763766   \n",
       "4             0.911403            0.668724            0.867453   \n",
       "\n",
       "   RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  target  \n",
       "0            0.165073              0.610637       0  \n",
       "1            0.467184              0.604327       0  \n",
       "2            0.285678              0.625030       0  \n",
       "3           -0.045453              0.652138       0  \n",
       "4            0.307724              0.658130       0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying Dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-01d62d68ef784a8a8b51e6efc2e24e6d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-01d62d68ef784a8a8b51e6efc2e24e6d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-01d62d68ef784a8a8b51e6efc2e24e6d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-45b0f0e8af1628084e89ee1ec4750d29\"}, \"mark\": {\"type\": \"bar\", \"size\": 50}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"label\"}, \"tooltip\": [{\"type\": \"quantitative\", \"aggregate\": \"count\", \"title\": \"Count\"}, {\"type\": \"nominal\", \"field\": \"label\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"title\": \"Pose\"}, \"field\": \"label\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"axis\": {\"title\": \"Count\"}}}, \"height\": 300, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Number of data in each pose\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-45b0f0e8af1628084e89ee1ec4750d29\": [{\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.DataFrame()\n",
    "label_df['label'] = list(map(lambda x: labelencoder.inverse_transform([x])[0], data['target']))\n",
    "\n",
    "bars = alt.Chart(label_df).mark_bar(size=50).encode(\n",
    "    x=alt.X('label', axis=alt.Axis(title='Pose')),\n",
    "    y=alt.Y(\"count()\", axis=alt.Axis(title='Count')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Count'), 'label'],\n",
    "    color='label'\n",
    ")\n",
    "\n",
    "(bars).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"Number of data in each pose\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Yolox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    # def __getattr__(self, x):\n",
    "    #     return self[x]\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack\n"
     ]
    }
   ],
   "source": [
    "from yolox.data_augment import random_perspective\n",
    "path = \"dataset/train/\"\n",
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "num_transformations = 0\n",
    "\n",
    "images_arrays = []\n",
    "target = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "#         print(img)\n",
    "        img = cv2.imread(img)\n",
    "        \n",
    "#         print('transformations')\n",
    "        transformations = [img] + [random_perspective(img, scale=[0.5,1.2])[0] for _ in range(num_transformations)]\n",
    "#         print('imgs')\n",
    "        imgs = [preproc(imgx, opt.test_size, opt.rgb_means, opt.std)[0] for imgx in transformations]\n",
    "#         img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        label = subdir.replace(path, '')\n",
    "        \n",
    "        \n",
    "        images_arrays.extend(imgs)\n",
    "        target.extend([label]*(len(imgs)))\n",
    "\n",
    "print('stack')\n",
    "# inp_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "# for b_i, image in enumerate(images_arrays):\n",
    "#     inp_imgs[b_i] = image\n",
    "inp_imgs = np.stack(images_arrays).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 3, 640, 640)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 0 ... 4 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "target = labelencoder.fit_transform(target)\n",
    "\n",
    "# Shuffle them, otherwise our training gets all screwy\n",
    "inp_imgs, target = shuffle(inp_imgs, target)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(images_arrays[1])\n",
    "print(images_arrays[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a baseline model and your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining baseline model\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "baseline_model = SVC(kernel = 'poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_sizes:List[int], input_channels:List[int], num_classes:int,\n",
    "                 hidden_features:int = 128, dropout=.5, num_hidden_layers=1):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc0a = nn.Linear(input_channels[0]*input_sizes[0]**2,hidden_features)\n",
    "        self.fc0b = nn.Linear(input_channels[1]*input_sizes[1]**2,hidden_features)\n",
    "        self.fc0c = nn.Linear(input_channels[2]*input_sizes[2]**2,hidden_features)\n",
    "        # Concatenate the three outputs into one linear layer\n",
    "        self.fc1 = nn.Linear(len(input_sizes) * hidden_features, hidden_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        # self.dropout2 = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.Sequential(*[nn.Sequential(nn.Linear(hidden_features, hidden_features),\n",
    "                                                           nn.ReLU(),\n",
    "                                                           nn.Dropout(dropout),\n",
    "                                                           )\n",
    "                                             for _ in range(num_hidden_layers)\n",
    "                                             ]\n",
    "                                           )\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a = F.relu(self.fc0a(torch.flatten(x[0],1)))\n",
    "        b = F.relu(self.fc0b(torch.flatten(x[1],1)))\n",
    "        c = F.relu(self.fc0c(torch.flatten(x[2],1)))\n",
    "        x = torch.cat([a,b,c], dim=1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.hidden_layers(x)\n",
    "        # x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = 1e-3\n",
    "                m.momentum = 0.03\n",
    "\n",
    "def generate_model(opt, hidden_features=128, dropout=0.5, freeze_layers=True, num_hidden_layers=1):\n",
    "    return get_model(opt,\n",
    "                     head=ClassificationHead([80,40,20], [64,128,256], 5,\n",
    "                                             hidden_features=hidden_features, dropout=dropout,\n",
    "                                             num_hidden_layers=num_hidden_layers),\n",
    "                     freeze_layers=freeze_layers)\n",
    "\n",
    "\n",
    "model = generate_model(opt)\n",
    "model.to(opt.device)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Run a training loop on a training set with both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs (i.e. all tensors are the right shape)\n",
    "# Use only the first 10 images, for speed's sake\n",
    "model = model.to(opt.device)\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(torch.as_tensor(inp_imgs[:10]).to(opt.device))\n",
    "    # print(yolo_outputs)\n",
    "    print(yolo_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1615\u001b[0m       \u001b[32m0.3264\u001b[0m        \u001b[35m1.5346\u001b[0m     +  1.4537\n",
      "      2        \u001b[36m1.6570\u001b[0m       0.2708        \u001b[35m1.5254\u001b[0m     +  1.4630\n",
      "      3        \u001b[36m1.5281\u001b[0m       \u001b[32m0.4236\u001b[0m        \u001b[35m1.4407\u001b[0m     +  1.4755\n",
      "      4        \u001b[36m1.5053\u001b[0m       \u001b[32m0.5486\u001b[0m        \u001b[35m1.3852\u001b[0m     +  1.4690\n",
      "      5        \u001b[36m1.4231\u001b[0m       0.5069        \u001b[35m1.2742\u001b[0m     +  1.4628\n",
      "      6        \u001b[36m1.3497\u001b[0m       0.4722        1.3009        1.4549\n",
      "      7        \u001b[36m1.2771\u001b[0m       0.5417        \u001b[35m1.2052\u001b[0m     +  1.4479\n",
      "      8        \u001b[36m1.1360\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m1.0904\u001b[0m     +  1.4672\n",
      "      9        \u001b[36m0.9850\u001b[0m       0.6042        \u001b[35m1.0172\u001b[0m     +  1.4625\n",
      "     10        \u001b[36m0.8913\u001b[0m       0.6042        1.0211        1.4690\n",
      "     11        \u001b[36m0.7399\u001b[0m       \u001b[32m0.6597\u001b[0m        \u001b[35m0.9809\u001b[0m     +  1.4448\n",
      "     12        \u001b[36m0.6700\u001b[0m       0.6250        \u001b[35m0.9258\u001b[0m     +  1.4643\n",
      "     13        \u001b[36m0.5519\u001b[0m       0.6528        0.9384        1.4660\n",
      "     14        \u001b[36m0.5237\u001b[0m       0.6458        \u001b[35m0.9197\u001b[0m     +  1.4492\n",
      "     15        \u001b[36m0.5069\u001b[0m       0.6389        0.9761        1.4730\n",
      "     16        \u001b[36m0.4305\u001b[0m       0.6181        1.0433        1.4562\n",
      "     17        \u001b[36m0.4100\u001b[0m       0.6597        1.0442        1.4468\n",
      "     18        \u001b[36m0.3076\u001b[0m       0.6597        1.1149        1.4469\n",
      "     19        \u001b[36m0.3073\u001b[0m       \u001b[32m0.6806\u001b[0m        1.0195        1.4473\n",
      "     20        \u001b[36m0.2477\u001b[0m       \u001b[32m0.6875\u001b[0m        0.9403        1.4472\n",
      "     21        0.2732       \u001b[32m0.7014\u001b[0m        \u001b[35m0.8722\u001b[0m     +  1.4468\n",
      "     22        \u001b[36m0.1431\u001b[0m       0.7014        0.9941        1.4575\n",
      "     23        \u001b[36m0.1400\u001b[0m       0.6736        1.2866        1.4452\n",
      "     24        \u001b[36m0.1102\u001b[0m       \u001b[32m0.7083\u001b[0m        1.1194        1.4478\n",
      "     25        \u001b[36m0.1037\u001b[0m       0.7083        1.1866        1.4463\n",
      "     26        \u001b[36m0.0985\u001b[0m       \u001b[32m0.7153\u001b[0m        1.1230        1.4447\n",
      "     27        \u001b[36m0.0808\u001b[0m       \u001b[32m0.7222\u001b[0m        1.1967        1.4561\n",
      "     28        \u001b[36m0.0634\u001b[0m       0.7153        1.2200        1.4449\n",
      "     29        \u001b[36m0.0541\u001b[0m       0.7014        1.2249        1.4530\n",
      "     30        0.0791       0.6597        1.4834        1.4453\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.561 total time=  54.9s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0965\u001b[0m       \u001b[32m0.3103\u001b[0m        \u001b[35m1.5648\u001b[0m     +  1.4670\n",
      "      2        \u001b[36m1.6950\u001b[0m       0.2759        \u001b[35m1.5577\u001b[0m     +  1.4942\n",
      "      3        \u001b[36m1.6166\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m1.5265\u001b[0m     +  1.4855\n",
      "      4        \u001b[36m1.5714\u001b[0m       \u001b[32m0.3724\u001b[0m        \u001b[35m1.4896\u001b[0m     +  1.5030\n",
      "      5        \u001b[36m1.5710\u001b[0m       0.2897        1.5247        1.4921\n",
      "      6        1.5731       0.3103        1.5331        1.4586\n",
      "      7        \u001b[36m1.5320\u001b[0m       0.3241        \u001b[35m1.4233\u001b[0m     +  1.4535\n",
      "      8        \u001b[36m1.4848\u001b[0m       0.3448        1.4449        1.4656\n",
      "      9        \u001b[36m1.4577\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.3826\u001b[0m     +  1.4548\n",
      "     10        \u001b[36m1.4488\u001b[0m       \u001b[32m0.4207\u001b[0m        \u001b[35m1.3141\u001b[0m     +  1.4704\n",
      "     11        \u001b[36m1.4443\u001b[0m       0.3793        1.3592        1.4650\n",
      "     12        \u001b[36m1.3128\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m1.1982\u001b[0m     +  1.4508\n",
      "     13        \u001b[36m1.2720\u001b[0m       \u001b[32m0.5172\u001b[0m        1.2795        1.4725\n",
      "     14        \u001b[36m1.2030\u001b[0m       \u001b[32m0.5241\u001b[0m        1.2729        1.4680\n",
      "     15        \u001b[36m1.1985\u001b[0m       \u001b[32m0.5655\u001b[0m        \u001b[35m1.1916\u001b[0m     +  1.4526\n",
      "     16        \u001b[36m1.1706\u001b[0m       0.4621        \u001b[35m1.1202\u001b[0m     +  1.4709\n",
      "     17        \u001b[36m1.0684\u001b[0m       0.5655        \u001b[35m1.0672\u001b[0m     +  1.4719\n",
      "     18        \u001b[36m0.9965\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m0.9420\u001b[0m     +  1.4637\n",
      "     19        \u001b[36m0.9514\u001b[0m       \u001b[32m0.6759\u001b[0m        0.9713        1.4728\n",
      "     20        \u001b[36m0.8448\u001b[0m       0.6414        1.0308        1.4512\n",
      "     21        \u001b[36m0.7473\u001b[0m       0.6621        1.0787        1.4493\n",
      "     22        \u001b[36m0.7163\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.8830\u001b[0m     +  1.4517\n",
      "     23        \u001b[36m0.6403\u001b[0m       0.6552        \u001b[35m0.8693\u001b[0m     +  1.4704\n",
      "     24        \u001b[36m0.5800\u001b[0m       \u001b[32m0.7379\u001b[0m        \u001b[35m0.7782\u001b[0m     +  1.4733\n",
      "     25        \u001b[36m0.4520\u001b[0m       0.5931        0.9858        1.4680\n",
      "     26        \u001b[36m0.3762\u001b[0m       0.6966        \u001b[35m0.7699\u001b[0m     +  1.4507\n",
      "     27        \u001b[36m0.2869\u001b[0m       0.6966        0.7739        1.4698\n",
      "     28        \u001b[36m0.2653\u001b[0m       0.6966        0.7750        1.4499\n",
      "     29        \u001b[36m0.2579\u001b[0m       0.6552        0.8548        1.4506\n",
      "     30        \u001b[36m0.2233\u001b[0m       0.7034        0.8515        1.4506\n",
      "     31        0.2311       0.6621        1.1503        1.4519\n",
      "     32        \u001b[36m0.1565\u001b[0m       0.6897        1.0333        1.4517\n",
      "     33        0.1731       0.7310        0.9774        1.5012\n",
      "     34        \u001b[36m0.1403\u001b[0m       0.6966        0.9940        1.4582\n",
      "     35        \u001b[36m0.1202\u001b[0m       0.7379        0.8504        1.4625\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.161 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.3995\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.5370\u001b[0m     +  1.4625\n",
      "      2        \u001b[36m1.5840\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.5152\u001b[0m     +  1.4748\n",
      "      3        \u001b[36m1.5742\u001b[0m       \u001b[32m0.3793\u001b[0m        \u001b[35m1.4834\u001b[0m     +  1.4763\n",
      "      4        1.5804       0.3655        \u001b[35m1.4474\u001b[0m     +  1.4827\n",
      "      5        \u001b[36m1.5098\u001b[0m       0.3793        1.4504        1.4688\n",
      "      6        1.5361       0.3448        \u001b[35m1.4311\u001b[0m     +  1.4520\n",
      "      7        1.5314       \u001b[32m0.4345\u001b[0m        \u001b[35m1.4270\u001b[0m     +  1.4741\n",
      "      8        \u001b[36m1.4440\u001b[0m       0.4069        \u001b[35m1.3503\u001b[0m     +  1.4782\n",
      "      9        \u001b[36m1.4162\u001b[0m       0.3931        \u001b[35m1.3427\u001b[0m     +  1.4732\n",
      "     10        1.4399       0.4207        1.3615        1.4801\n",
      "     11        \u001b[36m1.2599\u001b[0m       \u001b[32m0.4759\u001b[0m        1.3771        1.4563\n",
      "     12        1.3094       0.4138        \u001b[35m1.2895\u001b[0m     +  1.4505\n",
      "     13        \u001b[36m1.2400\u001b[0m       0.4345        \u001b[35m1.2147\u001b[0m     +  1.4666\n",
      "     14        \u001b[36m1.1415\u001b[0m       0.4345        1.3150        1.4756\n",
      "     15        \u001b[36m1.0817\u001b[0m       \u001b[32m0.5310\u001b[0m        \u001b[35m1.1008\u001b[0m     +  1.4476\n",
      "     16        \u001b[36m1.0499\u001b[0m       \u001b[32m0.6345\u001b[0m        1.1094        1.4717\n",
      "     17        \u001b[36m0.8747\u001b[0m       0.5793        1.1531        1.4514\n",
      "     18        0.8858       0.5517        \u001b[35m1.0849\u001b[0m     +  1.4531\n",
      "     19        \u001b[36m0.7178\u001b[0m       \u001b[32m0.6483\u001b[0m        \u001b[35m1.0699\u001b[0m     +  1.4697\n",
      "     20        \u001b[36m0.6401\u001b[0m       \u001b[32m0.6690\u001b[0m        1.0868        1.4767\n",
      "     21        \u001b[36m0.5329\u001b[0m       0.6000        1.3027        1.4507\n",
      "     22        \u001b[36m0.4736\u001b[0m       0.6621        1.1684        1.4500\n",
      "     23        \u001b[36m0.3965\u001b[0m       \u001b[32m0.6897\u001b[0m        1.0800        1.4506\n",
      "     24        \u001b[36m0.3278\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m1.0168\u001b[0m     +  1.4509\n",
      "     25        \u001b[36m0.2772\u001b[0m       \u001b[32m0.7034\u001b[0m        1.1478        1.4683\n",
      "     26        0.3341       0.6897        1.0632        1.4529\n",
      "     27        \u001b[36m0.2368\u001b[0m       0.6966        1.0693        1.4495\n",
      "     28        \u001b[36m0.2082\u001b[0m       0.6690        1.3189        1.4526\n",
      "     29        \u001b[36m0.1613\u001b[0m       0.6828        1.2722        1.4503\n",
      "     30        \u001b[36m0.1447\u001b[0m       0.6966        1.5304        1.4682\n",
      "     31        0.1632       0.6690        1.7649        1.4715\n",
      "     32        0.1740       0.6897        1.2104        1.4695\n",
      "     33        0.1492       0.6690        1.4053        1.4666\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-0.958 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0393\u001b[0m       \u001b[32m0.2778\u001b[0m        \u001b[35m1.5839\u001b[0m     +  1.4658\n",
      "      2        \u001b[36m1.6608\u001b[0m       \u001b[32m0.3194\u001b[0m        \u001b[35m1.5792\u001b[0m     +  1.4671\n",
      "      3        \u001b[36m1.5976\u001b[0m       0.3194        \u001b[35m1.5480\u001b[0m     +  1.4685\n",
      "      4        1.5991       0.3194        \u001b[35m1.5456\u001b[0m     +  1.4686\n",
      "      5        \u001b[36m1.5687\u001b[0m       \u001b[32m0.3611\u001b[0m        \u001b[35m1.5208\u001b[0m     +  1.4750\n",
      "      6        \u001b[36m1.5225\u001b[0m       0.3611        \u001b[35m1.5027\u001b[0m     +  1.4712\n",
      "      7        1.5300       \u001b[32m0.4514\u001b[0m        \u001b[35m1.4758\u001b[0m     +  1.4790\n",
      "      8        \u001b[36m1.4713\u001b[0m       0.3958        \u001b[35m1.4386\u001b[0m     +  1.4930\n",
      "      9        \u001b[36m1.4623\u001b[0m       0.4236        \u001b[35m1.3820\u001b[0m     +  1.4950\n",
      "     10        \u001b[36m1.3873\u001b[0m       0.4514        1.3876        1.4850\n",
      "     11        \u001b[36m1.3427\u001b[0m       \u001b[32m0.4931\u001b[0m        \u001b[35m1.2962\u001b[0m     +  1.4567\n",
      "     12        \u001b[36m1.2598\u001b[0m       \u001b[32m0.5764\u001b[0m        \u001b[35m1.2293\u001b[0m     +  1.4878\n",
      "     13        \u001b[36m1.1623\u001b[0m       0.5278        \u001b[35m1.1551\u001b[0m     +  1.4757\n",
      "     14        \u001b[36m1.0746\u001b[0m       \u001b[32m0.6181\u001b[0m        \u001b[35m1.0746\u001b[0m     +  1.4705\n",
      "     15        \u001b[36m0.9802\u001b[0m       0.6181        \u001b[35m1.0623\u001b[0m     +  1.4789\n",
      "     16        \u001b[36m0.9155\u001b[0m       \u001b[32m0.6806\u001b[0m        \u001b[35m1.0236\u001b[0m     +  1.4970\n",
      "     17        \u001b[36m0.8659\u001b[0m       0.6528        \u001b[35m1.0199\u001b[0m     +  1.4869\n",
      "     18        \u001b[36m0.7599\u001b[0m       0.6111        \u001b[35m0.9903\u001b[0m     +  1.4815\n",
      "     19        \u001b[36m0.7340\u001b[0m       0.6389        1.0197        1.4958\n",
      "     20        \u001b[36m0.6270\u001b[0m       0.6667        \u001b[35m0.9760\u001b[0m     +  1.4630\n",
      "     21        \u001b[36m0.5709\u001b[0m       0.6528        \u001b[35m0.9559\u001b[0m     +  1.4770\n",
      "     22        0.6074       0.6597        \u001b[35m0.9394\u001b[0m     +  1.4953\n",
      "     23        \u001b[36m0.5621\u001b[0m       0.6250        0.9897        1.4821\n",
      "     24        \u001b[36m0.4987\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.8813\u001b[0m     +  1.4638\n",
      "     25        \u001b[36m0.4313\u001b[0m       0.6597        0.9013        1.4911\n",
      "     26        \u001b[36m0.3786\u001b[0m       0.6528        0.9492        1.4695\n",
      "     27        \u001b[36m0.3783\u001b[0m       0.6528        0.9570        1.4632\n",
      "     28        \u001b[36m0.2996\u001b[0m       0.6319        1.1023        1.4622\n",
      "     29        \u001b[36m0.2905\u001b[0m       0.6319        1.1206        1.4621\n",
      "     30        \u001b[36m0.2892\u001b[0m       0.6667        1.1036        1.4622\n",
      "     31        \u001b[36m0.2639\u001b[0m       0.6042        1.2796        1.4618\n",
      "     32        \u001b[36m0.2426\u001b[0m       0.6458        1.0768        1.4614\n",
      "     33        \u001b[36m0.2291\u001b[0m       0.6875        1.1188        1.4768\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-0.937 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9755\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.5846\u001b[0m     +  1.5066\n",
      "      2        \u001b[36m1.6363\u001b[0m       \u001b[32m0.3172\u001b[0m        \u001b[35m1.5833\u001b[0m     +  1.4803\n",
      "      3        \u001b[36m1.6186\u001b[0m       0.2621        \u001b[35m1.5822\u001b[0m     +  1.4735\n",
      "      4        1.6242       \u001b[32m0.3379\u001b[0m        \u001b[35m1.5644\u001b[0m     +  1.4856\n",
      "      5        \u001b[36m1.5770\u001b[0m       0.3379        \u001b[35m1.5530\u001b[0m     +  1.4754\n",
      "      6        1.6036       0.3310        \u001b[35m1.5393\u001b[0m     +  1.4767\n",
      "      7        \u001b[36m1.5500\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.5230\u001b[0m     +  1.4928\n",
      "      8        \u001b[36m1.5413\u001b[0m       0.3793        \u001b[35m1.4838\u001b[0m     +  1.4762\n",
      "      9        \u001b[36m1.5033\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4682\u001b[0m     +  1.4906\n",
      "     10        \u001b[36m1.4667\u001b[0m       \u001b[32m0.4207\u001b[0m        \u001b[35m1.4356\u001b[0m     +  1.4838\n",
      "     11        \u001b[36m1.4407\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.4161\u001b[0m     +  1.4763\n",
      "     12        1.4491       0.4000        \u001b[35m1.4066\u001b[0m     +  1.4937\n",
      "     13        \u001b[36m1.4125\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m1.3712\u001b[0m     +  1.4964\n",
      "     14        \u001b[36m1.3042\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.3185\u001b[0m     +  1.5012\n",
      "     15        \u001b[36m1.2532\u001b[0m       0.4345        \u001b[35m1.2748\u001b[0m     +  1.4826\n",
      "     16        \u001b[36m1.2260\u001b[0m       \u001b[32m0.5034\u001b[0m        \u001b[35m1.2533\u001b[0m     +  1.4833\n",
      "     17        \u001b[36m1.1369\u001b[0m       0.4414        \u001b[35m1.2296\u001b[0m     +  1.4908\n",
      "     18        \u001b[36m1.0685\u001b[0m       \u001b[32m0.5310\u001b[0m        1.2359        1.4916\n",
      "     19        \u001b[36m1.0299\u001b[0m       \u001b[32m0.5724\u001b[0m        1.2546        1.4596\n",
      "     20        \u001b[36m0.9856\u001b[0m       \u001b[32m0.5862\u001b[0m        \u001b[35m1.1929\u001b[0m     +  1.4585\n",
      "     21        \u001b[36m0.9414\u001b[0m       0.5793        \u001b[35m1.1388\u001b[0m     +  1.4793\n",
      "     22        \u001b[36m0.8591\u001b[0m       \u001b[32m0.6276\u001b[0m        1.1942        1.4762\n",
      "     23        \u001b[36m0.8447\u001b[0m       0.5931        1.1773        1.4580\n",
      "     24        \u001b[36m0.7806\u001b[0m       0.5655        1.2189        1.4586\n",
      "     25        \u001b[36m0.7333\u001b[0m       0.5931        \u001b[35m1.1049\u001b[0m     +  1.4610\n",
      "     26        \u001b[36m0.7245\u001b[0m       0.6069        1.1240        1.4855\n",
      "     27        \u001b[36m0.6276\u001b[0m       \u001b[32m0.6552\u001b[0m        \u001b[35m1.0903\u001b[0m     +  1.4713\n",
      "     28        \u001b[36m0.6126\u001b[0m       0.6345        \u001b[35m1.0315\u001b[0m     +  1.4807\n",
      "     29        \u001b[36m0.5792\u001b[0m       0.6000        1.0859        1.4779\n",
      "     30        \u001b[36m0.4954\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m1.0298\u001b[0m     +  1.4612\n",
      "     31        0.5080       0.6000        1.2499        1.4809\n",
      "     32        0.5912       0.6552        1.0324        1.4618\n",
      "     33        \u001b[36m0.4790\u001b[0m       0.6207        1.0558        1.4640\n",
      "     34        \u001b[36m0.4385\u001b[0m       0.6414        \u001b[35m0.9965\u001b[0m     +  1.4600\n",
      "     35        \u001b[36m0.4186\u001b[0m       \u001b[32m0.7172\u001b[0m        1.1116        1.4939\n",
      "     36        \u001b[36m0.4060\u001b[0m       0.6828        1.0420        1.4926\n",
      "     37        \u001b[36m0.3905\u001b[0m       0.6690        1.1718        1.4729\n",
      "     38        \u001b[36m0.3494\u001b[0m       0.7034        1.0098        1.4668\n",
      "     39        \u001b[36m0.3346\u001b[0m       0.6552        1.0854        1.4605\n",
      "     40        0.3464       0.6483        1.3218        1.4596\n",
      "     41        \u001b[36m0.2941\u001b[0m       0.6690        1.0772        1.4607\n",
      "     42        \u001b[36m0.2344\u001b[0m       0.6759        1.2034        1.4653\n",
      "     43        \u001b[36m0.2269\u001b[0m       0.6690        1.2655        1.4614\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.538 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0754\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5951\u001b[0m     +  1.4879\n",
      "      2        \u001b[36m1.6154\u001b[0m       0.2414        1.6029        1.4946\n",
      "      3        \u001b[36m1.6137\u001b[0m       0.2483        1.6005        1.4791\n",
      "      4        \u001b[36m1.6084\u001b[0m       0.2483        1.6031        1.4782\n",
      "      5        1.6244       0.2414        1.5997        1.4722\n",
      "      6        \u001b[36m1.6009\u001b[0m       0.2483        1.5996        1.4717\n",
      "      7        1.6123       0.2483        1.5962        1.4714\n",
      "      8        1.6298       0.2483        1.6044        1.4758\n",
      "      9        1.6162       0.2483        1.6013        1.4735\n",
      "     10        1.6158       0.2483        \u001b[35m1.5939\u001b[0m     +  1.4740\n",
      "     11        \u001b[36m1.5977\u001b[0m       0.2483        1.5965        1.5011\n",
      "     12        \u001b[36m1.5914\u001b[0m       0.2483        \u001b[35m1.5768\u001b[0m     +  1.4723\n",
      "     13        \u001b[36m1.5691\u001b[0m       \u001b[32m0.2552\u001b[0m        \u001b[35m1.5329\u001b[0m     +  1.4926\n",
      "     14        1.5749       0.2483        1.5356        1.5007\n",
      "     15        \u001b[36m1.5338\u001b[0m       0.2483        \u001b[35m1.5276\u001b[0m     +  1.4822\n",
      "     16        1.5389       \u001b[32m0.2690\u001b[0m        \u001b[35m1.4970\u001b[0m     +  1.4912\n",
      "     17        \u001b[36m1.4951\u001b[0m       0.2690        \u001b[35m1.4415\u001b[0m     +  1.4930\n",
      "     18        1.5080       0.2621        1.5102        1.5034\n",
      "     19        1.5024       \u001b[32m0.2966\u001b[0m        1.4460        1.4864\n",
      "     20        \u001b[36m1.4027\u001b[0m       0.2759        \u001b[35m1.4215\u001b[0m     +  1.4816\n",
      "     21        1.4252       0.2828        \u001b[35m1.3734\u001b[0m     +  1.5136\n",
      "     22        1.4312       \u001b[32m0.3172\u001b[0m        \u001b[35m1.3228\u001b[0m     +  1.5158\n",
      "     23        \u001b[36m1.3789\u001b[0m       0.2966        1.3897        1.5631\n",
      "     24        \u001b[36m1.3511\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.2686\u001b[0m     +  1.5318\n",
      "     25        \u001b[36m1.2525\u001b[0m       \u001b[32m0.4138\u001b[0m        1.2946        1.5354\n",
      "     26        \u001b[36m1.2228\u001b[0m       0.3931        1.2913        1.5398\n",
      "     27        1.2285       0.3793        \u001b[35m1.2359\u001b[0m     +  1.5247\n",
      "     28        \u001b[36m1.1562\u001b[0m       0.4138        \u001b[35m1.1564\u001b[0m     +  1.5306\n",
      "     29        1.1571       \u001b[32m0.4483\u001b[0m        \u001b[35m1.1471\u001b[0m     +  1.5044\n",
      "     30        \u001b[36m1.1416\u001b[0m       0.4345        1.1710        1.5073\n",
      "     31        1.1452       0.4000        \u001b[35m1.1417\u001b[0m     +  1.4885\n",
      "     32        \u001b[36m1.0640\u001b[0m       \u001b[32m0.4759\u001b[0m        1.1743        1.4965\n",
      "     33        1.0723       \u001b[32m0.5034\u001b[0m        1.2297        1.4752\n",
      "     34        \u001b[36m0.9996\u001b[0m       0.4966        \u001b[35m1.1229\u001b[0m     +  1.4712\n",
      "     35        \u001b[36m0.9198\u001b[0m       \u001b[32m0.5241\u001b[0m        \u001b[35m1.1193\u001b[0m     +  1.4914\n",
      "     36        \u001b[36m0.9015\u001b[0m       0.4966        1.1322        1.5126\n",
      "     37        \u001b[36m0.8802\u001b[0m       0.4690        1.1708        1.4958\n",
      "     38        \u001b[36m0.8315\u001b[0m       0.5241        1.2101        1.4780\n",
      "     39        0.8350       \u001b[32m0.5517\u001b[0m        1.3602        1.4706\n",
      "     40        \u001b[36m0.8115\u001b[0m       0.5310        1.2107        1.4710\n",
      "     41        0.8119       0.5103        1.2352        1.4704\n",
      "     42        \u001b[36m0.7621\u001b[0m       0.5034        1.3133        1.4709\n",
      "     43        0.7775       \u001b[32m0.5655\u001b[0m        1.2126        1.4708\n",
      "     44        0.7843       0.5241        1.3166        1.4710\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.497 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8219\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5906\u001b[0m     +  1.4936\n",
      "      2        \u001b[36m1.6125\u001b[0m       0.2431        \u001b[35m1.5895\u001b[0m     +  1.4763\n",
      "      3        \u001b[36m1.5913\u001b[0m       0.2292        \u001b[35m1.5846\u001b[0m     +  1.4925\n",
      "      4        1.5948       \u001b[32m0.2708\u001b[0m        \u001b[35m1.5789\u001b[0m     +  1.4784\n",
      "      5        \u001b[36m1.5830\u001b[0m       0.2431        \u001b[35m1.5703\u001b[0m     +  1.4795\n",
      "      6        \u001b[36m1.5736\u001b[0m       0.2431        \u001b[35m1.5632\u001b[0m     +  1.4993\n",
      "      7        \u001b[36m1.5666\u001b[0m       0.2431        \u001b[35m1.5551\u001b[0m     +  1.4971\n",
      "      8        \u001b[36m1.5586\u001b[0m       0.2431        \u001b[35m1.5404\u001b[0m     +  1.4765\n",
      "      9        \u001b[36m1.5310\u001b[0m       0.2500        \u001b[35m1.5230\u001b[0m     +  1.4831\n",
      "     10        \u001b[36m1.5147\u001b[0m       0.2500        \u001b[35m1.5040\u001b[0m     +  1.4815\n",
      "     11        \u001b[36m1.4855\u001b[0m       0.2500        \u001b[35m1.4798\u001b[0m     +  1.4788\n",
      "     12        \u001b[36m1.4578\u001b[0m       0.2500        \u001b[35m1.4513\u001b[0m     +  1.4957\n",
      "     13        \u001b[36m1.4404\u001b[0m       0.2500        1.4561        1.4913\n",
      "     14        \u001b[36m1.3862\u001b[0m       0.2500        \u001b[35m1.4196\u001b[0m     +  1.4657\n",
      "     15        \u001b[36m1.3659\u001b[0m       0.2639        \u001b[35m1.3916\u001b[0m     +  1.4839\n",
      "     16        \u001b[36m1.3205\u001b[0m       \u001b[32m0.3542\u001b[0m        \u001b[35m1.3614\u001b[0m     +  1.4848\n",
      "     17        \u001b[36m1.2705\u001b[0m       0.3472        \u001b[35m1.3336\u001b[0m     +  1.4864\n",
      "     18        \u001b[36m1.2098\u001b[0m       \u001b[32m0.3958\u001b[0m        \u001b[35m1.2847\u001b[0m     +  1.4839\n",
      "     19        \u001b[36m1.1921\u001b[0m       \u001b[32m0.5139\u001b[0m        \u001b[35m1.2611\u001b[0m     +  1.4864\n",
      "     20        \u001b[36m1.1051\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m1.2092\u001b[0m     +  1.4903\n",
      "     21        \u001b[36m1.0493\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.1870\u001b[0m     +  1.4835\n",
      "     22        \u001b[36m0.9753\u001b[0m       0.5139        1.1983        1.4951\n",
      "     23        \u001b[36m0.9366\u001b[0m       \u001b[32m0.5764\u001b[0m        \u001b[35m1.1672\u001b[0m     +  1.4762\n",
      "     24        \u001b[36m0.9086\u001b[0m       0.5694        1.1842        1.4881\n",
      "     25        \u001b[36m0.8587\u001b[0m       0.5347        1.1677        1.4544\n",
      "     26        \u001b[36m0.8071\u001b[0m       0.5208        1.1909        1.4546\n",
      "     27        \u001b[36m0.7893\u001b[0m       0.5208        1.1988        1.4549\n",
      "     28        \u001b[36m0.7842\u001b[0m       0.5556        \u001b[35m1.1626\u001b[0m     +  1.4553\n",
      "     29        \u001b[36m0.7395\u001b[0m       0.5278        1.2672        1.4710\n",
      "     30        \u001b[36m0.6853\u001b[0m       0.5625        1.2685        1.4528\n",
      "     31        \u001b[36m0.6554\u001b[0m       0.4931        1.3898        1.4556\n",
      "     32        \u001b[36m0.5958\u001b[0m       0.5625        1.3148        1.4542\n",
      "     33        \u001b[36m0.5790\u001b[0m       0.5625        1.3623        1.4530\n",
      "     34        \u001b[36m0.5660\u001b[0m       \u001b[32m0.5903\u001b[0m        1.4886        1.4541\n",
      "     35        \u001b[36m0.5489\u001b[0m       0.4931        1.4004        1.4546\n",
      "     36        \u001b[36m0.5250\u001b[0m       0.4444        1.6389        1.4571\n",
      "     37        \u001b[36m0.4899\u001b[0m       0.5556        1.2163        1.4569\n",
      "     38        \u001b[36m0.4750\u001b[0m       0.5833        \u001b[35m1.1405\u001b[0m     +  1.4577\n",
      "     39        \u001b[36m0.4346\u001b[0m       0.5764        1.4267        1.4852\n",
      "     40        0.4574       0.5625        1.4125        1.4617\n",
      "     41        \u001b[36m0.3806\u001b[0m       0.5694        1.3870        1.4601\n",
      "     42        \u001b[36m0.3607\u001b[0m       \u001b[32m0.6042\u001b[0m        1.5300        1.4593\n",
      "     43        0.4091       0.6042        1.5212        1.4566\n",
      "     44        0.3691       0.5625        1.3136        1.4582\n",
      "     45        \u001b[36m0.3322\u001b[0m       0.5694        1.4901        1.4568\n",
      "     46        0.4217       0.5833        1.5532        1.4576\n",
      "     47        0.3413       0.5764        1.5223        1.4606\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.439 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8286\u001b[0m       \u001b[32m0.2138\u001b[0m        \u001b[35m1.6139\u001b[0m     +  1.4775\n",
      "      2        \u001b[36m1.6944\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.6072\u001b[0m     +  1.5098\n",
      "      3        \u001b[36m1.6256\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5983\u001b[0m     +  1.5160\n",
      "      4        1.6315       0.2414        \u001b[35m1.5951\u001b[0m     +  1.5048\n",
      "      5        1.6317       0.2483        \u001b[35m1.5879\u001b[0m     +  1.5114\n",
      "      6        \u001b[36m1.6178\u001b[0m       \u001b[32m0.2621\u001b[0m        1.5898        1.5071\n",
      "      7        \u001b[36m1.5961\u001b[0m       \u001b[32m0.2690\u001b[0m        \u001b[35m1.5849\u001b[0m     +  1.4750\n",
      "      8        \u001b[36m1.5914\u001b[0m       0.2483        \u001b[35m1.5751\u001b[0m     +  1.5097\n",
      "      9        \u001b[36m1.5816\u001b[0m       \u001b[32m0.2897\u001b[0m        1.5800        1.5154\n",
      "     10        1.5822       \u001b[32m0.3310\u001b[0m        \u001b[35m1.5504\u001b[0m     +  1.4864\n",
      "     11        \u001b[36m1.5735\u001b[0m       0.2621        1.5506        1.5069\n",
      "     12        \u001b[36m1.5513\u001b[0m       \u001b[32m0.5034\u001b[0m        \u001b[35m1.5308\u001b[0m     +  1.4765\n",
      "     13        \u001b[36m1.5363\u001b[0m       0.4000        \u001b[35m1.5145\u001b[0m     +  1.5069\n",
      "     14        \u001b[36m1.5156\u001b[0m       0.4552        \u001b[35m1.4804\u001b[0m     +  1.5070\n",
      "     15        \u001b[36m1.5096\u001b[0m       0.3793        \u001b[35m1.4517\u001b[0m     +  1.5082\n",
      "     16        \u001b[36m1.4729\u001b[0m       0.3931        \u001b[35m1.4301\u001b[0m     +  1.5110\n",
      "     17        \u001b[36m1.4029\u001b[0m       0.4000        \u001b[35m1.3839\u001b[0m     +  1.5091\n",
      "     18        \u001b[36m1.3891\u001b[0m       0.4276        \u001b[35m1.3340\u001b[0m     +  1.5026\n",
      "     19        \u001b[36m1.2906\u001b[0m       0.4966        \u001b[35m1.3144\u001b[0m     +  1.5040\n",
      "     20        \u001b[36m1.2535\u001b[0m       0.4414        \u001b[35m1.2749\u001b[0m     +  1.5068\n",
      "     21        \u001b[36m1.1852\u001b[0m       0.5034        \u001b[35m1.2565\u001b[0m     +  1.5006\n",
      "     22        \u001b[36m1.1384\u001b[0m       \u001b[32m0.5241\u001b[0m        \u001b[35m1.2365\u001b[0m     +  1.5039\n",
      "     23        \u001b[36m1.0528\u001b[0m       \u001b[32m0.5655\u001b[0m        \u001b[35m1.1382\u001b[0m     +  1.5015\n",
      "     24        \u001b[36m0.9882\u001b[0m       \u001b[32m0.5931\u001b[0m        \u001b[35m1.1382\u001b[0m     +  1.5104\n",
      "     25        \u001b[36m0.9548\u001b[0m       \u001b[32m0.6207\u001b[0m        1.1603        1.5168\n",
      "     26        \u001b[36m0.8746\u001b[0m       0.5586        \u001b[35m1.1070\u001b[0m     +  1.4767\n",
      "     27        \u001b[36m0.8335\u001b[0m       \u001b[32m0.6414\u001b[0m        1.1363        1.5210\n",
      "     28        \u001b[36m0.7925\u001b[0m       0.6276        \u001b[35m1.0395\u001b[0m     +  1.5130\n",
      "     29        \u001b[36m0.7221\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m1.0005\u001b[0m     +  1.4979\n",
      "     30        0.7278       0.6621        1.1987        1.5082\n",
      "     31        \u001b[36m0.6490\u001b[0m       0.6207        1.1329        1.4956\n",
      "     32        0.6642       0.6828        \u001b[35m0.9243\u001b[0m     +  1.4891\n",
      "     33        0.6521       0.6828        1.0761        1.5072\n",
      "     34        \u001b[36m0.6196\u001b[0m       0.6345        1.0340        1.4807\n",
      "     35        \u001b[36m0.5166\u001b[0m       0.6345        1.2058        1.4884\n",
      "     36        0.5338       0.6690        1.0400        1.4816\n",
      "     37        0.5305       0.6690        0.9335        1.4789\n",
      "     38        \u001b[36m0.5054\u001b[0m       \u001b[32m0.6966\u001b[0m        1.0467        1.4762\n",
      "     39        \u001b[36m0.4599\u001b[0m       \u001b[32m0.7034\u001b[0m        1.2353        1.4806\n",
      "     40        0.4748       0.6552        1.1753        1.4776\n",
      "     41        \u001b[36m0.4081\u001b[0m       \u001b[32m0.7172\u001b[0m        0.9512        1.4796\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.282 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8012\u001b[0m       \u001b[32m0.1586\u001b[0m        \u001b[35m1.6151\u001b[0m     +  1.4934\n",
      "      2        \u001b[36m1.6492\u001b[0m       \u001b[32m0.1655\u001b[0m        1.6184        1.5150\n",
      "      3        \u001b[36m1.6379\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.6003\u001b[0m     +  1.4785\n",
      "      4        \u001b[36m1.6092\u001b[0m       0.2828        1.6015        1.5177\n",
      "      5        \u001b[36m1.6053\u001b[0m       \u001b[32m0.3172\u001b[0m        \u001b[35m1.5871\u001b[0m     +  1.4963\n",
      "      6        1.6113       \u001b[32m0.3448\u001b[0m        \u001b[35m1.5768\u001b[0m     +  1.5196\n",
      "      7        \u001b[36m1.5763\u001b[0m       0.3448        \u001b[35m1.5545\u001b[0m     +  1.5101\n",
      "      8        \u001b[36m1.5500\u001b[0m       0.2552        \u001b[35m1.5407\u001b[0m     +  1.5111\n",
      "      9        \u001b[36m1.5278\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.5236\u001b[0m     +  1.5136\n",
      "     10        \u001b[36m1.5252\u001b[0m       0.3172        \u001b[35m1.5056\u001b[0m     +  1.5214\n",
      "     11        \u001b[36m1.4888\u001b[0m       \u001b[32m0.3655\u001b[0m        \u001b[35m1.4707\u001b[0m     +  1.5079\n",
      "     12        \u001b[36m1.4652\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4523\u001b[0m     +  1.5199\n",
      "     13        \u001b[36m1.4374\u001b[0m       0.3862        \u001b[35m1.4090\u001b[0m     +  1.5068\n",
      "     14        \u001b[36m1.4283\u001b[0m       \u001b[32m0.4069\u001b[0m        1.4225        1.5152\n",
      "     15        \u001b[36m1.3917\u001b[0m       0.3931        \u001b[35m1.3997\u001b[0m     +  1.5157\n",
      "     16        \u001b[36m1.3685\u001b[0m       0.3931        \u001b[35m1.3920\u001b[0m     +  1.4995\n",
      "     17        \u001b[36m1.3056\u001b[0m       \u001b[32m0.4414\u001b[0m        \u001b[35m1.3808\u001b[0m     +  1.5168\n",
      "     18        \u001b[36m1.2588\u001b[0m       0.4276        \u001b[35m1.3416\u001b[0m     +  1.5068\n",
      "     19        \u001b[36m1.2095\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.3054\u001b[0m     +  1.5176\n",
      "     20        \u001b[36m1.1498\u001b[0m       \u001b[32m0.5034\u001b[0m        \u001b[35m1.2639\u001b[0m     +  1.5057\n",
      "     21        \u001b[36m1.0689\u001b[0m       0.4828        \u001b[35m1.2459\u001b[0m     +  1.4956\n",
      "     22        \u001b[36m1.0628\u001b[0m       \u001b[32m0.5310\u001b[0m        \u001b[35m1.2342\u001b[0m     +  1.5031\n",
      "     23        \u001b[36m1.0371\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.2132\u001b[0m     +  1.5148\n",
      "     24        \u001b[36m0.9662\u001b[0m       0.4966        1.2189        1.5135\n",
      "     25        \u001b[36m0.9261\u001b[0m       0.5103        1.2915        1.4981\n",
      "     26        \u001b[36m0.8838\u001b[0m       0.5379        1.3041        1.4764\n",
      "     27        \u001b[36m0.8304\u001b[0m       0.4966        1.4047        1.5013\n",
      "     28        \u001b[36m0.7911\u001b[0m       0.4483        1.7688        1.4789\n",
      "     29        \u001b[36m0.7512\u001b[0m       0.4690        1.9380        1.4752\n",
      "     30        \u001b[36m0.7377\u001b[0m       0.4483        1.9081        1.4764\n",
      "     31        \u001b[36m0.7035\u001b[0m       0.4897        1.4965        1.4944\n",
      "     32        \u001b[36m0.6277\u001b[0m       0.4897        2.3830        1.4987\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.529 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.3416\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5596\u001b[0m     +  1.6874\n",
      "      2        \u001b[36m1.6708\u001b[0m       \u001b[32m0.2639\u001b[0m        \u001b[35m1.5518\u001b[0m     +  1.7242\n",
      "      3        \u001b[36m1.6221\u001b[0m       \u001b[32m0.3819\u001b[0m        \u001b[35m1.4994\u001b[0m     +  1.7249\n",
      "      4        \u001b[36m1.5567\u001b[0m       \u001b[32m0.4444\u001b[0m        1.5037        1.7205\n",
      "      5        \u001b[36m1.5222\u001b[0m       0.3889        \u001b[35m1.4880\u001b[0m     +  1.6970\n",
      "      6        \u001b[36m1.5108\u001b[0m       0.4167        \u001b[35m1.4159\u001b[0m     +  1.7111\n",
      "      7        \u001b[36m1.4719\u001b[0m       0.4444        \u001b[35m1.3850\u001b[0m     +  1.7282\n",
      "      8        \u001b[36m1.4135\u001b[0m       0.4306        \u001b[35m1.3371\u001b[0m     +  1.7228\n",
      "      9        \u001b[36m1.3801\u001b[0m       0.3819        1.4151        1.7144\n",
      "     10        \u001b[36m1.3657\u001b[0m       \u001b[32m0.5069\u001b[0m        \u001b[35m1.3096\u001b[0m     +  1.7067\n",
      "     11        \u001b[36m1.2437\u001b[0m       0.4167        \u001b[35m1.2931\u001b[0m     +  1.7218\n",
      "     12        \u001b[36m1.1196\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m1.1719\u001b[0m     +  1.7183\n",
      "     13        \u001b[36m1.0104\u001b[0m       \u001b[32m0.5833\u001b[0m        \u001b[35m1.0765\u001b[0m     +  1.7035\n",
      "     14        \u001b[36m0.9247\u001b[0m       \u001b[32m0.5903\u001b[0m        1.0882        1.7155\n",
      "     15        \u001b[36m0.8429\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9938\u001b[0m     +  1.6783\n",
      "     16        \u001b[36m0.6929\u001b[0m       0.6181        1.0056        1.7101\n",
      "     17        \u001b[36m0.6880\u001b[0m       0.6250        \u001b[35m0.9654\u001b[0m     +  1.6773\n",
      "     18        \u001b[36m0.5612\u001b[0m       \u001b[32m0.6458\u001b[0m        1.0055        1.7045\n",
      "     19        \u001b[36m0.4505\u001b[0m       \u001b[32m0.6528\u001b[0m        1.0656        1.6768\n",
      "     20        \u001b[36m0.3933\u001b[0m       \u001b[32m0.7014\u001b[0m        0.9959        1.6765\n",
      "     21        \u001b[36m0.2804\u001b[0m       0.6944        1.0428        1.6834\n",
      "     22        \u001b[36m0.2672\u001b[0m       0.6597        1.0710        1.6811\n",
      "     23        \u001b[36m0.2255\u001b[0m       0.6875        1.1420        1.6816\n",
      "     24        \u001b[36m0.2089\u001b[0m       0.6458        1.4842        1.6809\n",
      "     25        \u001b[36m0.1822\u001b[0m       0.6250        1.5137        1.6969\n",
      "     26        \u001b[36m0.1685\u001b[0m       0.6111        1.5622        1.7017\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.434 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.8112\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.5283\u001b[0m     +  1.7032\n",
      "      2        \u001b[36m1.6576\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.4552\u001b[0m     +  1.7167\n",
      "      3        \u001b[36m1.5050\u001b[0m       \u001b[32m0.4759\u001b[0m        \u001b[35m1.3621\u001b[0m     +  1.7151\n",
      "      4        \u001b[36m1.3622\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.2770\u001b[0m     +  1.7137\n",
      "      5        \u001b[36m1.2191\u001b[0m       \u001b[32m0.5586\u001b[0m        \u001b[35m1.1027\u001b[0m     +  1.7229\n",
      "      6        \u001b[36m0.9496\u001b[0m       \u001b[32m0.6276\u001b[0m        \u001b[35m0.9572\u001b[0m     +  1.7107\n",
      "      7        \u001b[36m0.7431\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m0.8478\u001b[0m     +  1.7121\n",
      "      8        \u001b[36m0.4802\u001b[0m       0.6138        0.9590        1.7210\n",
      "      9        0.4871       \u001b[32m0.6690\u001b[0m        \u001b[35m0.8256\u001b[0m     +  1.6851\n",
      "     10        \u001b[36m0.3314\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m0.8064\u001b[0m     +  1.7200\n",
      "     11        \u001b[36m0.2197\u001b[0m       0.6828        \u001b[35m0.8033\u001b[0m     +  1.7177\n",
      "     12        \u001b[36m0.1631\u001b[0m       0.6483        1.0153        1.7179\n",
      "     13        \u001b[36m0.1257\u001b[0m       0.6690        1.0237        1.6813\n",
      "     14        \u001b[36m0.1157\u001b[0m       0.6483        1.2337        1.6805\n",
      "     15        \u001b[36m0.0868\u001b[0m       0.6759        1.0654        1.6811\n",
      "     16        \u001b[36m0.0565\u001b[0m       0.6966        1.1580        1.6843\n",
      "     17        0.0914       0.6897        1.0927        1.6808\n",
      "     18        0.0614       0.6759        1.2127        1.6828\n",
      "     19        \u001b[36m0.0343\u001b[0m       0.6759        1.1855        1.6819\n",
      "     20        0.0472       \u001b[32m0.7103\u001b[0m        1.0576        1.6810\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.288 total time=  52.2s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.7261\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m1.5957\u001b[0m     +  1.6888\n",
      "      2        \u001b[36m1.6979\u001b[0m       \u001b[32m0.3724\u001b[0m        \u001b[35m1.5069\u001b[0m     +  1.7040\n",
      "      3        \u001b[36m1.5403\u001b[0m       0.3724        \u001b[35m1.4700\u001b[0m     +  1.7275\n",
      "      4        \u001b[36m1.4762\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.3169\u001b[0m     +  1.7173\n",
      "      5        \u001b[36m1.3302\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.2127\u001b[0m     +  1.7209\n",
      "      6        \u001b[36m1.1275\u001b[0m       0.4690        \u001b[35m1.1932\u001b[0m     +  1.7152\n",
      "      7        \u001b[36m0.9585\u001b[0m       \u001b[32m0.6621\u001b[0m        \u001b[35m0.9594\u001b[0m     +  1.7137\n",
      "      8        \u001b[36m0.6815\u001b[0m       0.5931        1.0143        1.7074\n",
      "      9        \u001b[36m0.5240\u001b[0m       0.5517        1.1935        1.6819\n",
      "     10        \u001b[36m0.4177\u001b[0m       0.6138        1.0703        1.6904\n",
      "     11        \u001b[36m0.3646\u001b[0m       0.6138        1.1257        1.6863\n",
      "     12        \u001b[36m0.2954\u001b[0m       0.6552        0.9702        1.6877\n",
      "     13        \u001b[36m0.1978\u001b[0m       0.6414        1.0657        1.6853\n",
      "     14        \u001b[36m0.1665\u001b[0m       0.6552        1.1409        1.6876\n",
      "     15        \u001b[36m0.1048\u001b[0m       \u001b[32m0.6897\u001b[0m        0.9726        1.6856\n",
      "     16        \u001b[36m0.0988\u001b[0m       \u001b[32m0.6966\u001b[0m        1.0217        1.6854\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.931 total time=  41.0s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4611\u001b[0m       \u001b[32m0.2778\u001b[0m        \u001b[35m1.5795\u001b[0m     +  1.7075\n",
      "      2        \u001b[36m1.6557\u001b[0m       0.2431        1.5865        1.7218\n",
      "      3        \u001b[36m1.6153\u001b[0m       0.2708        \u001b[35m1.5752\u001b[0m     +  1.6838\n",
      "      4        \u001b[36m1.6104\u001b[0m       \u001b[32m0.3194\u001b[0m        \u001b[35m1.5507\u001b[0m     +  1.7170\n",
      "      5        \u001b[36m1.5751\u001b[0m       0.3194        \u001b[35m1.5186\u001b[0m     +  1.7153\n",
      "      6        \u001b[36m1.5628\u001b[0m       \u001b[32m0.3472\u001b[0m        \u001b[35m1.4935\u001b[0m     +  1.7096\n",
      "      7        \u001b[36m1.4904\u001b[0m       \u001b[32m0.3681\u001b[0m        \u001b[35m1.4261\u001b[0m     +  1.7161\n",
      "      8        \u001b[36m1.4244\u001b[0m       \u001b[32m0.3889\u001b[0m        \u001b[35m1.3670\u001b[0m     +  1.7202\n",
      "      9        \u001b[36m1.3605\u001b[0m       \u001b[32m0.4653\u001b[0m        \u001b[35m1.2715\u001b[0m     +  1.7180\n",
      "     10        \u001b[36m1.2585\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.2038\u001b[0m     +  1.7189\n",
      "     11        \u001b[36m1.1850\u001b[0m       0.4861        1.2229        1.7170\n",
      "     12        \u001b[36m0.9871\u001b[0m       0.5208        \u001b[35m1.1440\u001b[0m     +  1.6799\n",
      "     13        \u001b[36m0.9071\u001b[0m       \u001b[32m0.5903\u001b[0m        \u001b[35m1.0347\u001b[0m     +  1.7124\n",
      "     14        \u001b[36m0.7089\u001b[0m       \u001b[32m0.6597\u001b[0m        \u001b[35m1.0045\u001b[0m     +  1.7084\n",
      "     15        \u001b[36m0.5873\u001b[0m       0.6250        \u001b[35m0.9576\u001b[0m     +  1.7103\n",
      "     16        \u001b[36m0.5168\u001b[0m       \u001b[32m0.6667\u001b[0m        0.9903        1.7106\n",
      "     17        \u001b[36m0.3856\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.9371\u001b[0m     +  1.6815\n",
      "     18        \u001b[36m0.3181\u001b[0m       0.6875        0.9775        1.7087\n",
      "     19        \u001b[36m0.1736\u001b[0m       0.6736        1.2822        1.6809\n",
      "     20        \u001b[36m0.1463\u001b[0m       \u001b[32m0.7222\u001b[0m        1.2612        1.6780\n",
      "     21        \u001b[36m0.1332\u001b[0m       0.6806        1.3023        1.6780\n",
      "     22        \u001b[36m0.1001\u001b[0m       0.6528        1.5755        1.6790\n",
      "     23        0.1308       0.6875        1.4465        1.6838\n",
      "     24        0.1508       0.6806        1.5838        1.6785\n",
      "     25        0.1721       0.6250        1.9291        1.6778\n",
      "     26        0.1061       0.6458        1.7438        1.6786\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-2.003 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0265\u001b[0m       \u001b[32m0.3793\u001b[0m        \u001b[35m1.5725\u001b[0m     +  1.6954\n",
      "      2        \u001b[36m1.6002\u001b[0m       0.3241        \u001b[35m1.5062\u001b[0m     +  1.7237\n",
      "      3        \u001b[36m1.5743\u001b[0m       0.3172        \u001b[35m1.4905\u001b[0m     +  1.7181\n",
      "      4        \u001b[36m1.5636\u001b[0m       0.3793        1.5008        1.7226\n",
      "      5        \u001b[36m1.5026\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.4044\u001b[0m     +  1.6942\n",
      "      6        \u001b[36m1.4204\u001b[0m       \u001b[32m0.4483\u001b[0m        \u001b[35m1.3284\u001b[0m     +  1.7126\n",
      "      7        \u001b[36m1.3073\u001b[0m       \u001b[32m0.4759\u001b[0m        \u001b[35m1.2264\u001b[0m     +  1.7170\n",
      "      8        \u001b[36m1.2684\u001b[0m       0.4621        \u001b[35m1.1802\u001b[0m     +  1.7149\n",
      "      9        \u001b[36m1.0907\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.0548\u001b[0m     +  1.7220\n",
      "     10        \u001b[36m1.0106\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m1.0402\u001b[0m     +  1.7202\n",
      "     11        \u001b[36m0.8204\u001b[0m       \u001b[32m0.6276\u001b[0m        \u001b[35m0.9472\u001b[0m     +  1.7164\n",
      "     12        \u001b[36m0.7419\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.8639\u001b[0m     +  1.7177\n",
      "     13        \u001b[36m0.5368\u001b[0m       0.6621        0.9144        1.7295\n",
      "     14        \u001b[36m0.4946\u001b[0m       0.5862        1.3489        1.6879\n",
      "     15        \u001b[36m0.3465\u001b[0m       \u001b[32m0.7172\u001b[0m        0.8897        1.6878\n",
      "     16        \u001b[36m0.2508\u001b[0m       0.6414        1.2349        1.6856\n",
      "     17        \u001b[36m0.1882\u001b[0m       0.6897        1.1291        1.6880\n",
      "     18        \u001b[36m0.1704\u001b[0m       0.6207        1.6935        1.6875\n",
      "     19        \u001b[36m0.1324\u001b[0m       0.6690        1.2723        1.6855\n",
      "     20        0.1358       \u001b[32m0.7241\u001b[0m        1.2627        1.6885\n",
      "     21        \u001b[36m0.0711\u001b[0m       0.6966        1.4806        1.6880\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.433 total time=  55.7s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.2240\u001b[0m       \u001b[32m0.2759\u001b[0m        \u001b[35m1.6141\u001b[0m     +  1.6995\n",
      "      2        \u001b[36m1.7066\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.5916\u001b[0m     +  1.7118\n",
      "      3        \u001b[36m1.6149\u001b[0m       \u001b[32m0.3448\u001b[0m        \u001b[35m1.5471\u001b[0m     +  1.7128\n",
      "      4        \u001b[36m1.6120\u001b[0m       0.3379        \u001b[35m1.5340\u001b[0m     +  1.7260\n",
      "      5        \u001b[36m1.5620\u001b[0m       0.3448        \u001b[35m1.4708\u001b[0m     +  1.7165\n",
      "      6        \u001b[36m1.5573\u001b[0m       0.3448        1.4820        1.7156\n",
      "      7        \u001b[36m1.4869\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4637\u001b[0m     +  1.6889\n",
      "      8        \u001b[36m1.4383\u001b[0m       0.3931        \u001b[35m1.3191\u001b[0m     +  1.7176\n",
      "      9        \u001b[36m1.3574\u001b[0m       0.3793        1.4109        1.7189\n",
      "     10        \u001b[36m1.2949\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.2186\u001b[0m     +  1.6869\n",
      "     11        \u001b[36m1.1789\u001b[0m       0.3517        1.3132        1.7319\n",
      "     12        \u001b[36m1.0622\u001b[0m       \u001b[32m0.4690\u001b[0m        \u001b[35m1.1271\u001b[0m     +  1.7205\n",
      "     13        \u001b[36m0.8935\u001b[0m       \u001b[32m0.4759\u001b[0m        1.1989        1.7327\n",
      "     14        \u001b[36m0.8090\u001b[0m       \u001b[32m0.5586\u001b[0m        1.1631        1.7048\n",
      "     15        \u001b[36m0.6878\u001b[0m       \u001b[32m0.5793\u001b[0m        \u001b[35m1.0185\u001b[0m     +  1.6887\n",
      "     16        \u001b[36m0.5395\u001b[0m       \u001b[32m0.5862\u001b[0m        1.1128        1.7240\n",
      "     17        \u001b[36m0.4217\u001b[0m       \u001b[32m0.6414\u001b[0m        1.0888        1.6959\n",
      "     18        \u001b[36m0.3497\u001b[0m       \u001b[32m0.6621\u001b[0m        \u001b[35m0.9775\u001b[0m     +  1.6882\n",
      "     19        \u001b[36m0.1966\u001b[0m       \u001b[32m0.6828\u001b[0m        1.0486        1.7135\n",
      "     20        0.2010       0.6483        1.3455        1.6888\n",
      "     21        \u001b[36m0.1462\u001b[0m       \u001b[32m0.7172\u001b[0m        1.1601        1.6876\n",
      "     22        \u001b[36m0.1150\u001b[0m       0.6759        1.3616        1.6875\n",
      "     23        \u001b[36m0.0595\u001b[0m       0.6897        1.7920        1.6890\n",
      "     24        0.0602       0.6759        1.6148        1.6890\n",
      "     25        \u001b[36m0.0532\u001b[0m       0.6897        1.6267        1.6902\n",
      "     26        \u001b[36m0.0513\u001b[0m       0.6759        1.8035        1.6882\n",
      "     27        \u001b[36m0.0434\u001b[0m       0.6483        2.3584        1.6872\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.473 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9376\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5955\u001b[0m     +  1.7011\n",
      "      2        \u001b[36m1.6225\u001b[0m       0.2431        \u001b[35m1.5943\u001b[0m     +  1.7274\n",
      "      3        1.6237       0.2431        \u001b[35m1.5929\u001b[0m     +  1.7158\n",
      "      4        \u001b[36m1.6089\u001b[0m       \u001b[32m0.2500\u001b[0m        \u001b[35m1.5911\u001b[0m     +  1.7282\n",
      "      5        \u001b[36m1.5962\u001b[0m       0.2014        \u001b[35m1.5845\u001b[0m     +  1.7226\n",
      "      6        1.6049       0.2292        \u001b[35m1.5835\u001b[0m     +  1.7141\n",
      "      7        \u001b[36m1.5839\u001b[0m       0.2431        1.5852        1.7279\n",
      "      8        \u001b[36m1.5781\u001b[0m       0.2500        \u001b[35m1.5698\u001b[0m     +  1.6890\n",
      "      9        1.5835       \u001b[32m0.2569\u001b[0m        1.5739        1.7198\n",
      "     10        \u001b[36m1.5550\u001b[0m       0.2431        \u001b[35m1.5479\u001b[0m     +  1.6996\n",
      "     11        \u001b[36m1.5083\u001b[0m       0.2569        \u001b[35m1.5219\u001b[0m     +  1.7141\n",
      "     12        \u001b[36m1.4626\u001b[0m       0.2500        \u001b[35m1.4930\u001b[0m     +  1.7160\n",
      "     13        \u001b[36m1.4504\u001b[0m       \u001b[32m0.2708\u001b[0m        \u001b[35m1.4598\u001b[0m     +  1.7227\n",
      "     14        \u001b[36m1.3798\u001b[0m       \u001b[32m0.2917\u001b[0m        \u001b[35m1.4334\u001b[0m     +  1.7227\n",
      "     15        \u001b[36m1.3351\u001b[0m       0.2917        \u001b[35m1.4227\u001b[0m     +  1.7108\n",
      "     16        \u001b[36m1.2576\u001b[0m       \u001b[32m0.3681\u001b[0m        \u001b[35m1.4109\u001b[0m     +  1.7039\n",
      "     17        \u001b[36m1.2225\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.3694\u001b[0m     +  1.7228\n",
      "     18        \u001b[36m1.1187\u001b[0m       0.3472        1.4118        1.7203\n",
      "     19        \u001b[36m1.0637\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.2909\u001b[0m     +  1.6848\n",
      "     20        \u001b[36m0.9658\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.2303\u001b[0m     +  1.7210\n",
      "     21        \u001b[36m0.8563\u001b[0m       \u001b[32m0.5903\u001b[0m        1.4300        1.7090\n",
      "     22        \u001b[36m0.7505\u001b[0m       \u001b[32m0.6181\u001b[0m        1.4368        1.6810\n",
      "     23        \u001b[36m0.6678\u001b[0m       0.6111        1.2538        1.6810\n",
      "     24        \u001b[36m0.5999\u001b[0m       0.6111        1.4222        1.6828\n",
      "     25        0.6108       0.5556        1.3626        1.6842\n",
      "     26        \u001b[36m0.5756\u001b[0m       0.5764        1.4208        1.6798\n",
      "     27        \u001b[36m0.5530\u001b[0m       0.5486        1.4806        1.6796\n",
      "     28        \u001b[36m0.5095\u001b[0m       0.5139        1.3380        1.6809\n",
      "     29        \u001b[36m0.4900\u001b[0m       0.4514        2.4949        1.6791\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.246 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9606\u001b[0m       \u001b[32m0.1655\u001b[0m        \u001b[35m1.5998\u001b[0m     +  1.6942\n",
      "      2        \u001b[36m1.6619\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5997\u001b[0m     +  1.7222\n",
      "      3        \u001b[36m1.6350\u001b[0m       \u001b[32m0.2552\u001b[0m        \u001b[35m1.5862\u001b[0m     +  1.7278\n",
      "      4        1.6388       \u001b[32m0.2897\u001b[0m        1.5889        1.7132\n",
      "      5        \u001b[36m1.6218\u001b[0m       \u001b[32m0.2966\u001b[0m        1.5863        1.6871\n",
      "      6        \u001b[36m1.6144\u001b[0m       0.2552        1.6030        1.6908\n",
      "      7        \u001b[36m1.6013\u001b[0m       0.2690        \u001b[35m1.5711\u001b[0m     +  1.6870\n",
      "      8        \u001b[36m1.5740\u001b[0m       0.2276        \u001b[35m1.5565\u001b[0m     +  1.7301\n",
      "      9        \u001b[36m1.5546\u001b[0m       0.2621        1.5609        1.7171\n",
      "     10        1.5683       0.2690        \u001b[35m1.5304\u001b[0m     +  1.6888\n",
      "     11        \u001b[36m1.5268\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.5217\u001b[0m     +  1.7323\n",
      "     12        \u001b[36m1.4714\u001b[0m       0.3034        \u001b[35m1.4560\u001b[0m     +  1.7324\n",
      "     13        \u001b[36m1.4401\u001b[0m       0.4138        \u001b[35m1.4017\u001b[0m     +  1.7177\n",
      "     14        \u001b[36m1.3750\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.3412\u001b[0m     +  1.7282\n",
      "     15        \u001b[36m1.2884\u001b[0m       \u001b[32m0.5448\u001b[0m        1.3506        1.7154\n",
      "     16        \u001b[36m1.1636\u001b[0m       0.5310        \u001b[35m1.2081\u001b[0m     +  1.6887\n",
      "     17        \u001b[36m1.0834\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.1365\u001b[0m     +  1.7250\n",
      "     18        \u001b[36m1.0311\u001b[0m       0.5655        \u001b[35m1.0909\u001b[0m     +  1.7197\n",
      "     19        \u001b[36m0.9150\u001b[0m       0.4759        \u001b[35m1.0850\u001b[0m     +  1.7277\n",
      "     20        \u001b[36m0.8919\u001b[0m       0.5241        \u001b[35m1.0344\u001b[0m     +  1.7353\n",
      "     21        \u001b[36m0.8451\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.9817\u001b[0m     +  1.7210\n",
      "     22        \u001b[36m0.7406\u001b[0m       \u001b[32m0.6690\u001b[0m        \u001b[35m0.9540\u001b[0m     +  1.7274\n",
      "     23        \u001b[36m0.6032\u001b[0m       0.5724        1.0096        1.7263\n",
      "     24        \u001b[36m0.5456\u001b[0m       0.5931        1.0311        1.6921\n",
      "     25        \u001b[36m0.4937\u001b[0m       0.6345        \u001b[35m0.8995\u001b[0m     +  1.6913\n",
      "     26        \u001b[36m0.4143\u001b[0m       0.6138        1.0285        1.7304\n",
      "     27        \u001b[36m0.3404\u001b[0m       0.6483        0.9906        1.7087\n",
      "     28        \u001b[36m0.3109\u001b[0m       \u001b[32m0.6828\u001b[0m        1.0858        1.7016\n",
      "     29        0.3241       0.5034        1.6219        1.6930\n",
      "     30        \u001b[36m0.2390\u001b[0m       \u001b[32m0.6897\u001b[0m        0.9433        1.6984\n",
      "     31        \u001b[36m0.1929\u001b[0m       \u001b[32m0.7172\u001b[0m        0.9946        1.6961\n",
      "     32        \u001b[36m0.1911\u001b[0m       0.6207        1.3101        1.6913\n",
      "     33        \u001b[36m0.1539\u001b[0m       \u001b[32m0.7310\u001b[0m        1.1032        1.6946\n",
      "     34        \u001b[36m0.1206\u001b[0m       0.6552        1.3790        1.6928\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.513 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8324\u001b[0m       \u001b[32m0.2069\u001b[0m        \u001b[35m1.5971\u001b[0m     +  1.7017\n",
      "      2        \u001b[36m1.6412\u001b[0m       0.2000        1.6063        1.7147\n",
      "      3        \u001b[36m1.6329\u001b[0m       \u001b[32m0.2138\u001b[0m        1.6009        1.6940\n",
      "      4        \u001b[36m1.6146\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5866\u001b[0m     +  1.6945\n",
      "      5        1.6172       0.2276        1.6003        1.7176\n",
      "      6        \u001b[36m1.5887\u001b[0m       0.2276        1.5926        1.6911\n",
      "      7        \u001b[36m1.5882\u001b[0m       \u001b[32m0.3172\u001b[0m        \u001b[35m1.5809\u001b[0m     +  1.6894\n",
      "      8        \u001b[36m1.5754\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.5760\u001b[0m     +  1.7087\n",
      "      9        \u001b[36m1.5288\u001b[0m       0.3448        \u001b[35m1.5440\u001b[0m     +  1.7202\n",
      "     10        \u001b[36m1.4875\u001b[0m       0.3448        \u001b[35m1.5250\u001b[0m     +  1.7280\n",
      "     11        \u001b[36m1.4542\u001b[0m       0.3241        1.5280        1.7164\n",
      "     12        \u001b[36m1.4301\u001b[0m       0.3448        \u001b[35m1.4258\u001b[0m     +  1.6904\n",
      "     13        \u001b[36m1.4029\u001b[0m       0.3172        \u001b[35m1.4240\u001b[0m     +  1.7172\n",
      "     14        \u001b[36m1.3759\u001b[0m       0.3448        \u001b[35m1.3822\u001b[0m     +  1.7183\n",
      "     15        \u001b[36m1.3527\u001b[0m       0.3103        \u001b[35m1.3556\u001b[0m     +  1.7232\n",
      "     16        \u001b[36m1.3381\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.3523\u001b[0m     +  1.7172\n",
      "     17        \u001b[36m1.2651\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.3507\u001b[0m     +  1.7168\n",
      "     18        \u001b[36m1.2607\u001b[0m       0.3931        \u001b[35m1.3258\u001b[0m     +  1.7150\n",
      "     19        \u001b[36m1.2456\u001b[0m       \u001b[32m0.4276\u001b[0m        1.3609        1.7089\n",
      "     20        \u001b[36m1.2241\u001b[0m       0.4069        1.3749        1.6892\n",
      "     21        1.2659       0.3586        1.3622        1.6890\n",
      "     22        1.3028       0.4069        \u001b[35m1.3196\u001b[0m     +  1.6921\n",
      "     23        \u001b[36m1.2087\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.2832\u001b[0m     +  1.7268\n",
      "     24        \u001b[36m1.1920\u001b[0m       0.4276        1.2925        1.7196\n",
      "     25        1.1966       0.4138        1.3062        1.6897\n",
      "     26        \u001b[36m1.1815\u001b[0m       0.4138        1.3073        1.6885\n",
      "     27        \u001b[36m1.1432\u001b[0m       \u001b[32m0.4552\u001b[0m        1.3229        1.6908\n",
      "     28        \u001b[36m1.1424\u001b[0m       0.4276        1.3230        1.6920\n",
      "     29        \u001b[36m1.0799\u001b[0m       0.4483        1.3249        1.6895\n",
      "     30        \u001b[36m1.0717\u001b[0m       0.3586        1.3178        1.6904\n",
      "     31        1.0830       \u001b[32m0.4966\u001b[0m        \u001b[35m1.2780\u001b[0m     +  1.6904\n",
      "     32        \u001b[36m1.0373\u001b[0m       0.4621        \u001b[35m1.2325\u001b[0m     +  1.7253\n",
      "     33        \u001b[36m0.9642\u001b[0m       \u001b[32m0.5172\u001b[0m        1.2468        1.7241\n",
      "     34        \u001b[36m0.9284\u001b[0m       \u001b[32m0.5241\u001b[0m        1.2684        1.6910\n",
      "     35        0.9487       0.5172        \u001b[35m1.2240\u001b[0m     +  1.6926\n",
      "     36        \u001b[36m0.9079\u001b[0m       0.4966        1.3677        1.7181\n",
      "     37        0.9518       \u001b[32m0.5379\u001b[0m        1.3125        1.6925\n",
      "     38        \u001b[36m0.8895\u001b[0m       \u001b[32m0.5448\u001b[0m        1.3312        1.6916\n",
      "     39        \u001b[36m0.8398\u001b[0m       0.4828        1.4298        1.6932\n",
      "     40        \u001b[36m0.8380\u001b[0m       0.5241        1.2298        1.6945\n",
      "     41        \u001b[36m0.8363\u001b[0m       \u001b[32m0.5862\u001b[0m        \u001b[35m1.1679\u001b[0m     +  1.6903\n",
      "     42        \u001b[36m0.7926\u001b[0m       0.4828        1.5464        1.7217\n",
      "     43        0.8190       0.5448        1.1844        1.6943\n",
      "     44        \u001b[36m0.7320\u001b[0m       0.5310        1.4452        1.6914\n",
      "     45        0.7336       0.4897        2.4069        1.6905\n",
      "     46        \u001b[36m0.7260\u001b[0m       0.5241        1.3956        1.6940\n",
      "     47        \u001b[36m0.6813\u001b[0m       0.5448        1.2891        1.6994\n",
      "     48        \u001b[36m0.6679\u001b[0m       0.5724        1.4768        1.6987\n",
      "     49        \u001b[36m0.6245\u001b[0m       \u001b[32m0.6069\u001b[0m        1.2230        1.6953\n",
      "     50        \u001b[36m0.5938\u001b[0m       0.5172        1.8755        1.6941\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.650 total time= 2.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.2725\u001b[0m       \u001b[32m0.2708\u001b[0m        \u001b[35m1.5163\u001b[0m     +  2.0989\n",
      "      2        \u001b[36m1.6605\u001b[0m       \u001b[32m0.4028\u001b[0m        \u001b[35m1.4408\u001b[0m     +  2.1446\n",
      "      3        \u001b[36m1.5708\u001b[0m       0.3472        \u001b[35m1.4222\u001b[0m     +  2.1399\n",
      "      4        \u001b[36m1.4752\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.3507\u001b[0m     +  2.1340\n",
      "      5        \u001b[36m1.3165\u001b[0m       \u001b[32m0.5347\u001b[0m        \u001b[35m1.2043\u001b[0m     +  2.1348\n",
      "      6        \u001b[36m1.1101\u001b[0m       \u001b[32m0.5972\u001b[0m        \u001b[35m1.0709\u001b[0m     +  2.1494\n",
      "      7        \u001b[36m0.9188\u001b[0m       \u001b[32m0.6181\u001b[0m        \u001b[35m1.0170\u001b[0m     +  2.1294\n",
      "      8        \u001b[36m0.7459\u001b[0m       \u001b[32m0.6458\u001b[0m        \u001b[35m0.9288\u001b[0m     +  2.1397\n",
      "      9        \u001b[36m0.5068\u001b[0m       0.6389        1.0107        2.1413\n",
      "     10        \u001b[36m0.4278\u001b[0m       \u001b[32m0.6597\u001b[0m        0.9581        2.1130\n",
      "     11        \u001b[36m0.2897\u001b[0m       \u001b[32m0.6806\u001b[0m        1.0886        2.1098\n",
      "     12        \u001b[36m0.2409\u001b[0m       0.6458        1.2556        2.1101\n",
      "     13        \u001b[36m0.2399\u001b[0m       0.6458        1.0874        2.1094\n",
      "     14        \u001b[36m0.2042\u001b[0m       \u001b[32m0.6944\u001b[0m        1.0530        2.1095\n",
      "     15        \u001b[36m0.1799\u001b[0m       0.6042        1.5690        2.1102\n",
      "     16        \u001b[36m0.1254\u001b[0m       0.6111        1.8584        2.1101\n",
      "     17        \u001b[36m0.1177\u001b[0m       0.6875        1.2846        2.1126\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.349 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.5719\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.4861\u001b[0m     +  2.1187\n",
      "      2        \u001b[36m1.6369\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4158\u001b[0m     +  2.1464\n",
      "      3        \u001b[36m1.4377\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.2808\u001b[0m     +  2.1530\n",
      "      4        \u001b[36m1.1840\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.1047\u001b[0m     +  2.1437\n",
      "      5        \u001b[36m0.9591\u001b[0m       \u001b[32m0.6621\u001b[0m        \u001b[35m0.9483\u001b[0m     +  2.1421\n",
      "      6        \u001b[36m0.6722\u001b[0m       0.6276        \u001b[35m0.9082\u001b[0m     +  2.1476\n",
      "      7        \u001b[36m0.4564\u001b[0m       0.6552        \u001b[35m0.8904\u001b[0m     +  2.1520\n",
      "      8        \u001b[36m0.2329\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m0.8811\u001b[0m     +  2.1486\n",
      "      9        \u001b[36m0.1552\u001b[0m       0.6690        1.1114        2.1372\n",
      "     10        \u001b[36m0.1005\u001b[0m       \u001b[32m0.6897\u001b[0m        1.1454        2.1136\n",
      "     11        \u001b[36m0.0985\u001b[0m       \u001b[32m0.6966\u001b[0m        1.0634        2.1139\n",
      "     12        \u001b[36m0.0660\u001b[0m       0.6621        1.1190        2.1157\n",
      "     13        \u001b[36m0.0462\u001b[0m       0.6828        1.4442        2.1229\n",
      "     14        0.0486       \u001b[32m0.7103\u001b[0m        1.0654        2.1199\n",
      "     15        \u001b[36m0.0433\u001b[0m       0.6621        1.3778        2.1180\n",
      "     16        \u001b[36m0.0353\u001b[0m       0.7103        1.2668        2.1182\n",
      "     17        0.0857       0.6966        1.2282        2.1198\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-2.078 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.2982\u001b[0m       \u001b[32m0.3793\u001b[0m        \u001b[35m1.5616\u001b[0m     +  2.1216\n",
      "      2        \u001b[36m1.6612\u001b[0m       0.3655        \u001b[35m1.5140\u001b[0m     +  2.1568\n",
      "      3        \u001b[36m1.5622\u001b[0m       0.3724        \u001b[35m1.4256\u001b[0m     +  2.1397\n",
      "      4        \u001b[36m1.4495\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.3537\u001b[0m     +  2.1520\n",
      "      5        \u001b[36m1.3296\u001b[0m       0.3793        1.3581        2.1406\n",
      "      6        \u001b[36m1.2628\u001b[0m       \u001b[32m0.4483\u001b[0m        \u001b[35m1.1941\u001b[0m     +  2.1178\n",
      "      7        \u001b[36m1.1556\u001b[0m       0.3862        1.3960        2.1542\n",
      "      8        \u001b[36m1.0126\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.1871\u001b[0m     +  2.1162\n",
      "      9        \u001b[36m0.7026\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m1.0628\u001b[0m     +  2.1492\n",
      "     10        \u001b[36m0.5837\u001b[0m       0.6207        \u001b[35m1.0584\u001b[0m     +  2.1492\n",
      "     11        \u001b[36m0.3282\u001b[0m       \u001b[32m0.6621\u001b[0m        \u001b[35m0.9932\u001b[0m     +  2.1418\n",
      "     12        \u001b[36m0.2462\u001b[0m       0.6138        1.2176        2.1417\n",
      "     13        \u001b[36m0.1602\u001b[0m       0.6414        1.1818        2.1163\n",
      "     14        \u001b[36m0.1249\u001b[0m       \u001b[32m0.6690\u001b[0m        1.2112        2.1163\n",
      "     15        \u001b[36m0.0701\u001b[0m       \u001b[32m0.6966\u001b[0m        1.1501        2.1186\n",
      "     16        \u001b[36m0.0560\u001b[0m       0.6966        1.2109        2.1174\n",
      "     17        \u001b[36m0.0490\u001b[0m       0.6621        1.3839        2.1174\n",
      "     18        \u001b[36m0.0218\u001b[0m       0.6621        1.5865        2.1174\n",
      "     19        0.0220       0.6414        1.5820        2.1174\n",
      "     20        \u001b[36m0.0077\u001b[0m       0.6345        1.5545        2.1206\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-2.276 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.6713\u001b[0m       \u001b[32m0.2153\u001b[0m        \u001b[35m1.6030\u001b[0m     +  2.1146\n",
      "      2        \u001b[36m1.6742\u001b[0m       \u001b[32m0.2986\u001b[0m        \u001b[35m1.5578\u001b[0m     +  2.1533\n",
      "      3        \u001b[36m1.6516\u001b[0m       \u001b[32m0.3056\u001b[0m        \u001b[35m1.5245\u001b[0m     +  2.1357\n",
      "      4        \u001b[36m1.5462\u001b[0m       \u001b[32m0.3472\u001b[0m        \u001b[35m1.4856\u001b[0m     +  2.1438\n",
      "      5        1.5651       \u001b[32m0.3681\u001b[0m        \u001b[35m1.4400\u001b[0m     +  2.1430\n",
      "      6        \u001b[36m1.4725\u001b[0m       \u001b[32m0.3819\u001b[0m        \u001b[35m1.3631\u001b[0m     +  2.1317\n",
      "      7        \u001b[36m1.3580\u001b[0m       \u001b[32m0.4236\u001b[0m        \u001b[35m1.2726\u001b[0m     +  2.1563\n",
      "      8        \u001b[36m1.2144\u001b[0m       \u001b[32m0.4861\u001b[0m        \u001b[35m1.1969\u001b[0m     +  2.1544\n",
      "      9        \u001b[36m1.0322\u001b[0m       \u001b[32m0.5278\u001b[0m        \u001b[35m1.1355\u001b[0m     +  2.1483\n",
      "     10        \u001b[36m0.7963\u001b[0m       \u001b[32m0.6042\u001b[0m        \u001b[35m1.0168\u001b[0m     +  2.1517\n",
      "     11        \u001b[36m0.6029\u001b[0m       \u001b[32m0.6319\u001b[0m        \u001b[35m0.9692\u001b[0m     +  2.1456\n",
      "     12        \u001b[36m0.4603\u001b[0m       \u001b[32m0.6389\u001b[0m        1.0023        2.1568\n",
      "     13        \u001b[36m0.3294\u001b[0m       \u001b[32m0.6944\u001b[0m        1.0158        2.1490\n",
      "     14        \u001b[36m0.2796\u001b[0m       0.6458        1.1079        2.1164\n",
      "     15        \u001b[36m0.1788\u001b[0m       0.6806        1.1147        2.1148\n",
      "     16        \u001b[36m0.1519\u001b[0m       0.6806        1.2308        2.1128\n",
      "     17        0.1779       0.6250        1.1932        2.1138\n",
      "     18        \u001b[36m0.1437\u001b[0m       0.6806        1.3763        2.1131\n",
      "     19        \u001b[36m0.1329\u001b[0m       0.6250        1.6066        2.1138\n",
      "     20        0.1898       0.6319        1.7057        2.1141\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.453 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.5503\u001b[0m       \u001b[32m0.3448\u001b[0m        \u001b[35m1.5753\u001b[0m     +  2.1260\n",
      "      2        \u001b[36m1.6399\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.5641\u001b[0m     +  2.1469\n",
      "      3        \u001b[36m1.6171\u001b[0m       0.3862        \u001b[35m1.5122\u001b[0m     +  2.1570\n",
      "      4        \u001b[36m1.5791\u001b[0m       0.3241        \u001b[35m1.4957\u001b[0m     +  2.1512\n",
      "      5        \u001b[36m1.5514\u001b[0m       0.3586        \u001b[35m1.3936\u001b[0m     +  2.1581\n",
      "      6        \u001b[36m1.4588\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.3710\u001b[0m     +  2.1541\n",
      "      7        \u001b[36m1.4038\u001b[0m       \u001b[32m0.4276\u001b[0m        \u001b[35m1.2723\u001b[0m     +  2.1600\n",
      "      8        \u001b[36m1.3000\u001b[0m       \u001b[32m0.4483\u001b[0m        \u001b[35m1.2101\u001b[0m     +  2.1701\n",
      "      9        \u001b[36m1.1528\u001b[0m       \u001b[32m0.5103\u001b[0m        \u001b[35m1.1320\u001b[0m     +  2.1581\n",
      "     10        \u001b[36m0.9999\u001b[0m       \u001b[32m0.5862\u001b[0m        \u001b[35m1.0044\u001b[0m     +  2.1691\n",
      "     11        \u001b[36m0.8785\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m0.9470\u001b[0m     +  2.1715\n",
      "     12        \u001b[36m0.7181\u001b[0m       \u001b[32m0.6483\u001b[0m        \u001b[35m0.8815\u001b[0m     +  2.1615\n",
      "     13        \u001b[36m0.5972\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.8078\u001b[0m     +  2.1511\n",
      "     14        \u001b[36m0.4888\u001b[0m       0.6138        1.0038        2.1612\n",
      "     15        \u001b[36m0.4366\u001b[0m       0.6207        1.1185        2.1336\n",
      "     16        \u001b[36m0.3894\u001b[0m       0.6897        0.9114        2.1234\n",
      "     17        \u001b[36m0.2469\u001b[0m       \u001b[32m0.6966\u001b[0m        0.9488        2.1208\n",
      "     18        \u001b[36m0.1710\u001b[0m       0.6276        1.3004        2.1234\n",
      "     19        0.1778       0.5586        1.8055        2.1512\n",
      "     20        \u001b[36m0.1292\u001b[0m       \u001b[32m0.7103\u001b[0m        1.1062        2.1270\n",
      "     21        \u001b[36m0.0911\u001b[0m       0.6828        1.5966        2.1220\n",
      "     22        \u001b[36m0.0677\u001b[0m       0.6759        1.3306        2.1229\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.375 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.3595\u001b[0m       \u001b[32m0.2690\u001b[0m        \u001b[35m1.5869\u001b[0m     +  2.1313\n",
      "      2        \u001b[36m1.6886\u001b[0m       \u001b[32m0.2897\u001b[0m        \u001b[35m1.5854\u001b[0m     +  2.1708\n",
      "      3        \u001b[36m1.6357\u001b[0m       \u001b[32m0.3655\u001b[0m        \u001b[35m1.5233\u001b[0m     +  2.1604\n",
      "      4        \u001b[36m1.5905\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.4892\u001b[0m     +  2.1437\n",
      "      5        \u001b[36m1.5347\u001b[0m       0.3724        \u001b[35m1.4550\u001b[0m     +  2.1669\n",
      "      6        \u001b[36m1.4470\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.3953\u001b[0m     +  2.1657\n",
      "      7        \u001b[36m1.3456\u001b[0m       \u001b[32m0.4000\u001b[0m        1.4121        2.1551\n",
      "      8        \u001b[36m1.2976\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.1694\u001b[0m     +  2.1205\n",
      "      9        \u001b[36m1.2046\u001b[0m       \u001b[32m0.5241\u001b[0m        1.2090        2.1622\n",
      "     10        \u001b[36m1.0134\u001b[0m       0.4621        1.2188        2.1269\n",
      "     11        \u001b[36m0.9004\u001b[0m       0.4966        1.3580        2.1211\n",
      "     12        \u001b[36m0.7697\u001b[0m       0.5172        1.2082        2.1213\n",
      "     13        \u001b[36m0.7003\u001b[0m       \u001b[32m0.5517\u001b[0m        1.2919        2.1242\n",
      "     14        \u001b[36m0.5314\u001b[0m       \u001b[32m0.5655\u001b[0m        1.3094        2.1213\n",
      "     15        \u001b[36m0.3162\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.1421\u001b[0m     +  2.1265\n",
      "     16        \u001b[36m0.2210\u001b[0m       0.5793        1.4110        2.1498\n",
      "     17        \u001b[36m0.1451\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m1.0956\u001b[0m     +  2.1198\n",
      "     18        \u001b[36m0.1039\u001b[0m       0.7103        1.2262        2.1542\n",
      "     19        \u001b[36m0.0932\u001b[0m       0.6966        1.1800        2.1273\n",
      "     20        \u001b[36m0.0623\u001b[0m       \u001b[32m0.7310\u001b[0m        1.2872        2.1257\n",
      "     21        \u001b[36m0.0564\u001b[0m       \u001b[32m0.7448\u001b[0m        1.3790        2.1377\n",
      "     22        \u001b[36m0.0266\u001b[0m       0.6759        1.6212        2.1221\n",
      "     23        \u001b[36m0.0219\u001b[0m       0.6759        1.6283        2.1385\n",
      "     24        0.0323       0.6828        1.6381        2.1266\n",
      "     25        0.0489       0.7103        1.4906        2.1215\n",
      "     26        0.0455       0.6345        1.8778        2.1231\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.284 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.2140\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.6097\u001b[0m     +  2.1167\n",
      "      2        \u001b[36m1.7012\u001b[0m       0.2222        \u001b[35m1.5944\u001b[0m     +  2.1544\n",
      "      3        \u001b[36m1.6502\u001b[0m       0.2431        \u001b[35m1.5823\u001b[0m     +  2.1491\n",
      "      4        \u001b[36m1.6123\u001b[0m       \u001b[32m0.2986\u001b[0m        \u001b[35m1.5686\u001b[0m     +  2.1422\n",
      "      5        \u001b[36m1.5970\u001b[0m       0.2569        1.5746        2.1448\n",
      "      6        \u001b[36m1.5766\u001b[0m       0.2500        \u001b[35m1.5459\u001b[0m     +  2.1224\n",
      "      7        \u001b[36m1.5386\u001b[0m       0.2847        \u001b[35m1.5123\u001b[0m     +  2.1495\n",
      "      8        \u001b[36m1.5050\u001b[0m       \u001b[32m0.3403\u001b[0m        \u001b[35m1.4731\u001b[0m     +  2.1505\n",
      "      9        \u001b[36m1.4541\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.4272\u001b[0m     +  2.1451\n",
      "     10        \u001b[36m1.3702\u001b[0m       \u001b[32m0.4167\u001b[0m        \u001b[35m1.3375\u001b[0m     +  2.1510\n",
      "     11        \u001b[36m1.2433\u001b[0m       0.3611        \u001b[35m1.2785\u001b[0m     +  2.1474\n",
      "     12        \u001b[36m1.1500\u001b[0m       \u001b[32m0.4583\u001b[0m        \u001b[35m1.2429\u001b[0m     +  2.1433\n",
      "     13        \u001b[36m1.0502\u001b[0m       \u001b[32m0.5069\u001b[0m        1.2559        2.1582\n",
      "     14        \u001b[36m0.9219\u001b[0m       0.4722        1.3886        2.1334\n",
      "     15        \u001b[36m0.8429\u001b[0m       \u001b[32m0.5972\u001b[0m        \u001b[35m1.1604\u001b[0m     +  2.1139\n",
      "     16        \u001b[36m0.7511\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m1.1122\u001b[0m     +  2.1491\n",
      "     17        \u001b[36m0.5320\u001b[0m       \u001b[32m0.6458\u001b[0m        \u001b[35m1.0493\u001b[0m     +  2.1518\n",
      "     18        \u001b[36m0.4886\u001b[0m       0.6250        1.2614        2.1474\n",
      "     19        \u001b[36m0.3964\u001b[0m       0.5903        1.5144        2.1148\n",
      "     20        0.5764       0.6389        1.1927        2.1146\n",
      "     21        \u001b[36m0.3078\u001b[0m       \u001b[32m0.6528\u001b[0m        1.1196        2.1164\n",
      "     22        \u001b[36m0.1721\u001b[0m       \u001b[32m0.6736\u001b[0m        1.3457        2.1162\n",
      "     23        0.1970       \u001b[32m0.7083\u001b[0m        1.1273        2.1183\n",
      "     24        \u001b[36m0.1231\u001b[0m       0.7083        1.2211        2.1200\n",
      "     25        \u001b[36m0.0757\u001b[0m       \u001b[32m0.7153\u001b[0m        1.3168        2.1163\n",
      "     26        \u001b[36m0.0506\u001b[0m       0.7014        1.3536        2.1172\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.110 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0029\u001b[0m       \u001b[32m0.1586\u001b[0m        \u001b[35m1.5953\u001b[0m     +  2.1219\n",
      "      2        \u001b[36m1.6766\u001b[0m       \u001b[32m0.2345\u001b[0m        1.6041        2.1495\n",
      "      3        \u001b[36m1.6231\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m1.5829\u001b[0m     +  2.1245\n",
      "      4        \u001b[36m1.6038\u001b[0m       \u001b[32m0.4276\u001b[0m        \u001b[35m1.5603\u001b[0m     +  2.1480\n",
      "      5        \u001b[36m1.6019\u001b[0m       0.3517        1.5715        2.1560\n",
      "      6        \u001b[36m1.5929\u001b[0m       0.3379        \u001b[35m1.5239\u001b[0m     +  2.1249\n",
      "      7        \u001b[36m1.5703\u001b[0m       0.3793        \u001b[35m1.5213\u001b[0m     +  2.1508\n",
      "      8        \u001b[36m1.5077\u001b[0m       0.4000        \u001b[35m1.4233\u001b[0m     +  2.1563\n",
      "      9        \u001b[36m1.4799\u001b[0m       0.3655        \u001b[35m1.3453\u001b[0m     +  2.1601\n",
      "     10        \u001b[36m1.3337\u001b[0m       0.3793        \u001b[35m1.2702\u001b[0m     +  2.1530\n",
      "     11        \u001b[36m1.2563\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m1.1949\u001b[0m     +  2.1570\n",
      "     12        \u001b[36m1.1811\u001b[0m       0.4483        1.2121        2.1680\n",
      "     13        \u001b[36m1.0377\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.0361\u001b[0m     +  2.1257\n",
      "     14        \u001b[36m0.9435\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.0354\u001b[0m     +  2.1541\n",
      "     15        \u001b[36m0.8468\u001b[0m       0.5310        \u001b[35m1.0172\u001b[0m     +  2.1538\n",
      "     16        \u001b[36m0.7126\u001b[0m       \u001b[32m0.5793\u001b[0m        \u001b[35m0.9613\u001b[0m     +  2.1559\n",
      "     17        0.7377       0.5793        1.2037        2.1651\n",
      "     18        \u001b[36m0.6845\u001b[0m       \u001b[32m0.6483\u001b[0m        \u001b[35m0.8767\u001b[0m     +  2.1351\n",
      "     19        \u001b[36m0.4511\u001b[0m       \u001b[32m0.6552\u001b[0m        1.0752        2.1724\n",
      "     20        \u001b[36m0.4117\u001b[0m       0.6069        1.1299        2.1392\n",
      "     21        \u001b[36m0.3367\u001b[0m       0.6483        1.4061        2.1261\n",
      "     22        \u001b[36m0.2612\u001b[0m       0.6414        1.2731        2.1294\n",
      "     23        \u001b[36m0.2426\u001b[0m       \u001b[32m0.6621\u001b[0m        1.1126        2.1285\n",
      "     24        \u001b[36m0.1123\u001b[0m       \u001b[32m0.6759\u001b[0m        1.2998        2.1257\n",
      "     25        \u001b[36m0.0659\u001b[0m       \u001b[32m0.6897\u001b[0m        1.3245        2.1287\n",
      "     26        \u001b[36m0.0483\u001b[0m       \u001b[32m0.6966\u001b[0m        1.2927        2.1306\n",
      "     27        0.1037       \u001b[32m0.7034\u001b[0m        1.4268        2.1292\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.538 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9228\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5918\u001b[0m     +  2.1410\n",
      "      2        \u001b[36m1.6559\u001b[0m       \u001b[32m0.2759\u001b[0m        \u001b[35m1.5857\u001b[0m     +  2.1542\n",
      "      3        \u001b[36m1.6251\u001b[0m       \u001b[32m0.3517\u001b[0m        1.5867        2.1525\n",
      "      4        1.6409       0.3448        \u001b[35m1.5805\u001b[0m     +  2.1263\n",
      "      5        \u001b[36m1.6116\u001b[0m       0.2966        \u001b[35m1.5637\u001b[0m     +  2.1588\n",
      "      6        \u001b[36m1.5848\u001b[0m       0.3448        \u001b[35m1.5477\u001b[0m     +  2.1546\n",
      "      7        \u001b[36m1.5644\u001b[0m       0.3517        \u001b[35m1.5285\u001b[0m     +  2.1677\n",
      "      8        \u001b[36m1.5205\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.4497\u001b[0m     +  2.1577\n",
      "      9        \u001b[36m1.4720\u001b[0m       0.3655        \u001b[35m1.3494\u001b[0m     +  2.1653\n",
      "     10        \u001b[36m1.3344\u001b[0m       0.3379        1.3770        2.1708\n",
      "     11        \u001b[36m1.2540\u001b[0m       0.3724        \u001b[35m1.2786\u001b[0m     +  2.1395\n",
      "     12        \u001b[36m1.1872\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.2526\u001b[0m     +  2.1645\n",
      "     13        \u001b[36m1.0400\u001b[0m       \u001b[32m0.4483\u001b[0m        \u001b[35m1.2120\u001b[0m     +  2.1667\n",
      "     14        \u001b[36m0.9609\u001b[0m       \u001b[32m0.4828\u001b[0m        1.2517        2.1696\n",
      "     15        0.9735       0.4483        1.3441        2.1383\n",
      "     16        1.0568       \u001b[32m0.5517\u001b[0m        1.2400        2.1281\n",
      "     17        \u001b[36m0.8105\u001b[0m       \u001b[32m0.5931\u001b[0m        \u001b[35m1.1613\u001b[0m     +  2.1275\n",
      "     18        \u001b[36m0.5996\u001b[0m       0.5172        1.4624        2.1590\n",
      "     19        \u001b[36m0.4958\u001b[0m       \u001b[32m0.6207\u001b[0m        \u001b[35m1.1540\u001b[0m     +  2.1273\n",
      "     20        0.5064       0.5862        1.3933        2.1611\n",
      "     21        \u001b[36m0.4042\u001b[0m       \u001b[32m0.6621\u001b[0m        \u001b[35m1.0658\u001b[0m     +  2.1259\n",
      "     22        \u001b[36m0.2794\u001b[0m       \u001b[32m0.7241\u001b[0m        1.1459        2.1548\n",
      "     23        \u001b[36m0.1931\u001b[0m       0.6897        1.1322        2.1251\n",
      "     24        \u001b[36m0.1169\u001b[0m       0.7034        1.4898        2.1300\n",
      "     25        \u001b[36m0.0806\u001b[0m       0.6483        1.5466        2.1375\n",
      "     26        0.0845       0.7034        1.5763        2.1318\n",
      "     27        0.0886       0.6759        1.7927        2.1324\n",
      "     28        \u001b[36m0.0441\u001b[0m       0.6690        2.0171        2.1288\n",
      "     29        0.1005       0.6621        1.9659        2.1309\n",
      "     30        0.1367       0.7241        1.7628        2.1299\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.743 total time= 1.8min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1568\u001b[0m       \u001b[32m0.2569\u001b[0m        \u001b[35m1.5748\u001b[0m     +  2.9501\n",
      "      2        \u001b[36m1.5951\u001b[0m       \u001b[32m0.3125\u001b[0m        1.5801        2.9232\n",
      "      3        1.5952       \u001b[32m0.3194\u001b[0m        \u001b[35m1.5121\u001b[0m     +  2.9084\n",
      "      4        \u001b[36m1.5034\u001b[0m       \u001b[32m0.4028\u001b[0m        \u001b[35m1.3751\u001b[0m     +  2.9229\n",
      "      5        \u001b[36m1.3906\u001b[0m       \u001b[32m0.5486\u001b[0m        \u001b[35m1.1474\u001b[0m     +  2.9159\n",
      "      6        \u001b[36m1.1825\u001b[0m       0.5486        \u001b[35m1.0940\u001b[0m     +  2.9301\n",
      "      7        \u001b[36m1.1590\u001b[0m       \u001b[32m0.5972\u001b[0m        1.1794        2.9275\n",
      "      8        \u001b[36m0.9921\u001b[0m       0.5903        \u001b[35m0.9572\u001b[0m     +  2.9128\n",
      "      9        \u001b[36m0.8228\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7708\u001b[0m     +  2.9266\n",
      "     10        \u001b[36m0.7698\u001b[0m       0.6111        1.3118        2.9257\n",
      "     11        \u001b[36m0.6143\u001b[0m       0.7014        0.9711        2.9112\n",
      "     12        \u001b[36m0.6028\u001b[0m       0.6875        1.0010        2.9085\n",
      "     13        \u001b[36m0.5591\u001b[0m       \u001b[32m0.7361\u001b[0m        1.1486        2.9044\n",
      "     14        \u001b[36m0.5071\u001b[0m       0.6875        2.4543        2.9174\n",
      "     15        \u001b[36m0.3991\u001b[0m       0.6875        0.8819        2.9089\n",
      "     16        \u001b[36m0.3399\u001b[0m       0.6875        1.3096        2.9117\n",
      "     17        \u001b[36m0.2950\u001b[0m       0.6736        1.4194        2.9124\n",
      "     18        \u001b[36m0.2739\u001b[0m       0.6944        1.2881        2.9115\n",
      "     19        \u001b[36m0.2258\u001b[0m       \u001b[32m0.7847\u001b[0m        \u001b[35m0.7627\u001b[0m     +  2.9125\n",
      "     20        \u001b[36m0.1939\u001b[0m       0.7778        0.9981        2.9251\n",
      "     21        \u001b[36m0.1306\u001b[0m       \u001b[32m0.8194\u001b[0m        0.7660        2.9116\n",
      "     22        0.1374       0.7847        \u001b[35m0.7615\u001b[0m     +  2.9123\n",
      "     23        \u001b[36m0.1150\u001b[0m       0.7639        0.9179        2.9371\n",
      "     24        \u001b[36m0.1028\u001b[0m       0.8125        1.0038        2.9104\n",
      "     25        \u001b[36m0.0904\u001b[0m       0.7986        1.0340        2.9114\n",
      "     26        0.1230       0.6875        1.3499        2.9130\n",
      "     27        0.1066       0.7569        1.2434        2.9332\n",
      "     28        \u001b[36m0.0681\u001b[0m       0.7639        1.1235        2.9196\n",
      "     29        0.0919       0.7847        1.0774        2.9157\n",
      "     30        \u001b[36m0.0600\u001b[0m       0.7917        1.2609        2.9170\n",
      "     31        0.0628       0.7431        1.3453        2.9194\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.324 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1475\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.5779\u001b[0m     +  2.9480\n",
      "      2        \u001b[36m1.5817\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.4741\u001b[0m     +  2.9351\n",
      "      3        \u001b[36m1.4203\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.1605\u001b[0m     +  2.9376\n",
      "      4        \u001b[36m1.1835\u001b[0m       \u001b[32m0.6207\u001b[0m        \u001b[35m1.0812\u001b[0m     +  2.9303\n",
      "      5        \u001b[36m0.9671\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m0.7849\u001b[0m     +  2.9434\n",
      "      6        \u001b[36m0.7935\u001b[0m       0.7034        \u001b[35m0.7759\u001b[0m     +  2.9396\n",
      "      7        \u001b[36m0.6777\u001b[0m       \u001b[32m0.7724\u001b[0m        \u001b[35m0.7349\u001b[0m     +  2.9445\n",
      "      8        \u001b[36m0.5565\u001b[0m       0.7655        \u001b[35m0.7027\u001b[0m     +  2.9382\n",
      "      9        \u001b[36m0.4674\u001b[0m       \u001b[32m0.8000\u001b[0m        \u001b[35m0.5894\u001b[0m     +  2.9359\n",
      "     10        \u001b[36m0.4314\u001b[0m       0.8000        \u001b[35m0.5743\u001b[0m     +  2.9385\n",
      "     11        \u001b[36m0.3832\u001b[0m       0.7862        0.6512        2.9334\n",
      "     12        \u001b[36m0.2298\u001b[0m       0.7793        0.9072        2.9194\n",
      "     13        0.2601       0.7586        0.7424        2.9150\n",
      "     14        \u001b[36m0.2003\u001b[0m       \u001b[32m0.8069\u001b[0m        0.8552        2.9133\n",
      "     15        0.2018       0.7586        1.0626        2.9127\n",
      "     16        \u001b[36m0.1348\u001b[0m       \u001b[32m0.8138\u001b[0m        0.5984        2.9254\n",
      "     17        \u001b[36m0.1118\u001b[0m       \u001b[32m0.8207\u001b[0m        0.8459        2.9135\n",
      "     18        0.1236       0.8000        0.8490        2.9172\n",
      "     19        0.1651       0.7862        0.8923        2.9186\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-0.671 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0953\u001b[0m       \u001b[32m0.3172\u001b[0m        \u001b[35m1.5470\u001b[0m     +  2.9378\n",
      "      2        \u001b[36m1.6053\u001b[0m       \u001b[32m0.4690\u001b[0m        \u001b[35m1.3704\u001b[0m     +  2.9369\n",
      "      3        \u001b[36m1.4846\u001b[0m       \u001b[32m0.5103\u001b[0m        \u001b[35m1.2483\u001b[0m     +  2.9356\n",
      "      4        \u001b[36m1.2813\u001b[0m       0.3310        2.4627        2.9416\n",
      "      5        \u001b[36m1.1331\u001b[0m       \u001b[32m0.6069\u001b[0m        \u001b[35m0.9871\u001b[0m     +  2.9175\n",
      "      6        \u001b[36m1.0449\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.7912\u001b[0m     +  2.9349\n",
      "      7        \u001b[36m0.8167\u001b[0m       \u001b[32m0.7655\u001b[0m        \u001b[35m0.7168\u001b[0m     +  2.9416\n",
      "      8        \u001b[36m0.6666\u001b[0m       0.6276        1.1436        2.9397\n",
      "      9        \u001b[36m0.4874\u001b[0m       0.6345        1.2435        2.9226\n",
      "     10        \u001b[36m0.4730\u001b[0m       0.7448        1.2813        2.9238\n",
      "     11        \u001b[36m0.3196\u001b[0m       0.6759        3.1201        2.9238\n",
      "     12        \u001b[36m0.2955\u001b[0m       \u001b[32m0.8207\u001b[0m        1.0189        2.9265\n",
      "     13        \u001b[36m0.2203\u001b[0m       0.8069        0.9709        2.9240\n",
      "     14        0.2363       \u001b[32m0.8483\u001b[0m        0.7949        2.9236\n",
      "     15        \u001b[36m0.1822\u001b[0m       0.7862        1.4789        2.9238\n",
      "     16        0.2193       0.7586        1.1617        2.9154\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.275 total time=  56.1s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9001\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5931\u001b[0m     +  2.9302\n",
      "      2        \u001b[36m1.6368\u001b[0m       \u001b[32m0.2639\u001b[0m        1.6009        2.9407\n",
      "      3        \u001b[36m1.6025\u001b[0m       0.2431        \u001b[35m1.5793\u001b[0m     +  2.9204\n",
      "      4        \u001b[36m1.5872\u001b[0m       0.2431        \u001b[35m1.5338\u001b[0m     +  2.9346\n",
      "      5        \u001b[36m1.5429\u001b[0m       \u001b[32m0.4236\u001b[0m        \u001b[35m1.4573\u001b[0m     +  2.9416\n",
      "      6        \u001b[36m1.5355\u001b[0m       0.2569        1.8158        2.9413\n",
      "      7        \u001b[36m1.4553\u001b[0m       0.4236        \u001b[35m1.3351\u001b[0m     +  2.9331\n",
      "      8        \u001b[36m1.4144\u001b[0m       0.3611        1.3576        2.9308\n",
      "      9        \u001b[36m1.3701\u001b[0m       0.3194        1.9344        2.9167\n",
      "     10        \u001b[36m1.3465\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.3203\u001b[0m     +  2.9174\n",
      "     11        \u001b[36m1.2882\u001b[0m       \u001b[32m0.5833\u001b[0m        \u001b[35m1.2705\u001b[0m     +  2.9387\n",
      "     12        \u001b[36m1.2311\u001b[0m       0.5208        1.4039        2.9324\n",
      "     13        \u001b[36m1.1450\u001b[0m       0.5833        \u001b[35m1.2639\u001b[0m     +  2.9074\n",
      "     14        \u001b[36m1.1366\u001b[0m       0.5556        \u001b[35m1.2134\u001b[0m     +  2.9329\n",
      "     15        \u001b[36m1.0259\u001b[0m       0.4722        1.6557        2.9437\n",
      "     16        \u001b[36m0.9911\u001b[0m       \u001b[32m0.5972\u001b[0m        1.3316        2.9388\n",
      "     17        \u001b[36m0.9679\u001b[0m       \u001b[32m0.6181\u001b[0m        \u001b[35m1.0770\u001b[0m     +  2.9325\n",
      "     18        \u001b[36m0.8714\u001b[0m       0.6111        1.1161        2.9297\n",
      "     19        \u001b[36m0.8335\u001b[0m       \u001b[32m0.6528\u001b[0m        \u001b[35m1.0719\u001b[0m     +  2.9074\n",
      "     20        \u001b[36m0.7842\u001b[0m       0.6319        1.3824        2.9470\n",
      "     21        \u001b[36m0.7838\u001b[0m       \u001b[32m0.6597\u001b[0m        1.1086        2.9284\n",
      "     22        \u001b[36m0.7529\u001b[0m       0.6458        1.5146        2.9159\n",
      "     23        0.7529       0.6458        1.1719        2.9153\n",
      "     24        \u001b[36m0.6659\u001b[0m       0.6597        1.1887        2.9173\n",
      "     25        \u001b[36m0.6606\u001b[0m       0.6458        1.3077        2.9114\n",
      "     26        \u001b[36m0.6467\u001b[0m       0.6458        1.3732        2.9116\n",
      "     27        \u001b[36m0.6414\u001b[0m       0.6528        1.4929        2.9231\n",
      "     28        \u001b[36m0.6168\u001b[0m       \u001b[32m0.6944\u001b[0m        1.3932        2.9200\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.462 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.7449\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5730\u001b[0m     +  2.9388\n",
      "      2        \u001b[36m1.5910\u001b[0m       \u001b[32m0.2483\u001b[0m        1.5739        2.9460\n",
      "      3        \u001b[36m1.5676\u001b[0m       \u001b[32m0.2690\u001b[0m        \u001b[35m1.5375\u001b[0m     +  2.9293\n",
      "      4        \u001b[36m1.5661\u001b[0m       \u001b[32m0.2897\u001b[0m        \u001b[35m1.5178\u001b[0m     +  2.9453\n",
      "      5        \u001b[36m1.5350\u001b[0m       \u001b[32m0.3379\u001b[0m        1.6355        2.9445\n",
      "      6        \u001b[36m1.4898\u001b[0m       \u001b[32m0.3448\u001b[0m        \u001b[35m1.3898\u001b[0m     +  2.9303\n",
      "      7        \u001b[36m1.4504\u001b[0m       0.3172        1.3899        2.9419\n",
      "      8        \u001b[36m1.3240\u001b[0m       0.3448        \u001b[35m1.3178\u001b[0m     +  2.9201\n",
      "      9        \u001b[36m1.2884\u001b[0m       0.3241        1.4209        2.9446\n",
      "     10        \u001b[36m1.2689\u001b[0m       \u001b[32m0.4276\u001b[0m        1.3434        2.9278\n",
      "     11        \u001b[36m1.1888\u001b[0m       0.4069        \u001b[35m1.2356\u001b[0m     +  2.9233\n",
      "     12        \u001b[36m1.1043\u001b[0m       0.3862        \u001b[35m1.2077\u001b[0m     +  2.9370\n",
      "     13        \u001b[36m1.0313\u001b[0m       \u001b[32m0.4414\u001b[0m        1.2121        2.9467\n",
      "     14        \u001b[36m0.9917\u001b[0m       0.4276        2.0050        2.9226\n",
      "     15        \u001b[36m0.9528\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m1.0741\u001b[0m     +  2.9163\n",
      "     16        \u001b[36m0.8554\u001b[0m       0.5655        1.2398        2.9411\n",
      "     17        \u001b[36m0.8350\u001b[0m       \u001b[32m0.6483\u001b[0m        1.2255        2.9189\n",
      "     18        \u001b[36m0.8170\u001b[0m       0.6069        1.3921        2.9178\n",
      "     19        \u001b[36m0.8079\u001b[0m       0.6483        1.5913        2.9250\n",
      "     20        \u001b[36m0.7974\u001b[0m       0.6069        1.4392        2.9226\n",
      "     21        \u001b[36m0.7643\u001b[0m       0.5793        1.4051        2.9248\n",
      "     22        \u001b[36m0.6991\u001b[0m       0.6276        1.1748        2.9233\n",
      "     23        \u001b[36m0.6767\u001b[0m       0.6483        1.1092        2.9221\n",
      "     24        \u001b[36m0.6027\u001b[0m       \u001b[32m0.6897\u001b[0m        1.1076        2.9350\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.193 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8312\u001b[0m       \u001b[32m0.2897\u001b[0m        \u001b[35m1.5993\u001b[0m     +  2.9495\n",
      "      2        \u001b[36m1.6357\u001b[0m       0.2483        1.6067        2.9536\n",
      "      3        \u001b[36m1.6314\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.5889\u001b[0m     +  2.9251\n",
      "      4        \u001b[36m1.6184\u001b[0m       0.2414        1.5954        2.9431\n",
      "      5        \u001b[36m1.5881\u001b[0m       0.2897        \u001b[35m1.5582\u001b[0m     +  2.9330\n",
      "      6        \u001b[36m1.5841\u001b[0m       0.2483        \u001b[35m1.5117\u001b[0m     +  2.9498\n",
      "      7        \u001b[36m1.5152\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.3930\u001b[0m     +  2.9415\n",
      "      8        \u001b[36m1.4821\u001b[0m       0.3172        1.4179        2.9438\n",
      "      9        \u001b[36m1.4231\u001b[0m       \u001b[32m0.4207\u001b[0m        \u001b[35m1.2293\u001b[0m     +  2.9354\n",
      "     10        \u001b[36m1.3321\u001b[0m       \u001b[32m0.4621\u001b[0m        \u001b[35m1.1771\u001b[0m     +  2.9480\n",
      "     11        \u001b[36m1.2530\u001b[0m       \u001b[32m0.4759\u001b[0m        1.1848        2.9485\n",
      "     12        1.2900       0.3862        1.3565        2.9306\n",
      "     13        \u001b[36m1.1576\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m0.9822\u001b[0m     +  2.9256\n",
      "     14        \u001b[36m1.1087\u001b[0m       0.5172        1.1593        2.9406\n",
      "     15        \u001b[36m0.9980\u001b[0m       0.5103        1.1290        2.9257\n",
      "     16        \u001b[36m0.9935\u001b[0m       \u001b[32m0.5931\u001b[0m        1.2655        2.9248\n",
      "     17        \u001b[36m0.9714\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m0.9526\u001b[0m     +  2.9253\n",
      "     18        \u001b[36m0.8691\u001b[0m       0.4759        1.3258        2.9367\n",
      "     19        \u001b[36m0.8284\u001b[0m       0.6276        1.1020        2.9236\n",
      "     20        \u001b[36m0.7490\u001b[0m       \u001b[32m0.6897\u001b[0m        0.9737        2.9236\n",
      "     21        \u001b[36m0.6063\u001b[0m       \u001b[32m0.7034\u001b[0m        0.9697        2.9492\n",
      "     22        0.6456       0.6966        1.0987        2.9250\n",
      "     23        0.6332       0.6828        0.9948        2.9239\n",
      "     24        \u001b[36m0.5344\u001b[0m       0.7034        0.9757        2.9256\n",
      "     25        \u001b[36m0.4303\u001b[0m       \u001b[32m0.7517\u001b[0m        0.9732        2.9244\n",
      "     26        \u001b[36m0.3515\u001b[0m       \u001b[32m0.7655\u001b[0m        1.0114        2.9272\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.275 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.7144\u001b[0m       \u001b[32m0.2361\u001b[0m        \u001b[35m1.6087\u001b[0m     +  2.9405\n",
      "      2        \u001b[36m1.6267\u001b[0m       0.2361        1.6117        2.9414\n",
      "      3        1.6339       0.2361        \u001b[35m1.6010\u001b[0m     +  2.9219\n",
      "      4        \u001b[36m1.6261\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5985\u001b[0m     +  2.9411\n",
      "      5        1.6265       \u001b[32m0.2500\u001b[0m        \u001b[35m1.5929\u001b[0m     +  2.9422\n",
      "      6        \u001b[36m1.6187\u001b[0m       \u001b[32m0.2569\u001b[0m        1.5972        2.9453\n",
      "      7        \u001b[36m1.5997\u001b[0m       0.2361        1.5941        2.9277\n",
      "      8        1.6151       0.2361        1.6000        2.9207\n",
      "      9        1.5998       0.2431        \u001b[35m1.5920\u001b[0m     +  2.9191\n",
      "     10        \u001b[36m1.5888\u001b[0m       0.2361        \u001b[35m1.5490\u001b[0m     +  2.9355\n",
      "     11        \u001b[36m1.5398\u001b[0m       0.2431        1.5866        2.9415\n",
      "     12        \u001b[36m1.5258\u001b[0m       \u001b[32m0.3125\u001b[0m        1.5494        2.9218\n",
      "     13        \u001b[36m1.5225\u001b[0m       \u001b[32m0.4097\u001b[0m        \u001b[35m1.4414\u001b[0m     +  2.9278\n",
      "     14        1.5681       0.3264        1.4911        2.9345\n",
      "     15        \u001b[36m1.4965\u001b[0m       \u001b[32m0.4375\u001b[0m        1.4717        2.9301\n",
      "     16        \u001b[36m1.4535\u001b[0m       0.3681        1.4463        2.9355\n",
      "     17        \u001b[36m1.3850\u001b[0m       0.3542        \u001b[35m1.4239\u001b[0m     +  2.9240\n",
      "     18        1.3853       0.3750        \u001b[35m1.4144\u001b[0m     +  2.9382\n",
      "     19        1.4377       0.3819        \u001b[35m1.3908\u001b[0m     +  2.9322\n",
      "     20        \u001b[36m1.3429\u001b[0m       0.4028        \u001b[35m1.3316\u001b[0m     +  2.9361\n",
      "     21        \u001b[36m1.3030\u001b[0m       0.4097        \u001b[35m1.3131\u001b[0m     +  2.9362\n",
      "     22        \u001b[36m1.2912\u001b[0m       0.3889        \u001b[35m1.2915\u001b[0m     +  2.9481\n",
      "     23        \u001b[36m1.2664\u001b[0m       0.4167        \u001b[35m1.2804\u001b[0m     +  2.9597\n",
      "     24        \u001b[36m1.2559\u001b[0m       0.3472        1.4356        2.9670\n",
      "     25        \u001b[36m1.2189\u001b[0m       0.4097        \u001b[35m1.2582\u001b[0m     +  2.9211\n",
      "     26        \u001b[36m1.2039\u001b[0m       0.3542        1.4586        2.9337\n",
      "     27        \u001b[36m1.1947\u001b[0m       0.3681        1.3701        2.9244\n",
      "     28        \u001b[36m1.1502\u001b[0m       0.3889        1.3853        2.9204\n",
      "     29        \u001b[36m1.1408\u001b[0m       0.4167        \u001b[35m1.1968\u001b[0m     +  2.9203\n",
      "     30        1.1511       0.3542        1.9815        2.9358\n",
      "     31        1.1785       0.4028        1.2343        2.9187\n",
      "     32        \u001b[36m1.1279\u001b[0m       0.3958        1.2428        2.9185\n",
      "     33        1.1553       0.3472        1.9698        2.9192\n",
      "     34        1.1609       \u001b[32m0.4861\u001b[0m        1.2972        2.9187\n",
      "     35        \u001b[36m1.1031\u001b[0m       \u001b[32m0.5278\u001b[0m        \u001b[35m1.1576\u001b[0m     +  2.9226\n",
      "     36        \u001b[36m1.0479\u001b[0m       0.5278        1.1744        2.9591\n",
      "     37        1.1209       0.4653        1.4916        2.9383\n",
      "     38        1.0766       0.5139        1.3965        2.9207\n",
      "     39        1.0916       0.4792        1.3942        2.9201\n",
      "     40        \u001b[36m1.0412\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m1.1207\u001b[0m     +  2.9213\n",
      "     41        1.0432       0.3750        1.6173        2.9452\n",
      "     42        \u001b[36m1.0373\u001b[0m       0.4931        1.8872        2.9238\n",
      "     43        \u001b[36m0.9135\u001b[0m       0.5347        1.4463        2.9320\n",
      "     44        0.9259       0.5347        1.1494        2.9270\n",
      "     45        \u001b[36m0.8846\u001b[0m       0.5139        2.0304        2.9231\n",
      "     46        \u001b[36m0.8779\u001b[0m       0.4167        1.3668        2.9297\n",
      "     47        0.8781       \u001b[32m0.5486\u001b[0m        1.2161        2.9240\n",
      "     48        0.8960       \u001b[32m0.5625\u001b[0m        \u001b[35m1.0367\u001b[0m     +  2.9221\n",
      "     49        \u001b[36m0.8187\u001b[0m       0.5486        1.5515        2.9402\n",
      "     50        \u001b[36m0.8004\u001b[0m       0.5486        1.7426        2.9220\n",
      "     51        0.8721       0.5625        1.1154        2.9188\n",
      "     52        0.8225       \u001b[32m0.5764\u001b[0m        1.3729        2.9200\n",
      "     53        0.8138       0.5694        1.1443        2.9158\n",
      "     54        \u001b[36m0.7613\u001b[0m       \u001b[32m0.5833\u001b[0m        1.1395        2.9107\n",
      "     55        0.7786       0.5486        1.3300        2.9253\n",
      "     56        0.8451       0.5694        1.3181        2.9374\n",
      "     57        0.8539       0.5625        1.0768        2.9256\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-2.196 total time= 3.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.7323\u001b[0m       \u001b[32m0.1586\u001b[0m        \u001b[35m1.6064\u001b[0m     +  2.9393\n",
      "      2        \u001b[36m1.6283\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5998\u001b[0m     +  2.9493\n",
      "      3        1.6411       0.2069        1.6000        2.9544\n",
      "      4        \u001b[36m1.6078\u001b[0m       0.2276        \u001b[35m1.5907\u001b[0m     +  2.9407\n",
      "      5        \u001b[36m1.5944\u001b[0m       \u001b[32m0.3034\u001b[0m        \u001b[35m1.5656\u001b[0m     +  2.9458\n",
      "      6        \u001b[36m1.5820\u001b[0m       0.2690        1.5846        2.9619\n",
      "      7        \u001b[36m1.5785\u001b[0m       0.2483        2.1829        2.9488\n",
      "      8        \u001b[36m1.5566\u001b[0m       0.2759        \u001b[35m1.4928\u001b[0m     +  2.9616\n",
      "      9        \u001b[36m1.5077\u001b[0m       0.2690        \u001b[35m1.4181\u001b[0m     +  2.9518\n",
      "     10        \u001b[36m1.5027\u001b[0m       \u001b[32m0.3310\u001b[0m        1.5255        2.9421\n",
      "     11        \u001b[36m1.4552\u001b[0m       0.2483        2.9604        2.9343\n",
      "     12        1.4648       0.3241        1.4333        2.9291\n",
      "     13        \u001b[36m1.4411\u001b[0m       0.3310        1.4702        2.9302\n",
      "     14        \u001b[36m1.3585\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.4103\u001b[0m     +  2.9267\n",
      "     15        1.3810       \u001b[32m0.4000\u001b[0m        1.4201        2.9443\n",
      "     16        \u001b[36m1.3289\u001b[0m       0.3310        1.7238        2.9290\n",
      "     17        \u001b[36m1.3115\u001b[0m       0.3862        1.4404        2.9268\n",
      "     18        \u001b[36m1.2976\u001b[0m       \u001b[32m0.4276\u001b[0m        \u001b[35m1.2906\u001b[0m     +  2.9334\n",
      "     19        \u001b[36m1.2360\u001b[0m       0.3724        2.3301        2.9550\n",
      "     20        1.2390       \u001b[32m0.4483\u001b[0m        1.3071        2.9391\n",
      "     21        \u001b[36m1.1915\u001b[0m       0.4483        1.2967        2.9267\n",
      "     22        \u001b[36m1.1542\u001b[0m       \u001b[32m0.4690\u001b[0m        \u001b[35m1.2794\u001b[0m     +  2.9387\n",
      "     23        \u001b[36m1.1250\u001b[0m       0.4483        1.3370        2.9419\n",
      "     24        1.1324       0.4621        1.2936        2.9410\n",
      "     25        \u001b[36m1.0625\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.2397\u001b[0m     +  2.9381\n",
      "     26        1.1577       0.4621        1.3868        2.9503\n",
      "     27        1.1905       0.2552        1.5561        2.9359\n",
      "     28        1.2359       0.3241       18.1754        2.9468\n",
      "     29        1.1779       0.4345        1.4841        2.9460\n",
      "     30        1.0860       0.4966        1.4372        2.9498\n",
      "     31        \u001b[36m0.9752\u001b[0m       \u001b[32m0.5034\u001b[0m        1.3818        2.9367\n",
      "     32        \u001b[36m0.9060\u001b[0m       \u001b[32m0.5448\u001b[0m        1.4857        2.9590\n",
      "     33        \u001b[36m0.8791\u001b[0m       0.5310        1.5056        2.9530\n",
      "     34        \u001b[36m0.8547\u001b[0m       0.5241        1.3729        2.9385\n",
      "     35        0.8547       0.4966        \u001b[35m1.2077\u001b[0m     +  2.9489\n",
      "     36        0.8907       0.4345        3.8148        2.9548\n",
      "     37        0.9318       0.4828        1.7264        2.9528\n",
      "     38        0.8911       0.4828        1.6032        2.9431\n",
      "     39        \u001b[36m0.8427\u001b[0m       \u001b[32m0.5517\u001b[0m        1.4432        2.9469\n",
      "     40        \u001b[36m0.7846\u001b[0m       0.5448        1.3078        2.9543\n",
      "     41        \u001b[36m0.7837\u001b[0m       0.5172        1.5390        2.9584\n",
      "     42        \u001b[36m0.7749\u001b[0m       0.5241        1.4694        2.9501\n",
      "     43        0.7751       \u001b[32m0.5724\u001b[0m        1.4239        2.9425\n",
      "     44        \u001b[36m0.7605\u001b[0m       0.5517        1.6069        2.9659\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.457 total time= 2.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.7553\u001b[0m       \u001b[32m0.2276\u001b[0m        \u001b[35m1.5966\u001b[0m     +  2.9357\n",
      "      2        \u001b[36m1.6167\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5917\u001b[0m     +  2.9572\n",
      "      3        \u001b[36m1.6031\u001b[0m       0.2483        1.5946        2.9587\n",
      "      4        \u001b[36m1.5992\u001b[0m       0.2345        1.5959        2.9476\n",
      "      5        \u001b[36m1.5963\u001b[0m       0.2069        1.5971        2.9368\n",
      "      6        1.6150       0.2483        1.5928        2.9385\n",
      "      7        1.6015       \u001b[32m0.2828\u001b[0m        \u001b[35m1.5899\u001b[0m     +  2.9208\n",
      "      8        1.6013       0.2138        1.5946        2.9475\n",
      "      9        1.6009       0.2345        1.5966        2.9303\n",
      "     10        1.5983       0.2069        \u001b[35m1.5826\u001b[0m     +  2.9303\n",
      "     11        \u001b[36m1.5954\u001b[0m       0.2345        1.5864        2.9584\n",
      "     12        \u001b[36m1.5764\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.5590\u001b[0m     +  2.9518\n",
      "     13        \u001b[36m1.5419\u001b[0m       0.2690        \u001b[35m1.4637\u001b[0m     +  2.9582\n",
      "     14        \u001b[36m1.4768\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.4623\u001b[0m     +  2.9623\n",
      "     15        \u001b[36m1.4397\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.3546\u001b[0m     +  2.9565\n",
      "     16        \u001b[36m1.4303\u001b[0m       0.3379        1.3698        2.9522\n",
      "     17        \u001b[36m1.4042\u001b[0m       0.3310        1.3678        2.9438\n",
      "     18        \u001b[36m1.3557\u001b[0m       0.3655        \u001b[35m1.2949\u001b[0m     +  2.9342\n",
      "     19        \u001b[36m1.2900\u001b[0m       \u001b[32m0.4966\u001b[0m        1.3054        2.9502\n",
      "     20        1.2987       0.4138        1.3268        2.9475\n",
      "     21        \u001b[36m1.2600\u001b[0m       0.4483        1.3438        2.9500\n",
      "     22        \u001b[36m1.2455\u001b[0m       \u001b[32m0.5103\u001b[0m        1.5395        2.9728\n",
      "     23        \u001b[36m1.1964\u001b[0m       0.4759        \u001b[35m1.2509\u001b[0m     +  2.9459\n",
      "     24        \u001b[36m1.1388\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.1667\u001b[0m     +  2.9652\n",
      "     25        \u001b[36m1.0976\u001b[0m       0.5517        1.1900        2.9459\n",
      "     26        \u001b[36m1.0462\u001b[0m       0.4759        1.2137        2.9306\n",
      "     27        1.0752       0.4897        1.7025        2.9531\n",
      "     28        \u001b[36m1.0304\u001b[0m       \u001b[32m0.5586\u001b[0m        \u001b[35m1.1651\u001b[0m     +  2.9697\n",
      "     29        \u001b[36m1.0128\u001b[0m       0.5448        1.2015        2.9588\n",
      "     30        \u001b[36m0.9819\u001b[0m       0.5241        1.2269        2.9315\n",
      "     31        \u001b[36m0.9395\u001b[0m       0.5517        \u001b[35m1.1485\u001b[0m     +  2.9297\n",
      "     32        0.9784       0.5310        1.3153        2.9769\n",
      "     33        \u001b[36m0.9264\u001b[0m       0.5241        1.2708        2.9528\n",
      "     34        \u001b[36m0.8706\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.1101\u001b[0m     +  2.9551\n",
      "     35        0.8943       0.5517        1.1695        2.9514\n",
      "     36        \u001b[36m0.8460\u001b[0m       0.5586        1.1248        2.9335\n",
      "     37        \u001b[36m0.7847\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.0014\u001b[0m     +  2.9317\n",
      "     38        \u001b[36m0.7599\u001b[0m       0.5517        1.2203        2.9378\n",
      "     39        \u001b[36m0.7037\u001b[0m       0.5862        1.1039        2.9271\n",
      "     40        \u001b[36m0.6942\u001b[0m       0.5655        1.0691        2.9275\n",
      "     41        0.8028       0.5448        1.6237        2.9375\n",
      "     42        0.8157       0.4897        1.3180        2.9367\n",
      "     43        \u001b[36m0.6767\u001b[0m       0.5310        1.2383        2.9353\n",
      "     44        \u001b[36m0.5868\u001b[0m       0.5517        1.1950        2.9336\n",
      "     45        \u001b[36m0.5635\u001b[0m       0.5586        1.3352        2.9402\n",
      "     46        \u001b[36m0.5342\u001b[0m       0.5655        1.2868        2.9304\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.453 total time= 2.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4363\u001b[0m       \u001b[32m0.3194\u001b[0m        \u001b[35m1.5685\u001b[0m     +  3.1295\n",
      "      2        \u001b[36m1.6253\u001b[0m       \u001b[32m0.4028\u001b[0m        \u001b[35m1.4503\u001b[0m     +  3.1653\n",
      "      3        \u001b[36m1.4653\u001b[0m       \u001b[32m0.5278\u001b[0m        \u001b[35m1.2148\u001b[0m     +  3.1733\n",
      "      4        \u001b[36m1.2410\u001b[0m       \u001b[32m0.6389\u001b[0m        \u001b[35m1.0937\u001b[0m     +  3.1714\n",
      "      5        \u001b[36m0.9924\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.8558\u001b[0m     +  3.1662\n",
      "      6        \u001b[36m0.7498\u001b[0m       \u001b[32m0.7431\u001b[0m        \u001b[35m0.8511\u001b[0m     +  3.1775\n",
      "      7        \u001b[36m0.5793\u001b[0m       0.6806        1.1339        3.1687\n",
      "      8        \u001b[36m0.4681\u001b[0m       0.6528        1.1438        3.1495\n",
      "      9        \u001b[36m0.3918\u001b[0m       0.7222        0.9927        3.1347\n",
      "     10        \u001b[36m0.3297\u001b[0m       0.6597        1.2883        3.1412\n",
      "     11        0.3315       0.7222        1.1594        3.1441\n",
      "     12        \u001b[36m0.1285\u001b[0m       \u001b[32m0.8194\u001b[0m        \u001b[35m0.8293\u001b[0m     +  3.1507\n",
      "     13        \u001b[36m0.0951\u001b[0m       0.8194        \u001b[35m0.7233\u001b[0m     +  3.1645\n",
      "     14        \u001b[36m0.0792\u001b[0m       0.7292        1.6970        3.1635\n",
      "     15        0.1037       0.8125        0.8289        3.1417\n",
      "     16        \u001b[36m0.0300\u001b[0m       0.7917        0.7659        3.1475\n",
      "     17        0.0350       0.8056        0.8582        3.1514\n",
      "     18        0.0592       0.6944        1.7527        3.1347\n",
      "     19        0.3372       0.6458        1.9429        3.1469\n",
      "     20        0.0831       0.7361        1.1281        3.1423\n",
      "     21        0.0505       0.7708        1.1046        3.1463\n",
      "     22        \u001b[36m0.0240\u001b[0m       0.7361        1.0697        3.1478\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.920 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.5543\u001b[0m       \u001b[32m0.1793\u001b[0m        \u001b[35m1.6092\u001b[0m     +  3.1887\n",
      "      2        \u001b[36m1.6334\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.4766\u001b[0m     +  3.1693\n",
      "      3        \u001b[36m1.5187\u001b[0m       0.3517        2.0347        3.1677\n",
      "      4        \u001b[36m1.3256\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.0696\u001b[0m     +  3.1491\n",
      "      5        \u001b[36m1.0465\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m0.9741\u001b[0m     +  3.1731\n",
      "      6        \u001b[36m0.8178\u001b[0m       \u001b[32m0.6897\u001b[0m        0.9787        3.1695\n",
      "      7        \u001b[36m0.6687\u001b[0m       0.6000        1.4756        3.1636\n",
      "      8        \u001b[36m0.5302\u001b[0m       0.6897        \u001b[35m0.9458\u001b[0m     +  3.1499\n",
      "      9        \u001b[36m0.3617\u001b[0m       \u001b[32m0.7862\u001b[0m        \u001b[35m0.6680\u001b[0m     +  3.1690\n",
      "     10        \u001b[36m0.1942\u001b[0m       \u001b[32m0.8138\u001b[0m        0.6937        3.1668\n",
      "     11        \u001b[36m0.1050\u001b[0m       \u001b[32m0.8207\u001b[0m        0.7933        3.1538\n",
      "     12        0.1694       0.7310        1.0629        3.1614\n",
      "     13        0.2094       0.7448        1.0029        3.1466\n",
      "     14        0.1371       0.6483        1.3633        3.1532\n",
      "     15        \u001b[36m0.0614\u001b[0m       0.7862        1.2638        3.1483\n",
      "     16        \u001b[36m0.0330\u001b[0m       \u001b[32m0.8414\u001b[0m        1.0295        3.1497\n",
      "     17        0.0411       \u001b[32m0.8483\u001b[0m        1.0967        3.1531\n",
      "     18        \u001b[36m0.0092\u001b[0m       0.8414        1.1440        3.1495\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.036 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.6202\u001b[0m       \u001b[32m0.2690\u001b[0m        \u001b[35m1.5602\u001b[0m     +  3.1597\n",
      "      2        \u001b[36m1.5468\u001b[0m       \u001b[32m0.3793\u001b[0m        1.7808        3.1798\n",
      "      3        \u001b[36m1.3805\u001b[0m       \u001b[32m0.4483\u001b[0m        \u001b[35m1.2581\u001b[0m     +  3.1736\n",
      "      4        \u001b[36m1.0779\u001b[0m       0.3793        3.1057        3.1763\n",
      "      5        \u001b[36m0.9034\u001b[0m       \u001b[32m0.6897\u001b[0m        1.3900        3.1761\n",
      "      6        \u001b[36m0.6577\u001b[0m       \u001b[32m0.7724\u001b[0m        \u001b[35m1.1550\u001b[0m     +  3.1793\n",
      "      7        \u001b[36m0.6248\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m0.9087\u001b[0m     +  3.1854\n",
      "      8        \u001b[36m0.5277\u001b[0m       0.7793        \u001b[35m0.5857\u001b[0m     +  3.1967\n",
      "      9        \u001b[36m0.3320\u001b[0m       0.6621        1.0627        3.1708\n",
      "     10        \u001b[36m0.2334\u001b[0m       0.7655        0.9687        3.1674\n",
      "     11        \u001b[36m0.1966\u001b[0m       0.7793        0.7381        3.1521\n",
      "     12        0.2500       0.7517        1.0918        3.1545\n",
      "     13        \u001b[36m0.1212\u001b[0m       \u001b[32m0.8276\u001b[0m        1.0680        3.1543\n",
      "     14        \u001b[36m0.0873\u001b[0m       \u001b[32m0.8345\u001b[0m        1.0486        3.1593\n",
      "     15        0.1400       0.7931        0.8221        3.1503\n",
      "     16        0.0884       0.8069        1.0430        3.1550\n",
      "     17        0.1052       0.7931        0.9069        3.1530\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.992 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9669\u001b[0m       \u001b[32m0.2500\u001b[0m        \u001b[35m1.6002\u001b[0m     +  3.1581\n",
      "      2        \u001b[36m1.6877\u001b[0m       0.2083        1.6253        3.1697\n",
      "      3        \u001b[36m1.6449\u001b[0m       0.2500        \u001b[35m1.5875\u001b[0m     +  3.1432\n",
      "      4        \u001b[36m1.6317\u001b[0m       \u001b[32m0.2847\u001b[0m        1.6174        3.1669\n",
      "      5        \u001b[36m1.6238\u001b[0m       0.2292        1.5905        3.1627\n",
      "      6        \u001b[36m1.6101\u001b[0m       0.2431        1.5929        3.1552\n",
      "      7        \u001b[36m1.5776\u001b[0m       0.2431        \u001b[35m1.5112\u001b[0m     +  3.1530\n",
      "      8        1.5872       0.2431        1.5852        3.1660\n",
      "      9        \u001b[36m1.5026\u001b[0m       0.2431        \u001b[35m1.4372\u001b[0m     +  3.1457\n",
      "     10        \u001b[36m1.4695\u001b[0m       0.2431        1.4921        3.1595\n",
      "     11        \u001b[36m1.4101\u001b[0m       \u001b[32m0.3125\u001b[0m        \u001b[35m1.3899\u001b[0m     +  3.1493\n",
      "     12        1.4462       0.2917        1.4375        3.1568\n",
      "     13        1.4490       \u001b[32m0.3819\u001b[0m        1.4835        3.1472\n",
      "     14        \u001b[36m1.3500\u001b[0m       \u001b[32m0.5139\u001b[0m        \u001b[35m1.3727\u001b[0m     +  3.1623\n",
      "     15        \u001b[36m1.2100\u001b[0m       0.4722        1.7897        3.1593\n",
      "     16        \u001b[36m1.1960\u001b[0m       0.4514        1.4783        3.1438\n",
      "     17        \u001b[36m1.1499\u001b[0m       0.4444        1.4943        3.1450\n",
      "     18        \u001b[36m1.1263\u001b[0m       0.3889        2.0409        3.1458\n",
      "     19        \u001b[36m1.0974\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.0877\u001b[0m     +  3.1452\n",
      "     20        1.1184       0.3194        1.4995        3.1623\n",
      "     21        1.3002       0.4236        1.3277        3.1490\n",
      "     22        1.1677       0.5486        1.3827        3.1513\n",
      "     23        \u001b[36m0.9931\u001b[0m       \u001b[32m0.5764\u001b[0m        1.1614        3.1504\n",
      "     24        \u001b[36m0.9155\u001b[0m       0.5625        1.2213        3.1453\n",
      "     25        \u001b[36m0.8951\u001b[0m       \u001b[32m0.5972\u001b[0m        1.2232        3.1431\n",
      "     26        \u001b[36m0.8648\u001b[0m       0.5903        1.2768        3.1417\n",
      "     27        \u001b[36m0.8491\u001b[0m       0.5694        1.2703        3.1435\n",
      "     28        \u001b[36m0.8455\u001b[0m       0.5347        1.9624        3.1419\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.254 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0371\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5828\u001b[0m     +  3.1510\n",
      "      2        \u001b[36m1.6709\u001b[0m       0.2483        1.6029        3.1780\n",
      "      3        \u001b[36m1.6408\u001b[0m       0.2483        1.5839        3.1494\n",
      "      4        \u001b[36m1.6207\u001b[0m       \u001b[32m0.2828\u001b[0m        1.5850        3.1641\n",
      "      5        \u001b[36m1.5864\u001b[0m       \u001b[32m0.2897\u001b[0m        \u001b[35m1.4807\u001b[0m     +  3.1644\n",
      "      6        \u001b[36m1.5739\u001b[0m       0.2345        1.5085        3.1717\n",
      "      7        \u001b[36m1.4964\u001b[0m       \u001b[32m0.3103\u001b[0m        1.6537        3.1520\n",
      "      8        \u001b[36m1.4425\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m1.4451\u001b[0m     +  3.1564\n",
      "      9        \u001b[36m1.4191\u001b[0m       \u001b[32m0.3517\u001b[0m        1.5116        3.1721\n",
      "     10        \u001b[36m1.3479\u001b[0m       \u001b[32m0.3862\u001b[0m        1.4457        3.1501\n",
      "     11        1.3689       \u001b[32m0.4345\u001b[0m        \u001b[35m1.3574\u001b[0m     +  3.1508\n",
      "     12        \u001b[36m1.2570\u001b[0m       \u001b[32m0.4414\u001b[0m        \u001b[35m1.2387\u001b[0m     +  3.1666\n",
      "     13        \u001b[36m1.2343\u001b[0m       \u001b[32m0.4897\u001b[0m        1.3878        3.1794\n",
      "     14        \u001b[36m1.1877\u001b[0m       \u001b[32m0.5103\u001b[0m        1.3844        3.1527\n",
      "     15        \u001b[36m1.1715\u001b[0m       0.4759        1.2889        3.1501\n",
      "     16        \u001b[36m1.0561\u001b[0m       0.5034        1.2495        3.1564\n",
      "     17        \u001b[36m1.0111\u001b[0m       \u001b[32m0.5172\u001b[0m        1.2766        3.1507\n",
      "     18        1.0121       0.3241        3.5848        3.1564\n",
      "     19        1.0723       0.4897        1.6330        3.1619\n",
      "     20        \u001b[36m0.9629\u001b[0m       \u001b[32m0.5310\u001b[0m        1.2962        3.1730\n",
      "     21        \u001b[36m0.9510\u001b[0m       0.5103        1.4946        3.1606\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.244 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8723\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5919\u001b[0m     +  3.1575\n",
      "      2        \u001b[36m1.6226\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m1.5279\u001b[0m     +  3.1679\n",
      "      3        \u001b[36m1.6202\u001b[0m       \u001b[32m0.2483\u001b[0m        1.5575        3.1753\n",
      "      4        \u001b[36m1.5681\u001b[0m       \u001b[32m0.2552\u001b[0m        1.9237        3.1622\n",
      "      5        \u001b[36m1.5116\u001b[0m       \u001b[32m0.3793\u001b[0m        \u001b[35m1.4278\u001b[0m     +  3.1539\n",
      "      6        \u001b[36m1.4248\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m1.2745\u001b[0m     +  3.1916\n",
      "      7        \u001b[36m1.3214\u001b[0m       \u001b[32m0.4069\u001b[0m        1.3222        3.1707\n",
      "      8        \u001b[36m1.2209\u001b[0m       \u001b[32m0.4897\u001b[0m        1.4060        3.1514\n",
      "      9        \u001b[36m1.1084\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.1709\u001b[0m     +  3.1597\n",
      "     10        \u001b[36m0.9630\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m0.9053\u001b[0m     +  3.1790\n",
      "     11        \u001b[36m0.8267\u001b[0m       0.6138        0.9974        3.1783\n",
      "     12        \u001b[36m0.6890\u001b[0m       \u001b[32m0.7172\u001b[0m        1.0590        3.1537\n",
      "     13        \u001b[36m0.5462\u001b[0m       0.4414        1.4135        3.1539\n",
      "     14        \u001b[36m0.5288\u001b[0m       0.6138        3.2852        3.1669\n",
      "     15        0.5632       0.6207        1.4556        3.1585\n",
      "     16        \u001b[36m0.4762\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m0.7645\u001b[0m     +  3.1610\n",
      "     17        \u001b[36m0.2535\u001b[0m       \u001b[32m0.7931\u001b[0m        0.8715        3.1760\n",
      "     18        0.2600       \u001b[32m0.8069\u001b[0m        0.8953        3.1747\n",
      "     19        \u001b[36m0.2320\u001b[0m       0.7862        0.8794        3.1669\n",
      "     20        0.2438       0.7724        0.9762        3.1570\n",
      "     21        0.2987       0.7241        1.0686        3.1710\n",
      "     22        \u001b[36m0.1678\u001b[0m       0.7655        1.0990        3.1550\n",
      "     23        \u001b[36m0.0612\u001b[0m       0.8069        1.0557        3.1529\n",
      "     24        \u001b[36m0.0341\u001b[0m       \u001b[32m0.8276\u001b[0m        1.1045        3.1570\n",
      "     25        \u001b[36m0.0242\u001b[0m       \u001b[32m0.8414\u001b[0m        1.2819        3.1579\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.395 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8098\u001b[0m       \u001b[32m0.2083\u001b[0m        \u001b[35m1.6043\u001b[0m     +  3.1528\n",
      "      2        \u001b[36m1.6720\u001b[0m       0.2083        \u001b[35m1.6017\u001b[0m     +  3.1795\n",
      "      3        \u001b[36m1.6171\u001b[0m       \u001b[32m0.2153\u001b[0m        \u001b[35m1.5887\u001b[0m     +  3.1807\n",
      "      4        1.6275       \u001b[32m0.2500\u001b[0m        1.5976        3.1711\n",
      "      5        \u001b[36m1.5942\u001b[0m       0.2431        1.6116        3.1509\n",
      "      6        \u001b[36m1.5733\u001b[0m       \u001b[32m0.2778\u001b[0m        1.5903        3.1493\n",
      "      7        \u001b[36m1.5524\u001b[0m       0.2778        1.6112        3.1505\n",
      "      8        \u001b[36m1.4686\u001b[0m       \u001b[32m0.2847\u001b[0m        1.6629        3.1496\n",
      "      9        \u001b[36m1.3847\u001b[0m       0.2778        1.6337        3.1505\n",
      "     10        1.3951       \u001b[32m0.3542\u001b[0m        \u001b[35m1.5557\u001b[0m     +  3.1483\n",
      "     11        \u001b[36m1.3439\u001b[0m       0.3333        \u001b[35m1.5058\u001b[0m     +  3.1769\n",
      "     12        \u001b[36m1.3367\u001b[0m       0.3333        \u001b[35m1.4126\u001b[0m     +  3.1709\n",
      "     13        \u001b[36m1.2913\u001b[0m       \u001b[32m0.3819\u001b[0m        1.4607        3.1750\n",
      "     14        \u001b[36m1.2838\u001b[0m       \u001b[32m0.3958\u001b[0m        1.4269        3.1590\n",
      "     15        \u001b[36m1.2374\u001b[0m       0.3958        \u001b[35m1.3344\u001b[0m     +  3.1548\n",
      "     16        1.2711       \u001b[32m0.4097\u001b[0m        1.4780        3.1699\n",
      "     17        \u001b[36m1.2181\u001b[0m       0.4097        1.5125        3.1499\n",
      "     18        1.2188       0.4028        1.3390        3.1476\n",
      "     19        \u001b[36m1.2083\u001b[0m       0.4097        1.4251        3.1504\n",
      "     20        \u001b[36m1.1832\u001b[0m       0.4028        \u001b[35m1.3139\u001b[0m     +  3.1563\n",
      "     21        \u001b[36m1.1692\u001b[0m       0.4097        \u001b[35m1.3106\u001b[0m     +  3.1638\n",
      "     22        \u001b[36m1.1603\u001b[0m       0.4097        1.3164        3.1674\n",
      "     23        \u001b[36m1.1575\u001b[0m       0.4097        1.3184        3.1489\n",
      "     24        \u001b[36m1.1566\u001b[0m       \u001b[32m0.4167\u001b[0m        1.3195        3.1491\n",
      "     25        \u001b[36m1.1478\u001b[0m       0.3889        1.3378        3.1492\n",
      "     26        \u001b[36m1.1443\u001b[0m       0.3889        1.3468        3.1487\n",
      "     27        \u001b[36m1.1405\u001b[0m       0.4028        1.3480        3.1487\n",
      "     28        \u001b[36m1.1334\u001b[0m       0.4028        1.3572        3.1499\n",
      "     29        \u001b[36m1.1303\u001b[0m       0.4028        1.3732        3.1670\n",
      "     30        1.1358       0.3889        1.3269        3.1591\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.433 total time= 1.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8484\u001b[0m       \u001b[32m0.2138\u001b[0m        \u001b[35m1.6041\u001b[0m     +  3.1650\n",
      "      2        \u001b[36m1.6421\u001b[0m       \u001b[32m0.2621\u001b[0m        \u001b[35m1.6012\u001b[0m     +  3.2027\n",
      "      3        \u001b[36m1.6269\u001b[0m       0.2276        \u001b[35m1.5977\u001b[0m     +  3.1922\n",
      "      4        \u001b[36m1.6040\u001b[0m       0.2552        \u001b[35m1.5753\u001b[0m     +  3.1926\n",
      "      5        \u001b[36m1.5677\u001b[0m       0.2414        \u001b[35m1.5463\u001b[0m     +  3.1750\n",
      "      6        \u001b[36m1.5339\u001b[0m       0.2414        1.5567        3.1820\n",
      "      7        \u001b[36m1.4827\u001b[0m       \u001b[32m0.3310\u001b[0m        \u001b[35m1.5118\u001b[0m     +  3.1630\n",
      "      8        \u001b[36m1.4152\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.4449\u001b[0m     +  3.1765\n",
      "      9        \u001b[36m1.3858\u001b[0m       \u001b[32m0.3448\u001b[0m        1.5114        3.1878\n",
      "     10        \u001b[36m1.3511\u001b[0m       \u001b[32m0.3862\u001b[0m        1.4847        3.1800\n",
      "     11        \u001b[36m1.3015\u001b[0m       0.3724        \u001b[35m1.3903\u001b[0m     +  3.1824\n",
      "     12        \u001b[36m1.2828\u001b[0m       \u001b[32m0.4207\u001b[0m        1.4693        3.1863\n",
      "     13        \u001b[36m1.2625\u001b[0m       0.3655        1.3915        3.1676\n",
      "     14        \u001b[36m1.2454\u001b[0m       0.4207        1.4117        3.1649\n",
      "     15        \u001b[36m1.1967\u001b[0m       0.4000        1.4827        3.1574\n",
      "     16        \u001b[36m1.1353\u001b[0m       \u001b[32m0.4897\u001b[0m        \u001b[35m1.2600\u001b[0m     +  3.1562\n",
      "     17        1.1381       0.4621        1.2869        3.1940\n",
      "     18        \u001b[36m1.0381\u001b[0m       0.4414        1.6344        3.1644\n",
      "     19        \u001b[36m0.9700\u001b[0m       0.4138        1.7023        3.1725\n",
      "     20        \u001b[36m0.9564\u001b[0m       0.4000        2.6015        3.1783\n",
      "     21        0.9678       0.4069        3.6252        3.1720\n",
      "     22        0.9862       0.4138        2.9490        3.1643\n",
      "     23        1.0149       0.4690        1.6086        3.1810\n",
      "     24        \u001b[36m0.9372\u001b[0m       \u001b[32m0.5172\u001b[0m        1.3075        3.1566\n",
      "     25        \u001b[36m0.9149\u001b[0m       0.4897        2.3145        3.1657\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.218 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.8217\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.6010\u001b[0m     +  3.1744\n",
      "      2        \u001b[36m1.6403\u001b[0m       0.2483        \u001b[35m1.5911\u001b[0m     +  3.1918\n",
      "      3        \u001b[36m1.6287\u001b[0m       0.2483        1.5923        3.1896\n",
      "      4        \u001b[36m1.6058\u001b[0m       \u001b[32m0.2828\u001b[0m        \u001b[35m1.5600\u001b[0m     +  3.1613\n",
      "      5        \u001b[36m1.5657\u001b[0m       \u001b[32m0.3310\u001b[0m        1.5742        3.1751\n",
      "      6        \u001b[36m1.5193\u001b[0m       0.2552        1.8579        3.1589\n",
      "      7        \u001b[36m1.4930\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4071\u001b[0m     +  3.1589\n",
      "      8        \u001b[36m1.3810\u001b[0m       0.2483        8.1434        3.1794\n",
      "      9        \u001b[36m1.3347\u001b[0m       \u001b[32m0.4069\u001b[0m        1.6658        3.1592\n",
      "     10        \u001b[36m1.3224\u001b[0m       0.3310        1.5881        3.1591\n",
      "     11        \u001b[36m1.2787\u001b[0m       \u001b[32m0.4897\u001b[0m        \u001b[35m1.3348\u001b[0m     +  3.1564\n",
      "     12        1.2871       0.4759        1.3557        3.1848\n",
      "     13        1.3219       0.4138        1.7984        3.1630\n",
      "     14        \u001b[36m1.2603\u001b[0m       0.4414        1.4561        3.1639\n",
      "     15        \u001b[36m1.0974\u001b[0m       0.4828        1.8171        3.1624\n",
      "     16        \u001b[36m1.0534\u001b[0m       \u001b[32m0.5862\u001b[0m        \u001b[35m1.1076\u001b[0m     +  3.1625\n",
      "     17        \u001b[36m1.0014\u001b[0m       0.5448        1.3269        3.1792\n",
      "     18        \u001b[36m0.9165\u001b[0m       0.5448        1.1437        3.1626\n",
      "     19        0.9211       0.5862        \u001b[35m1.1030\u001b[0m     +  3.1608\n",
      "     20        \u001b[36m0.8747\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.0258\u001b[0m     +  3.1690\n",
      "     21        \u001b[36m0.8589\u001b[0m       0.5931        1.1357        3.1699\n",
      "     22        0.9760       0.4690        1.3326        3.1597\n",
      "     23        1.0582       0.5310        1.1158        3.1591\n",
      "     24        0.8862       0.5310        1.6653        3.1481\n",
      "     25        \u001b[36m0.8568\u001b[0m       0.5931        1.0289        3.1590\n",
      "     26        \u001b[36m0.8033\u001b[0m       0.5931        \u001b[35m1.0082\u001b[0m     +  3.1584\n",
      "     27        0.8180       0.5310        1.1020        3.1844\n",
      "     28        0.8098       0.5862        \u001b[35m1.0054\u001b[0m     +  3.1612\n",
      "     29        \u001b[36m0.7655\u001b[0m       0.6138        1.0098        3.1808\n",
      "     30        0.7711       0.5931        \u001b[35m0.9318\u001b[0m     +  3.1636\n",
      "     31        \u001b[36m0.7370\u001b[0m       \u001b[32m0.6276\u001b[0m        \u001b[35m0.8908\u001b[0m     +  3.1768\n",
      "     32        \u001b[36m0.7201\u001b[0m       0.5586        1.0443        3.1777\n",
      "     33        \u001b[36m0.7072\u001b[0m       0.5931        0.9778        3.1651\n",
      "     34        \u001b[36m0.6885\u001b[0m       0.5862        0.9193        3.1594\n",
      "     35        0.6902       0.6000        0.9590        3.1641\n",
      "     36        \u001b[36m0.6743\u001b[0m       0.5862        0.9426        3.1583\n",
      "     37        \u001b[36m0.6731\u001b[0m       0.5931        0.9031        3.1641\n",
      "     38        0.6779       0.6000        0.9515        3.1704\n",
      "     39        \u001b[36m0.6651\u001b[0m       0.5931        0.9406        3.1580\n",
      "     40        \u001b[36m0.6572\u001b[0m       0.5655        0.9884        3.1579\n",
      "     41        \u001b[36m0.6565\u001b[0m       0.6138        \u001b[35m0.8606\u001b[0m     +  3.1566\n",
      "     42        \u001b[36m0.6406\u001b[0m       0.6138        0.8654        3.1866\n",
      "     43        \u001b[36m0.6400\u001b[0m       0.6207        \u001b[35m0.8130\u001b[0m     +  3.1579\n",
      "     44        0.6418       0.6069        0.9016        3.1897\n",
      "     45        0.7307       0.4828        8.7117        3.1811\n",
      "     46        1.2727       0.5034        1.4287        3.1586\n",
      "     47        0.8719       0.5034        1.3416        3.1592\n",
      "     48        0.7015       0.5931        0.8779        3.1594\n",
      "     49        0.6649       0.5862        0.9812        3.1577\n",
      "     50        0.6403       0.5931        0.9573        3.1594\n",
      "     51        \u001b[36m0.6275\u001b[0m       0.6000        0.9498        3.1604\n",
      "     52        \u001b[36m0.6225\u001b[0m       0.6069        0.9629        3.1640\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.048 total time= 3.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.8419\u001b[0m       \u001b[32m0.2500\u001b[0m        \u001b[35m1.5756\u001b[0m     +  3.5841\n",
      "      2        \u001b[36m1.6119\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m1.1912\u001b[0m     +  3.6159\n",
      "      3        \u001b[36m1.1776\u001b[0m       \u001b[32m0.6528\u001b[0m        \u001b[35m0.9608\u001b[0m     +  3.6105\n",
      "      4        \u001b[36m0.8270\u001b[0m       \u001b[32m0.6944\u001b[0m        \u001b[35m0.9430\u001b[0m     +  3.6137\n",
      "      5        \u001b[36m0.5869\u001b[0m       0.6250        1.5537        3.6001\n",
      "      6        \u001b[36m0.4781\u001b[0m       0.6806        1.1750        3.5929\n",
      "      7        \u001b[36m0.3096\u001b[0m       \u001b[32m0.7153\u001b[0m        \u001b[35m0.8933\u001b[0m     +  3.5834\n",
      "      8        \u001b[36m0.2338\u001b[0m       \u001b[32m0.7708\u001b[0m        1.2545        3.6055\n",
      "      9        \u001b[36m0.1627\u001b[0m       0.7153        1.4407        3.5895\n",
      "     10        \u001b[36m0.1395\u001b[0m       0.7569        1.0289        3.5994\n",
      "     11        \u001b[36m0.0531\u001b[0m       \u001b[32m0.7986\u001b[0m        \u001b[35m0.8519\u001b[0m     +  3.5904\n",
      "     12        \u001b[36m0.0205\u001b[0m       \u001b[32m0.8056\u001b[0m        0.8765        3.6099\n",
      "     13        \u001b[36m0.0113\u001b[0m       0.7986        0.9412        3.5910\n",
      "     14        \u001b[36m0.0068\u001b[0m       \u001b[32m0.8194\u001b[0m        1.0151        3.6017\n",
      "     15        \u001b[36m0.0038\u001b[0m       0.8194        1.0548        3.6162\n",
      "     16        \u001b[36m0.0037\u001b[0m       0.8194        1.1084        3.6028\n",
      "     17        \u001b[36m0.0022\u001b[0m       0.8125        1.1777        3.5941\n",
      "     18        \u001b[36m0.0020\u001b[0m       0.8056        1.1335        3.5886\n",
      "     19        \u001b[36m0.0020\u001b[0m       0.8056        1.2014        3.5928\n",
      "     20        \u001b[36m0.0007\u001b[0m       0.8056        1.2525        3.6018\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.219 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.3004\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.4296\u001b[0m     +  3.6037\n",
      "      2        \u001b[36m1.5446\u001b[0m       \u001b[32m0.5586\u001b[0m        \u001b[35m1.0873\u001b[0m     +  3.6273\n",
      "      3        \u001b[36m1.1239\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m0.8383\u001b[0m     +  3.6244\n",
      "      4        \u001b[36m0.7910\u001b[0m       0.6690        0.9359        3.6271\n",
      "      5        \u001b[36m0.6557\u001b[0m       \u001b[32m0.7103\u001b[0m        1.2315        3.6029\n",
      "      6        \u001b[36m0.4971\u001b[0m       0.5448        3.5441        3.5925\n",
      "      7        \u001b[36m0.3391\u001b[0m       \u001b[32m0.8000\u001b[0m        \u001b[35m0.6292\u001b[0m     +  3.6075\n",
      "      8        \u001b[36m0.2024\u001b[0m       0.6483        1.8941        3.6341\n",
      "      9        \u001b[36m0.1556\u001b[0m       \u001b[32m0.8483\u001b[0m        0.7218        3.5961\n",
      "     10        \u001b[36m0.1134\u001b[0m       0.7793        0.9536        3.5947\n",
      "     11        0.1199       0.8000        1.0114        3.5963\n",
      "     12        \u001b[36m0.0804\u001b[0m       0.8000        0.9470        3.6103\n",
      "     13        0.0997       0.8138        0.6921        3.6129\n",
      "     14        0.1026       0.6828        1.8238        3.6166\n",
      "     15        0.0926       0.7931        0.9778        3.6034\n",
      "     16        \u001b[36m0.0648\u001b[0m       0.7724        1.0415        3.5978\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-0.861 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.0701\u001b[0m       \u001b[32m0.2000\u001b[0m        \u001b[35m1.6122\u001b[0m     +  3.6011\n",
      "      2        \u001b[36m1.6315\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m1.5468\u001b[0m     +  3.6112\n",
      "      3        \u001b[36m1.5337\u001b[0m       \u001b[32m0.4000\u001b[0m        1.6890        3.6153\n",
      "      4        \u001b[36m1.3094\u001b[0m       0.3034        3.3460        3.5960\n",
      "      5        \u001b[36m1.1403\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.2020\u001b[0m     +  3.6003\n",
      "      6        \u001b[36m0.9428\u001b[0m       0.5241        1.3460        3.6162\n",
      "      7        \u001b[36m0.8081\u001b[0m       0.5034        1.9360        3.5958\n",
      "      8        \u001b[36m0.6354\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m1.0223\u001b[0m     +  3.5972\n",
      "      9        \u001b[36m0.4026\u001b[0m       \u001b[32m0.7655\u001b[0m        \u001b[35m0.8120\u001b[0m     +  3.6114\n",
      "     10        \u001b[36m0.2023\u001b[0m       \u001b[32m0.8483\u001b[0m        0.8277        3.6121\n",
      "     11        \u001b[36m0.1403\u001b[0m       0.7931        1.0925        3.5943\n",
      "     12        \u001b[36m0.1067\u001b[0m       0.6828        2.0754        3.6022\n",
      "     13        \u001b[36m0.0892\u001b[0m       0.8069        1.1670        3.6012\n",
      "     14        0.1567       0.7586        1.4491        3.6043\n",
      "     15        0.2362       0.7586        1.6213        3.6008\n",
      "     16        0.1467       0.7448        1.8983        3.5995\n",
      "     17        0.1525       0.7310        1.9947        3.5996\n",
      "     18        \u001b[36m0.0439\u001b[0m       0.7586        1.6754        3.6024\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.303 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1580\u001b[0m       \u001b[32m0.2569\u001b[0m        \u001b[35m1.6036\u001b[0m     +  3.5985\n",
      "      2        \u001b[36m1.6950\u001b[0m       0.2431        \u001b[35m1.5712\u001b[0m     +  3.6258\n",
      "      3        \u001b[36m1.6459\u001b[0m       0.2431        1.5897        3.6252\n",
      "      4        \u001b[36m1.6181\u001b[0m       \u001b[32m0.3056\u001b[0m        \u001b[35m1.5238\u001b[0m     +  3.5958\n",
      "      5        \u001b[36m1.5451\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.4905\u001b[0m     +  3.6335\n",
      "      6        \u001b[36m1.4834\u001b[0m       0.3819        \u001b[35m1.4308\u001b[0m     +  3.6041\n",
      "      7        \u001b[36m1.3499\u001b[0m       \u001b[32m0.5833\u001b[0m        \u001b[35m1.0802\u001b[0m     +  3.6174\n",
      "      8        \u001b[36m1.1367\u001b[0m       0.5694        \u001b[35m1.0348\u001b[0m     +  3.6102\n",
      "      9        \u001b[36m0.8241\u001b[0m       \u001b[32m0.5972\u001b[0m        1.1105        3.6126\n",
      "     10        \u001b[36m0.5717\u001b[0m       0.5972        1.5333        3.5892\n",
      "     11        \u001b[36m0.4803\u001b[0m       \u001b[32m0.6111\u001b[0m        1.5254        3.5872\n",
      "     12        \u001b[36m0.3904\u001b[0m       \u001b[32m0.6389\u001b[0m        1.0602        3.5883\n",
      "     13        \u001b[36m0.2340\u001b[0m       \u001b[32m0.6667\u001b[0m        1.2930        3.5875\n",
      "     14        0.2365       \u001b[32m0.7778\u001b[0m        1.2969        3.5897\n",
      "     15        \u001b[36m0.2245\u001b[0m       0.7292        \u001b[35m0.8437\u001b[0m     +  3.5937\n",
      "     16        \u001b[36m0.0709\u001b[0m       0.7431        1.3703        3.6303\n",
      "     17        \u001b[36m0.0412\u001b[0m       0.7569        1.1910        3.6190\n",
      "     18        \u001b[36m0.0353\u001b[0m       \u001b[32m0.7847\u001b[0m        1.2220        3.6018\n",
      "     19        0.0548       0.7153        1.6381        3.5949\n",
      "     20        \u001b[36m0.0337\u001b[0m       0.7083        1.4585        3.5936\n",
      "     21        \u001b[36m0.0113\u001b[0m       0.7500        1.5574        3.5984\n",
      "     22        \u001b[36m0.0043\u001b[0m       0.7500        1.4727        3.5996\n",
      "     23        0.0723       0.6667        2.4087        3.5953\n",
      "     24        0.1114       0.6597        1.7957        3.5954\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.694 total time= 1.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1707\u001b[0m       \u001b[32m0.2759\u001b[0m        \u001b[35m1.6025\u001b[0m     +  3.6020\n",
      "      2        \u001b[36m1.6951\u001b[0m       0.2483        \u001b[35m1.5660\u001b[0m     +  3.6327\n",
      "      3        \u001b[36m1.6510\u001b[0m       \u001b[32m0.3448\u001b[0m        \u001b[35m1.5283\u001b[0m     +  3.6383\n",
      "      4        \u001b[36m1.5140\u001b[0m       \u001b[32m0.4069\u001b[0m        \u001b[35m1.2945\u001b[0m     +  3.6315\n",
      "      5        \u001b[36m1.3101\u001b[0m       0.3586        2.2186        3.6303\n",
      "      6        \u001b[36m1.1909\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.1344\u001b[0m     +  3.5972\n",
      "      7        \u001b[36m0.9771\u001b[0m       \u001b[32m0.5655\u001b[0m        1.6110        3.6149\n",
      "      8        \u001b[36m0.8761\u001b[0m       \u001b[32m0.6759\u001b[0m        1.1370        3.5958\n",
      "      9        \u001b[36m0.5762\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m1.0785\u001b[0m     +  3.5989\n",
      "     10        \u001b[36m0.3938\u001b[0m       \u001b[32m0.7379\u001b[0m        \u001b[35m0.8299\u001b[0m     +  3.6203\n",
      "     11        \u001b[36m0.3570\u001b[0m       0.6138        1.2780        3.6177\n",
      "     12        \u001b[36m0.3274\u001b[0m       0.7034        1.0255        3.5990\n",
      "     13        \u001b[36m0.2326\u001b[0m       \u001b[32m0.7448\u001b[0m        0.9066        3.6041\n",
      "     14        \u001b[36m0.1386\u001b[0m       0.6966        0.9909        3.6013\n",
      "     15        0.2865       0.7034        1.3635        3.6010\n",
      "     16        0.4238       0.6690        1.1996        3.6019\n",
      "     17        0.1723       \u001b[32m0.7724\u001b[0m        0.9247        3.6011\n",
      "     18        \u001b[36m0.0580\u001b[0m       \u001b[32m0.7862\u001b[0m        1.0561        3.6010\n",
      "     19        \u001b[36m0.0218\u001b[0m       0.7862        0.9591        3.6031\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-0.910 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.2967\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.5923\u001b[0m     +  3.5997\n",
      "      2        \u001b[36m1.6808\u001b[0m       \u001b[32m0.2552\u001b[0m        \u001b[35m1.5911\u001b[0m     +  3.6327\n",
      "      3        \u001b[36m1.6147\u001b[0m       0.2552        \u001b[35m1.5428\u001b[0m     +  3.6297\n",
      "      4        \u001b[36m1.5550\u001b[0m       0.2552        1.9672        3.6384\n",
      "      5        \u001b[36m1.3462\u001b[0m       \u001b[32m0.3310\u001b[0m        3.0118        3.6056\n",
      "      6        \u001b[36m1.0462\u001b[0m       \u001b[32m0.3793\u001b[0m        3.4603        3.6009\n",
      "      7        \u001b[36m0.9257\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.0206\u001b[0m     +  3.6008\n",
      "      8        \u001b[36m0.7301\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m0.8257\u001b[0m     +  3.6308\n",
      "      9        \u001b[36m0.4930\u001b[0m       \u001b[32m0.7931\u001b[0m        0.8266        3.6255\n",
      "     10        \u001b[36m0.3458\u001b[0m       0.4897        4.6459        3.6114\n",
      "     11        \u001b[36m0.2141\u001b[0m       \u001b[32m0.8000\u001b[0m        1.4641        3.6014\n",
      "     12        0.2834       0.7448        1.2141        3.6043\n",
      "     13        0.2674       \u001b[32m0.8138\u001b[0m        \u001b[35m0.7882\u001b[0m     +  3.6028\n",
      "     14        \u001b[36m0.1646\u001b[0m       0.7655        1.8410        3.6284\n",
      "     15        \u001b[36m0.0875\u001b[0m       \u001b[32m0.8483\u001b[0m        1.1433        3.6010\n",
      "     16        0.0955       0.7931        1.3956        3.6012\n",
      "     17        0.1450       0.6966        1.6022        3.6080\n",
      "     18        0.1486       0.8000        1.1302        3.6058\n",
      "     19        0.1260       0.6552        2.6058        3.6017\n",
      "     20        0.1891       0.7517        1.8925        3.6151\n",
      "     21        \u001b[36m0.0571\u001b[0m       0.7793        1.6193        3.6253\n",
      "     22        \u001b[36m0.0278\u001b[0m       0.8207        1.2396        3.6097\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.250 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.0167\u001b[0m       \u001b[32m0.2292\u001b[0m        \u001b[35m1.6165\u001b[0m     +  3.6062\n",
      "      2        \u001b[36m1.6646\u001b[0m       \u001b[32m0.2708\u001b[0m        \u001b[35m1.5875\u001b[0m     +  3.6221\n",
      "      3        \u001b[36m1.6282\u001b[0m       0.2569        \u001b[35m1.5824\u001b[0m     +  3.6290\n",
      "      4        \u001b[36m1.6079\u001b[0m       0.2431        \u001b[35m1.5739\u001b[0m     +  3.6270\n",
      "      5        \u001b[36m1.5930\u001b[0m       0.2500        1.5987        3.6271\n",
      "      6        \u001b[36m1.5536\u001b[0m       0.2431        3.5941        3.6015\n",
      "      7        \u001b[36m1.5163\u001b[0m       0.2500        2.4920        3.5989\n",
      "      8        \u001b[36m1.4589\u001b[0m       \u001b[32m0.3056\u001b[0m        \u001b[35m1.4678\u001b[0m     +  3.5979\n",
      "      9        \u001b[36m1.4572\u001b[0m       \u001b[32m0.3125\u001b[0m        1.8617        3.6160\n",
      "     10        \u001b[36m1.3817\u001b[0m       0.2639        2.5385        3.5956\n",
      "     11        \u001b[36m1.3178\u001b[0m       \u001b[32m0.3472\u001b[0m        \u001b[35m1.4451\u001b[0m     +  3.5947\n",
      "     12        1.3327       0.2986        3.1216        3.6104\n",
      "     13        \u001b[36m1.2965\u001b[0m       0.3403        \u001b[35m1.4308\u001b[0m     +  3.6004\n",
      "     14        \u001b[36m1.2704\u001b[0m       \u001b[32m0.3681\u001b[0m        1.5153        3.6289\n",
      "     15        \u001b[36m1.1629\u001b[0m       0.3333        1.4886        3.6215\n",
      "     16        1.2167       \u001b[32m0.4167\u001b[0m        \u001b[35m1.3375\u001b[0m     +  3.6239\n",
      "     17        \u001b[36m1.0977\u001b[0m       \u001b[32m0.4375\u001b[0m        1.4873        3.6337\n",
      "     18        \u001b[36m1.0788\u001b[0m       \u001b[32m0.4861\u001b[0m        2.1758        3.6224\n",
      "     19        \u001b[36m1.0321\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m1.1612\u001b[0m     +  3.6039\n",
      "     20        \u001b[36m0.9805\u001b[0m       0.4861        1.3766        3.6199\n",
      "     21        0.9824       0.5208        1.3300        3.5986\n",
      "     22        \u001b[36m0.9261\u001b[0m       0.5000        1.6339        3.5963\n",
      "     23        \u001b[36m0.8255\u001b[0m       \u001b[32m0.5417\u001b[0m        1.4857        3.5959\n",
      "     24        \u001b[36m0.7776\u001b[0m       0.5278        1.3770        3.6014\n",
      "     25        0.8174       0.5139        1.1700        3.6070\n",
      "     26        0.7792       \u001b[32m0.5903\u001b[0m        \u001b[35m1.1256\u001b[0m     +  3.5999\n",
      "     27        \u001b[36m0.6396\u001b[0m       \u001b[32m0.6111\u001b[0m        1.4299        3.6247\n",
      "     28        \u001b[36m0.5994\u001b[0m       \u001b[32m0.6458\u001b[0m        1.2360        3.5994\n",
      "     29        \u001b[36m0.5221\u001b[0m       \u001b[32m0.6667\u001b[0m        1.2033        3.6113\n",
      "     30        0.5413       0.5208        2.6282        3.5990\n",
      "     31        0.8030       0.5208        1.2561        3.5974\n",
      "     32        0.5299       0.6597        1.6907        3.6011\n",
      "     33        \u001b[36m0.4770\u001b[0m       0.6389        1.4606        3.6245\n",
      "     34        \u001b[36m0.3460\u001b[0m       0.6458        1.6174        3.6136\n",
      "     35        \u001b[36m0.2342\u001b[0m       \u001b[32m0.7222\u001b[0m        1.6728        3.6049\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-2.197 total time= 2.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9423\u001b[0m       \u001b[32m0.1172\u001b[0m        \u001b[35m1.6081\u001b[0m     +  3.6206\n",
      "      2        \u001b[36m1.6530\u001b[0m       \u001b[32m0.2552\u001b[0m        \u001b[35m1.5965\u001b[0m     +  3.6428\n",
      "      3        \u001b[36m1.6133\u001b[0m       0.2069        1.6018        3.6314\n",
      "      4        1.6297       0.2000        1.6035        3.6068\n",
      "      5        \u001b[36m1.6017\u001b[0m       0.2069        \u001b[35m1.5825\u001b[0m     +  3.6074\n",
      "      6        \u001b[36m1.5824\u001b[0m       0.2483        \u001b[35m1.5443\u001b[0m     +  3.6359\n",
      "      7        \u001b[36m1.5612\u001b[0m       \u001b[32m0.2828\u001b[0m        \u001b[35m1.5036\u001b[0m     +  3.6401\n",
      "      8        \u001b[36m1.4756\u001b[0m       0.2552        1.6088        3.6361\n",
      "      9        \u001b[36m1.4606\u001b[0m       0.2828        1.5517        3.6142\n",
      "     10        \u001b[36m1.4075\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.4738\u001b[0m     +  3.6075\n",
      "     11        \u001b[36m1.3437\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.3891\u001b[0m     +  3.6262\n",
      "     12        \u001b[36m1.3019\u001b[0m       0.3310        1.3919        3.6373\n",
      "     13        \u001b[36m1.2359\u001b[0m       \u001b[32m0.3448\u001b[0m        1.6471        3.6143\n",
      "     14        \u001b[36m1.2192\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.3643\u001b[0m     +  3.6060\n",
      "     15        \u001b[36m1.1837\u001b[0m       0.3517        1.8900        3.6260\n",
      "     16        1.1877       \u001b[32m0.3586\u001b[0m        1.5103        3.6139\n",
      "     17        1.2040       0.3241        3.0918        3.6139\n",
      "     18        \u001b[36m1.0800\u001b[0m       \u001b[32m0.4207\u001b[0m        1.8194        3.6087\n",
      "     19        \u001b[36m1.0171\u001b[0m       \u001b[32m0.4414\u001b[0m        \u001b[35m1.3591\u001b[0m     +  3.6083\n",
      "     20        \u001b[36m0.8768\u001b[0m       \u001b[32m0.4690\u001b[0m        1.6236        3.6406\n",
      "     21        \u001b[36m0.7592\u001b[0m       0.3448        3.1675        3.5954\n",
      "     22        0.8594       0.4483        5.1069        3.6112\n",
      "     23        0.8396       \u001b[32m0.5241\u001b[0m        \u001b[35m1.2076\u001b[0m     +  3.6087\n",
      "     24        \u001b[36m0.7334\u001b[0m       \u001b[32m0.5862\u001b[0m        2.3611        3.6398\n",
      "     25        \u001b[36m0.5979\u001b[0m       \u001b[32m0.6483\u001b[0m        1.4805        3.6087\n",
      "     26        \u001b[36m0.5461\u001b[0m       \u001b[32m0.6621\u001b[0m        1.3603        3.6133\n",
      "     27        0.5621       0.6138        1.9180        3.6059\n",
      "     28        \u001b[36m0.5433\u001b[0m       0.6552        \u001b[35m1.1493\u001b[0m     +  3.6056\n",
      "     29        \u001b[36m0.5062\u001b[0m       0.6552        1.3535        3.6281\n",
      "     30        \u001b[36m0.4695\u001b[0m       0.6414        1.2383        3.6051\n",
      "     31        \u001b[36m0.3582\u001b[0m       \u001b[32m0.7448\u001b[0m        1.4918        3.6090\n",
      "     32        0.3954       0.4897        4.2942        3.6058\n",
      "     33        0.4865       0.6483        2.1937        3.6104\n",
      "     34        0.5499       0.6345        1.3470        3.6142\n",
      "     35        \u001b[36m0.3152\u001b[0m       0.6621        1.6227        3.6106\n",
      "     36        \u001b[36m0.2444\u001b[0m       0.6759        1.5737        3.6105\n",
      "     37        \u001b[36m0.2172\u001b[0m       0.3517       13.6741        3.6332\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.886 total time= 2.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m1.9441\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.6017\u001b[0m     +  3.6098\n",
      "      2        \u001b[36m1.6587\u001b[0m       0.2483        \u001b[35m1.5877\u001b[0m     +  3.6230\n",
      "      3        \u001b[36m1.6041\u001b[0m       0.2483        1.5906        3.6304\n",
      "      4        1.6102       0.2483        \u001b[35m1.5764\u001b[0m     +  3.6144\n",
      "      5        \u001b[36m1.5799\u001b[0m       0.2483        \u001b[35m1.4841\u001b[0m     +  3.6382\n",
      "      6        \u001b[36m1.5480\u001b[0m       0.2483        1.5669        3.6499\n",
      "      7        \u001b[36m1.4587\u001b[0m       \u001b[32m0.2552\u001b[0m        1.6046        3.6115\n",
      "      8        \u001b[36m1.4443\u001b[0m       \u001b[32m0.2897\u001b[0m        1.5652        3.6135\n",
      "      9        \u001b[36m1.3807\u001b[0m       \u001b[32m0.3448\u001b[0m        1.6541        3.6078\n",
      "     10        \u001b[36m1.3578\u001b[0m       0.2483        4.6086        3.6100\n",
      "     11        \u001b[36m1.3342\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.3334\u001b[0m     +  3.6137\n",
      "     12        \u001b[36m1.2500\u001b[0m       \u001b[32m0.4690\u001b[0m        \u001b[35m1.3004\u001b[0m     +  3.6354\n",
      "     13        \u001b[36m1.1919\u001b[0m       0.4000        1.4400        3.6401\n",
      "     14        \u001b[36m1.0939\u001b[0m       0.4000        1.3568        3.6201\n",
      "     15        1.1972       0.4621        1.3050        3.6150\n",
      "     16        1.0985       0.4690        1.4331        3.6083\n",
      "     17        \u001b[36m0.9506\u001b[0m       0.4483        2.7215        3.6088\n",
      "     18        \u001b[36m0.8793\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.0790\u001b[0m     +  3.6110\n",
      "     19        \u001b[36m0.8147\u001b[0m       0.4483        2.5034        3.6384\n",
      "     20        0.8275       0.4207        5.7843        3.6319\n",
      "     21        \u001b[36m0.8067\u001b[0m       0.3103        4.7912        3.6179\n",
      "     22        \u001b[36m0.7411\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m0.9497\u001b[0m     +  3.6077\n",
      "     23        \u001b[36m0.6494\u001b[0m       0.6345        1.2474        3.6392\n",
      "     24        \u001b[36m0.5374\u001b[0m       \u001b[32m0.7379\u001b[0m        1.0903        3.6115\n",
      "     25        \u001b[36m0.4570\u001b[0m       0.7034        1.2343        3.6132\n",
      "     26        0.4824       0.7172        1.0446        3.6129\n",
      "     27        0.5304       0.5103        2.6638        3.6161\n",
      "     28        0.6050       0.5862        1.2022        3.6085\n",
      "     29        0.5179       0.7034        1.5495        3.6083\n",
      "     30        \u001b[36m0.3786\u001b[0m       0.6138        3.4585        3.6322\n",
      "     31        0.4349       0.7172        \u001b[35m0.9451\u001b[0m     +  3.6240\n",
      "     32        \u001b[36m0.3386\u001b[0m       0.6552        1.1097        3.6323\n",
      "     33        \u001b[36m0.2978\u001b[0m       0.7241        1.1318        3.6149\n",
      "     34        \u001b[36m0.2630\u001b[0m       0.7379        1.2203        3.6255\n",
      "     35        \u001b[36m0.2591\u001b[0m       0.7241        1.5070        3.6250\n",
      "     36        \u001b[36m0.2006\u001b[0m       \u001b[32m0.8000\u001b[0m        1.3244        3.6204\n",
      "     37        \u001b[36m0.1563\u001b[0m       0.7241        1.2926        3.6071\n",
      "     38        0.2029       0.7517        1.2077        3.6100\n",
      "     39        \u001b[36m0.1463\u001b[0m       0.7517        1.2203        3.6129\n",
      "     40        \u001b[36m0.1224\u001b[0m       0.7586        1.2463        3.6084\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.404 total time= 2.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.2202\u001b[0m       \u001b[32m0.3226\u001b[0m        \u001b[35m1.5031\u001b[0m     +  4.7373\n",
      "      2        \u001b[36m1.4779\u001b[0m       \u001b[32m0.4977\u001b[0m        \u001b[35m1.1920\u001b[0m     +  4.7499\n",
      "      3        \u001b[36m1.0948\u001b[0m       \u001b[32m0.7558\u001b[0m        \u001b[35m0.8390\u001b[0m     +  4.7386\n",
      "      4        \u001b[36m0.7280\u001b[0m       0.5115        2.0115        4.7630\n",
      "      5        \u001b[36m0.5766\u001b[0m       \u001b[32m0.8295\u001b[0m        \u001b[35m0.5601\u001b[0m     +  4.7344\n",
      "      6        \u001b[36m0.3307\u001b[0m       \u001b[32m0.8387\u001b[0m        0.5997        4.7490\n",
      "      7        \u001b[36m0.2120\u001b[0m       0.6959        1.5492        4.7215\n",
      "      8        \u001b[36m0.1762\u001b[0m       0.7143        1.2578        4.7176\n",
      "      9        \u001b[36m0.1541\u001b[0m       0.7097        1.2582        4.7193\n",
      "     10        0.1885       0.7512        0.9439        4.7170\n",
      "     11        \u001b[36m0.1466\u001b[0m       \u001b[32m0.8618\u001b[0m        0.6616        4.7175\n",
      "     12        \u001b[36m0.1010\u001b[0m       0.8618        0.7760        4.7171\n",
      "     13        \u001b[36m0.0870\u001b[0m       0.8203        0.8525        4.7189\n",
      "     14        \u001b[36m0.0751\u001b[0m       0.7880        1.0219        4.7232\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<function generate_model at 0x7f8e8eceb8b0>,\n",
       "  module__dropout=0.25,\n",
       "  module__hidden_features=128,\n",
       "  module__opt={},\n",
       "),\n",
       "             param_grid={'module__dropout': [0.5],\n",
       "                         'module__freeze_layers': [True, False],\n",
       "                         'module__hidden_features': [64, 128, 256],\n",
       "                         'module__num_hidden_layers': [1, 2, 3]},\n",
       "             scoring='neg_log_loss', verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Uncomment this for tuning hyperparameter and saving the best model\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "params={'module__hidden_features': [64, 128, 256],\n",
    "        'module__dropout': [0.5],\n",
    "        'module__num_hidden_layers': [1, 2, 3],\n",
    "        'module__freeze_layers': [True, False]}\n",
    "# params={'module__hidden_features': [256],\n",
    "#         'module__dropout': [0.5],\n",
    "#         'module__num_hidden_layers': [3],\n",
    "#         'module__freeze_layers': [False]}\n",
    "net = NeuralNetClassifier(\n",
    "    # model,\n",
    "    module=generate_model,\n",
    "    module__opt=opt,\n",
    "    module__hidden_features=128,\n",
    "    module__dropout=0.25,\n",
    "    max_epochs=100,\n",
    "    lr=3e-4,\n",
    "    device=opt.device,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=16,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[('checkpoint', Checkpoint()),\n",
    "               ('early_stopping', EarlyStopping(patience=10))]\n",
    ")\n",
    "# net.fit(inp_imgs, torch.as_tensor(target))\n",
    "\n",
    "gs = GridSearchCV(net, params,\n",
    "                  # refit=False,\n",
    "                  cv=3,\n",
    "                  scoring='neg_log_loss',\n",
    "                  verbose=3, error_score='raise')\n",
    "\n",
    "gs.fit(inp_imgs, torch.as_tensor(target))\n",
    "\n",
    "with open('yolox.pkl', 'wb') as f:\n",
    "    pickle.dump(gs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1_JU1gGpjT2DLx1hjDtuB8QVUesITCnWK&export=download&confirm=t\n",
      "To: /Users/aj/develop/unt/class/spring-2022/csce-5218/project/YOLOX_Classification_Project/yolox.pkl\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11G/1.11G [04:21<00:00, 4.25MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolox.pkl'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download yolox model from google drive that was tuned and trained by the team\n",
    "# If any issues with this cell, please download yolox model manually from the link below to the current directory and run the next cell \n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/u/0/uc?id=1_JU1gGpjT2DLx1hjDtuB8QVUesITCnWK&export=download&confirm=t'\n",
    "output = 'yolox.pkl'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from pickel file\n",
    "with open('yolox.pkl', 'rb') as f:\n",
    "    gs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module__dropout': 0.5,\n",
       " 'module__freeze_layers': False,\n",
       " 'module__hidden_features': 128,\n",
       " 'module__num_hidden_layers': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 0, 0, 4, 0, 2, 3, 3, 4, 0, 0, 2, 2, 0, 2, 3, 1, 0, 2, 4]),\n",
       " array([4, 0, 0, 4, 4, 2, 3, 3, 4, 0, 0, 2, 2, 0, 2, 3, 1, 2, 2, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(inp_imgs[:20]), target[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate both models on a withheld test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42 images that the baseline model could not classify\n"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "imgs = []\n",
    "for subdir, dirs, files in os.walk(test_path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image        \n",
    "        results = pose.process(imgRGB)\n",
    "        \n",
    "        imgs.append(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(baseline_model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(test_path, '')])[0])\n",
    "        else:\n",
    "            y_pred.append(-1)\n",
    "            y_test.append(labelencoder.transform([subdir.replace(test_path, '')])[0])\n",
    "print(f'There are {len([1 for y in y_pred if y == -1])} images that the baseline model could not classify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 s Â± 213 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y_pred = []\n",
    "for img in imgs:\n",
    "    temp=[]\n",
    "    blackie = np.zeros(img.shape) # Blank image\n",
    "    results = pose.process(img)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "            mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            for i,j in zip(points,landmarks):\n",
    "                    temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "            y_pred.append(baseline_model.predict([temp]))\n",
    "    else:\n",
    "        y_pred.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['downdog', 'goddess', 'plank', 'tree', 'warrior2'], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['downdog', 'goddess', 'plank', 'tree', 'warrior2', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "# We add an \"unknown\" class to mark those that the baseline couldn't make a prediction period.\n",
    "classes = labelencoder.classes_.tolist()+['Unknown']\n",
    "print(classes)\n",
    "baseline_report = classification_report(y_test, [int(y) if y != -1 else 5 for y in y_pred], target_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "test_path = \"dataset/test/\"\n",
    "\n",
    "# Creating Dataset\n",
    "y_test = []\n",
    "images_arrays = []\n",
    "        \n",
    "\n",
    "for subdir, dirs, files in os.walk(test_path):\n",
    "    for img in files:\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        images_arrays.append(img)\n",
    "        y_test.append(subdir.replace(path, ''))\n",
    "\n",
    "\n",
    "test_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images_arrays):\n",
    "    test_imgs[b_i] = image\n",
    "\n",
    "test_imgs = test_imgs\n",
    "y_test = labelencoder.fit_transform(y_test)\n",
    "# Predictions\n",
    "y_pred = gs.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 s Â± 98.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Predictions\n",
    "y_pred = gs.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the YOLOX model\n",
    "yolox_report = classification_report(y_test, y_pred, target_names=labelencoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Display results on the test set for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.95      0.74      0.83        97\n",
      "     goddess       0.70      0.50      0.58        80\n",
      "       plank       0.85      0.76      0.80       115\n",
      "        tree       0.94      0.70      0.80        69\n",
      "    warrior2       0.69      0.90      0.78       109\n",
      "     Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73       470\n",
      "   macro avg       0.69      0.60      0.63       470\n",
      "weighted avg       0.82      0.73      0.77       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      " dataset/test/downdog       0.80      1.00      0.89        97\n",
      " dataset/test/goddess       0.78      0.81      0.80        80\n",
      "   dataset/test/plank       0.82      0.86      0.84       115\n",
      "    dataset/test/tree       0.86      0.96      0.90        69\n",
      "dataset/test/warrior2       0.90      0.56      0.69       109\n",
      "\n",
      "             accuracy                           0.83       470\n",
      "            macro avg       0.83      0.84      0.82       470\n",
      "         weighted avg       0.83      0.83      0.82       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yolox_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
