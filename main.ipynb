{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIqOUQPKntq-"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJUk_dFhb7RF"
   },
   "source": [
    "Reference Link: https://www.analyticsvidhya.com/blog/2021/10/human-pose-estimation-using-machine-learning-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4VepFNk-cZY5"
   },
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dtAAkvRYcbY4",
    "outputId": "aeae7d69-f165-4804-cf47-022b77a321d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385088</td>\n",
       "      <td>0.702528</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.364045</td>\n",
       "      <td>0.705285</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.361666</td>\n",
       "      <td>0.700772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525770</td>\n",
       "      <td>0.781881</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>-0.215838</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.165073</td>\n",
       "      <td>0.610637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470336</td>\n",
       "      <td>0.691998</td>\n",
       "      <td>-0.604218</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.705398</td>\n",
       "      <td>-0.624718</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.437711</td>\n",
       "      <td>0.699783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526152</td>\n",
       "      <td>0.778034</td>\n",
       "      <td>0.569747</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.764064</td>\n",
       "      <td>0.639336</td>\n",
       "      <td>0.467184</td>\n",
       "      <td>0.604327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.615995</td>\n",
       "      <td>-0.057232</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.630020</td>\n",
       "      <td>-0.067001</td>\n",
       "      <td>0.988656</td>\n",
       "      <td>0.440118</td>\n",
       "      <td>0.630212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552748</td>\n",
       "      <td>0.769751</td>\n",
       "      <td>0.797690</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.937697</td>\n",
       "      <td>0.743715</td>\n",
       "      <td>0.761387</td>\n",
       "      <td>0.285678</td>\n",
       "      <td>0.625030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.401504</td>\n",
       "      <td>0.383240</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>0.378602</td>\n",
       "      <td>-0.318324</td>\n",
       "      <td>0.989338</td>\n",
       "      <td>0.404689</td>\n",
       "      <td>0.379723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>0.622063</td>\n",
       "      <td>0.713210</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>0.916048</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.763766</td>\n",
       "      <td>-0.045453</td>\n",
       "      <td>0.652138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.683425</td>\n",
       "      <td>-0.067524</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>0.446467</td>\n",
       "      <td>0.690085</td>\n",
       "      <td>-0.090036</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598638</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.911403</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.867453</td>\n",
       "      <td>0.307724</td>\n",
       "      <td>0.658130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>0.242243</td>\n",
       "      <td>0.467066</td>\n",
       "      <td>-0.179606</td>\n",
       "      <td>0.983067</td>\n",
       "      <td>0.225128</td>\n",
       "      <td>0.474538</td>\n",
       "      <td>-0.151332</td>\n",
       "      <td>0.981929</td>\n",
       "      <td>0.224699</td>\n",
       "      <td>0.476763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849715</td>\n",
       "      <td>0.841590</td>\n",
       "      <td>0.793305</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>0.837034</td>\n",
       "      <td>0.790180</td>\n",
       "      <td>-0.062608</td>\n",
       "      <td>0.853069</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>0.512464</td>\n",
       "      <td>0.721425</td>\n",
       "      <td>-0.543747</td>\n",
       "      <td>0.984726</td>\n",
       "      <td>0.510773</td>\n",
       "      <td>0.734221</td>\n",
       "      <td>-0.539054</td>\n",
       "      <td>0.983721</td>\n",
       "      <td>0.513721</td>\n",
       "      <td>0.735019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864014</td>\n",
       "      <td>0.547789</td>\n",
       "      <td>0.900536</td>\n",
       "      <td>0.296631</td>\n",
       "      <td>0.854198</td>\n",
       "      <td>0.317296</td>\n",
       "      <td>0.904507</td>\n",
       "      <td>-0.350878</td>\n",
       "      <td>0.866556</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>0.041084</td>\n",
       "      <td>0.480462</td>\n",
       "      <td>-0.290565</td>\n",
       "      <td>0.985741</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>0.440492</td>\n",
       "      <td>-0.271295</td>\n",
       "      <td>0.982165</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.434142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848213</td>\n",
       "      <td>0.983486</td>\n",
       "      <td>0.812803</td>\n",
       "      <td>0.070410</td>\n",
       "      <td>0.865286</td>\n",
       "      <td>0.972812</td>\n",
       "      <td>0.841870</td>\n",
       "      <td>-0.066042</td>\n",
       "      <td>0.875791</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0.276817</td>\n",
       "      <td>0.443246</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>0.987157</td>\n",
       "      <td>0.435037</td>\n",
       "      <td>0.453263</td>\n",
       "      <td>-0.042767</td>\n",
       "      <td>0.983942</td>\n",
       "      <td>0.434067</td>\n",
       "      <td>0.460111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859729</td>\n",
       "      <td>0.663250</td>\n",
       "      <td>0.575939</td>\n",
       "      <td>-0.073409</td>\n",
       "      <td>0.870568</td>\n",
       "      <td>0.707136</td>\n",
       "      <td>0.135086</td>\n",
       "      <td>-0.096244</td>\n",
       "      <td>0.876942</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>0.084846</td>\n",
       "      <td>0.372770</td>\n",
       "      <td>-0.048931</td>\n",
       "      <td>0.986106</td>\n",
       "      <td>0.099876</td>\n",
       "      <td>0.356499</td>\n",
       "      <td>-0.072201</td>\n",
       "      <td>0.984197</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.352938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800263</td>\n",
       "      <td>0.904275</td>\n",
       "      <td>0.633036</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>0.850006</td>\n",
       "      <td>0.851007</td>\n",
       "      <td>0.626946</td>\n",
       "      <td>0.219715</td>\n",
       "      <td>0.812880</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>959 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NOSE_x    NOSE_y    NOSE_z  NOSE_vis  LEFT_EYE_INNER_x  \\\n",
       "0    0.385088  0.702528 -0.004816  0.999651          0.364045   \n",
       "1    0.470336  0.691998 -0.604218  0.982091          0.445842   \n",
       "2    0.453251  0.615995 -0.057232  0.983838          0.440873   \n",
       "3    0.401504  0.383240 -0.309374  0.984927          0.436412   \n",
       "4    0.450490  0.683425 -0.067524  0.986200          0.446467   \n",
       "..        ...       ...       ...       ...               ...   \n",
       "954  0.242243  0.467066 -0.179606  0.983067          0.225128   \n",
       "955  0.512464  0.721425 -0.543747  0.984726          0.510773   \n",
       "956  0.041084  0.480462 -0.290565  0.985741         -0.001317   \n",
       "957  0.276817  0.443246 -0.009695  0.987157          0.435037   \n",
       "958  0.084846  0.372770 -0.048931  0.986106          0.099876   \n",
       "\n",
       "     LEFT_EYE_INNER_y  LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  \\\n",
       "0            0.705285         -0.031445            0.999706    0.361666   \n",
       "1            0.705398         -0.624718            0.987435    0.437711   \n",
       "2            0.630020         -0.067001            0.988656    0.440118   \n",
       "3            0.378602         -0.318324            0.989338    0.404689   \n",
       "4            0.690085         -0.090036            0.990196    0.426712   \n",
       "..                ...               ...                 ...         ...   \n",
       "954          0.474538         -0.151332            0.981929    0.224699   \n",
       "955          0.734221         -0.539054            0.983721    0.513721   \n",
       "956          0.440492         -0.271295            0.982165    0.000684   \n",
       "957          0.453263         -0.042767            0.983942    0.434067   \n",
       "958          0.356499         -0.072201            0.984197    0.100007   \n",
       "\n",
       "     LEFT_EYE_y  ...  RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  \\\n",
       "0      0.700772  ...        0.525770           0.781881           0.930616   \n",
       "1      0.699783  ...        0.526152           0.778034           0.569747   \n",
       "2      0.630212  ...        0.552748           0.769751           0.797690   \n",
       "3      0.379723  ...        0.591222           0.622063           0.713210   \n",
       "4      0.690337  ...        0.598638           0.656271           0.870245   \n",
       "..          ...  ...             ...                ...                ...   \n",
       "954    0.476763  ...        0.849715           0.841590           0.793305   \n",
       "955    0.735019  ...        0.864014           0.547789           0.900536   \n",
       "956    0.434142  ...        0.848213           0.983486           0.812803   \n",
       "957    0.460111  ...        0.859729           0.663250           0.575939   \n",
       "958    0.352938  ...        0.800263           0.904275           0.633036   \n",
       "\n",
       "     LEFT_FOOT_INDEX_z  LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  \\\n",
       "0            -0.215838             0.980343            0.763475   \n",
       "1             0.541724             0.935673            0.764064   \n",
       "2             0.004431             0.937697            0.743715   \n",
       "3            -0.035523             0.916048            0.569005   \n",
       "4             0.077734             0.911403            0.668724   \n",
       "..                 ...                  ...                 ...   \n",
       "954           0.051033             0.844406            0.837034   \n",
       "955           0.296631             0.854198            0.317296   \n",
       "956           0.070410             0.865286            0.972812   \n",
       "957          -0.073409             0.870568            0.707136   \n",
       "958          -0.067383             0.850006            0.851007   \n",
       "\n",
       "     RIGHT_FOOT_INDEX_y  RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  target  \n",
       "0              0.904605            0.165073              0.610637       0  \n",
       "1              0.639336            0.467184              0.604327       0  \n",
       "2              0.761387            0.285678              0.625030       0  \n",
       "3              0.763766           -0.045453              0.652138       0  \n",
       "4              0.867453            0.307724              0.658130       0  \n",
       "..                  ...                 ...                   ...     ...  \n",
       "954            0.790180           -0.062608              0.853069       2  \n",
       "955            0.904507           -0.350878              0.866556       2  \n",
       "956            0.841870           -0.066042              0.875791       2  \n",
       "957            0.135086           -0.096244              0.876942       2  \n",
       "958            0.626946            0.219715              0.812880       2  \n",
       "\n",
       "[959 rows x 133 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying Dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traning the baseline model with SVM\n",
    "from sklearn.svm import SVC\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "model = SVC(kernel = 'poly')\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.99      0.97      0.98       196\n",
      "     goddess       0.93      0.84      0.88       164\n",
      "       plank       0.94      0.98      0.96       225\n",
      "        tree       0.97      0.85      0.91       136\n",
      "    warrior2       0.86      0.96      0.91       238\n",
      "\n",
      "    accuracy                           0.93       959\n",
      "   macro avg       0.94      0.92      0.93       959\n",
      "weighted avg       0.93      0.93      0.93       959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = labelencoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm1vgXjEnuhH"
   },
   "source": [
    "# Implementing YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kcdC17YEcgTN"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, x):\n",
    "        return self['x']\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "--> Drop parameter head.stems.0.conv.weight.\n",
      "--> Drop parameter head.stems.0.bn.weight.\n",
      "--> Drop parameter head.stems.0.bn.bias.\n",
      "--> Drop parameter head.stems.0.bn.running_mean.\n",
      "--> Drop parameter head.stems.0.bn.running_var.\n",
      "--> Drop parameter head.stems.0.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.1.conv.weight.\n",
      "--> Drop parameter head.stems.1.bn.weight.\n",
      "--> Drop parameter head.stems.1.bn.bias.\n",
      "--> Drop parameter head.stems.1.bn.running_mean.\n",
      "--> Drop parameter head.stems.1.bn.running_var.\n",
      "--> Drop parameter head.stems.1.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.2.conv.weight.\n",
      "--> Drop parameter head.stems.2.bn.weight.\n",
      "--> Drop parameter head.stems.2.bn.bias.\n",
      "--> Drop parameter head.stems.2.bn.running_mean.\n",
      "--> Drop parameter head.stems.2.bn.running_var.\n",
      "--> Drop parameter head.stems.2.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_preds.0.weight.\n",
      "--> Drop parameter head.cls_preds.0.bias.\n",
      "--> Drop parameter head.cls_preds.1.weight.\n",
      "--> Drop parameter head.cls_preds.1.bias.\n",
      "--> Drop parameter head.cls_preds.2.weight.\n",
      "--> Drop parameter head.cls_preds.2.bias.\n",
      "--> Drop parameter head.reg_preds.0.weight.\n",
      "--> Drop parameter head.reg_preds.0.bias.\n",
      "--> Drop parameter head.reg_preds.1.weight.\n",
      "--> Drop parameter head.reg_preds.1.bias.\n",
      "--> Drop parameter head.reg_preds.2.weight.\n",
      "--> Drop parameter head.reg_preds.2.bias.\n",
      "--> Drop parameter head.obj_preds.0.weight.\n",
      "--> Drop parameter head.obj_preds.0.bias.\n",
      "--> Drop parameter head.obj_preds.1.weight.\n",
      "--> Drop parameter head.obj_preds.1.bias.\n",
      "--> Drop parameter head.obj_preds.2.weight.\n",
      "--> Drop parameter head.obj_preds.2.bias.\n",
      "No param head.fc0a.weight.\n",
      "No param head.fc0a.bias.\n",
      "No param head.fc0b.weight.\n",
      "No param head.fc0b.bias.\n",
      "No param head.fc0c.weight.\n",
      "No param head.fc0c.bias.\n",
      "No param head.fc1.weight.\n",
      "No param head.fc1.bias.\n",
      "No param head.fc2.weight.\n",
      "No param head.fc2.bias.\n",
      "No param head.fc3.weight.\n",
      "No param head.fc3.bias.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_sizes:List[int], input_channels:List[int], num_classes:int, hidden_features:int = 128):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc0a = nn.Linear(input_channels[0]*input_sizes[0]**2,hidden_features)\n",
    "        self.fc0b = nn.Linear(input_channels[1]*input_sizes[1]**2,hidden_features)\n",
    "        self.fc0c = nn.Linear(input_channels[2]*input_sizes[2]**2,hidden_features)\n",
    "        # Concatenate the three outputs into one linear layer\n",
    "        self.fc1 = nn.Linear(len(input_sizes) * hidden_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a = F.relu(self.fc0a(torch.flatten(x[0],1)))\n",
    "        b = F.relu(self.fc0b(torch.flatten(x[1],1)))\n",
    "        c = F.relu(self.fc0c(torch.flatten(x[2],1)))\n",
    "        x = torch.cat([a,b,c], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = 1e-3\n",
    "                m.momentum = 0.03\n",
    "\n",
    "\n",
    "model = get_model(opt,\n",
    "                  # head=IdentityModule(),\n",
    "                  head=ClassificationHead([80,40,20], [64,128,256], 5),\n",
    "                  freeze_layers=True)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 images\n",
      "Input image batch of shape: torch.Size([2, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# Load Images\n",
    "img_dir = 'imgs/'\n",
    "images = [cv2.imread(str(im)) for im in Path(img_dir).glob('*.jpg')]\n",
    "print(f'There are {len(images)} images')\n",
    "inp_imgs = np.zeros([len(images), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images):\n",
    "    img, r = preproc(image, opt.test_size, opt.rgb_means, opt.std)\n",
    "    inp_imgs[b_i] = img\n",
    "\n",
    "inp_imgs = torch.from_numpy(inp_imgs).to(opt.device)\n",
    "print(f'Input image batch of shape: {inp_imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs.\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(inp_imgs)\n",
    "    # print(yolo_outputs)\n",
    "    print(yolo_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## TODO: Implement Network Head for classification (probably some conv layers and a few fully connected layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "## Creating model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(12, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TODO: Implement a custom training loop\n",
    "As long as `get_model` is called with `freeze_layers=True`, the early layers (the YOLOX pretrained ones) will be frozen, so training should be fast--only the head needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model.fit(x,y, epochs=150, batch_size=10)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}