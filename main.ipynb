{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIqOUQPKntq-",
    "tags": []
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJUk_dFhb7RF"
   },
   "source": [
    "Reference Link: https://www.analyticsvidhya.com/blog/2021/10/human-pose-estimation-using-machine-learning-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "4VepFNk-cZY5"
   },
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dtAAkvRYcbY4",
    "outputId": "aeae7d69-f165-4804-cf47-022b77a321d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385088</td>\n",
       "      <td>0.702528</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.364045</td>\n",
       "      <td>0.705285</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.361666</td>\n",
       "      <td>0.700772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525770</td>\n",
       "      <td>0.781881</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>-0.215838</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.165073</td>\n",
       "      <td>0.610637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470336</td>\n",
       "      <td>0.691998</td>\n",
       "      <td>-0.604218</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.705398</td>\n",
       "      <td>-0.624718</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.437711</td>\n",
       "      <td>0.699783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526152</td>\n",
       "      <td>0.778034</td>\n",
       "      <td>0.569747</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.764064</td>\n",
       "      <td>0.639336</td>\n",
       "      <td>0.467184</td>\n",
       "      <td>0.604327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.615995</td>\n",
       "      <td>-0.057232</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.630020</td>\n",
       "      <td>-0.067001</td>\n",
       "      <td>0.988656</td>\n",
       "      <td>0.440118</td>\n",
       "      <td>0.630212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552748</td>\n",
       "      <td>0.769751</td>\n",
       "      <td>0.797690</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.937697</td>\n",
       "      <td>0.743715</td>\n",
       "      <td>0.761387</td>\n",
       "      <td>0.285678</td>\n",
       "      <td>0.625030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.401504</td>\n",
       "      <td>0.383240</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>0.378602</td>\n",
       "      <td>-0.318324</td>\n",
       "      <td>0.989338</td>\n",
       "      <td>0.404689</td>\n",
       "      <td>0.379723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>0.622063</td>\n",
       "      <td>0.713210</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>0.916048</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.763766</td>\n",
       "      <td>-0.045453</td>\n",
       "      <td>0.652138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.683425</td>\n",
       "      <td>-0.067524</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>0.446467</td>\n",
       "      <td>0.690085</td>\n",
       "      <td>-0.090036</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598638</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.911403</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.867453</td>\n",
       "      <td>0.307724</td>\n",
       "      <td>0.658130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOSE_x    NOSE_y    NOSE_z  NOSE_vis  LEFT_EYE_INNER_x  LEFT_EYE_INNER_y  \\\n",
       "0  0.385088  0.702528 -0.004816  0.999651          0.364045          0.705285   \n",
       "1  0.470336  0.691998 -0.604218  0.982091          0.445842          0.705398   \n",
       "2  0.453251  0.615995 -0.057232  0.983838          0.440873          0.630020   \n",
       "3  0.401504  0.383240 -0.309374  0.984927          0.436412          0.378602   \n",
       "4  0.450490  0.683425 -0.067524  0.986200          0.446467          0.690085   \n",
       "\n",
       "   LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  LEFT_EYE_y  ...  \\\n",
       "0         -0.031445            0.999706    0.361666    0.700772  ...   \n",
       "1         -0.624718            0.987435    0.437711    0.699783  ...   \n",
       "2         -0.067001            0.988656    0.440118    0.630212  ...   \n",
       "3         -0.318324            0.989338    0.404689    0.379723  ...   \n",
       "4         -0.090036            0.990196    0.426712    0.690337  ...   \n",
       "\n",
       "   RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  LEFT_FOOT_INDEX_z  \\\n",
       "0        0.525770           0.781881           0.930616          -0.215838   \n",
       "1        0.526152           0.778034           0.569747           0.541724   \n",
       "2        0.552748           0.769751           0.797690           0.004431   \n",
       "3        0.591222           0.622063           0.713210          -0.035523   \n",
       "4        0.598638           0.656271           0.870245           0.077734   \n",
       "\n",
       "   LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  RIGHT_FOOT_INDEX_y  \\\n",
       "0             0.980343            0.763475            0.904605   \n",
       "1             0.935673            0.764064            0.639336   \n",
       "2             0.937697            0.743715            0.761387   \n",
       "3             0.916048            0.569005            0.763766   \n",
       "4             0.911403            0.668724            0.867453   \n",
       "\n",
       "   RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  target  \n",
       "0            0.165073              0.610637       0  \n",
       "1            0.467184              0.604327       0  \n",
       "2            0.285678              0.625030       0  \n",
       "3           -0.045453              0.652138       0  \n",
       "4            0.307724              0.658130       0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying Dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.505858</td>\n",
       "      <td>0.415518</td>\n",
       "      <td>-0.246159</td>\n",
       "      <td>0.943101</td>\n",
       "      <td>0.509002</td>\n",
       "      <td>0.405122</td>\n",
       "      <td>-0.245095</td>\n",
       "      <td>0.946450</td>\n",
       "      <td>0.511065</td>\n",
       "      <td>0.404905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733155</td>\n",
       "      <td>0.540911</td>\n",
       "      <td>0.875051</td>\n",
       "      <td>0.117718</td>\n",
       "      <td>0.757308</td>\n",
       "      <td>0.438541</td>\n",
       "      <td>0.860734</td>\n",
       "      <td>0.123024</td>\n",
       "      <td>0.729785</td>\n",
       "      <td>2.058394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169495</td>\n",
       "      <td>0.206963</td>\n",
       "      <td>0.314906</td>\n",
       "      <td>0.048036</td>\n",
       "      <td>0.175483</td>\n",
       "      <td>0.211302</td>\n",
       "      <td>0.313211</td>\n",
       "      <td>0.045594</td>\n",
       "      <td>0.176174</td>\n",
       "      <td>0.210777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.421723</td>\n",
       "      <td>0.458191</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.252702</td>\n",
       "      <td>0.413459</td>\n",
       "      <td>0.467372</td>\n",
       "      <td>0.117507</td>\n",
       "      <td>1.456656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.515029</td>\n",
       "      <td>-0.864020</td>\n",
       "      <td>-2.438290</td>\n",
       "      <td>0.698547</td>\n",
       "      <td>-0.572157</td>\n",
       "      <td>-0.926884</td>\n",
       "      <td>-2.470850</td>\n",
       "      <td>0.713646</td>\n",
       "      <td>-0.586003</td>\n",
       "      <td>-0.926810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300710</td>\n",
       "      <td>-0.409310</td>\n",
       "      <td>-0.324660</td>\n",
       "      <td>-1.693826</td>\n",
       "      <td>0.305906</td>\n",
       "      <td>-0.555812</td>\n",
       "      <td>-0.288427</td>\n",
       "      <td>-1.124857</td>\n",
       "      <td>0.287977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.430587</td>\n",
       "      <td>0.251077</td>\n",
       "      <td>-0.403654</td>\n",
       "      <td>0.921806</td>\n",
       "      <td>0.434221</td>\n",
       "      <td>0.235587</td>\n",
       "      <td>-0.384058</td>\n",
       "      <td>0.923609</td>\n",
       "      <td>0.433971</td>\n",
       "      <td>0.235401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664399</td>\n",
       "      <td>0.342956</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>-0.141796</td>\n",
       "      <td>0.706594</td>\n",
       "      <td>0.252431</td>\n",
       "      <td>0.702412</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>0.656494</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.497861</td>\n",
       "      <td>0.397042</td>\n",
       "      <td>-0.207230</td>\n",
       "      <td>0.960206</td>\n",
       "      <td>0.503867</td>\n",
       "      <td>0.384749</td>\n",
       "      <td>-0.199818</td>\n",
       "      <td>0.962283</td>\n",
       "      <td>0.506907</td>\n",
       "      <td>0.381391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743797</td>\n",
       "      <td>0.547789</td>\n",
       "      <td>0.849946</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.421433</td>\n",
       "      <td>0.840961</td>\n",
       "      <td>0.052675</td>\n",
       "      <td>0.742752</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.565261</td>\n",
       "      <td>0.563995</td>\n",
       "      <td>-0.048967</td>\n",
       "      <td>0.976734</td>\n",
       "      <td>0.568594</td>\n",
       "      <td>0.563723</td>\n",
       "      <td>-0.059908</td>\n",
       "      <td>0.978745</td>\n",
       "      <td>0.572143</td>\n",
       "      <td>0.563686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819670</td>\n",
       "      <td>0.747333</td>\n",
       "      <td>0.937381</td>\n",
       "      <td>0.307297</td>\n",
       "      <td>0.832774</td>\n",
       "      <td>0.599478</td>\n",
       "      <td>0.931384</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.820857</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.226446</td>\n",
       "      <td>1.296077</td>\n",
       "      <td>1.270609</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>1.227893</td>\n",
       "      <td>1.272271</td>\n",
       "      <td>1.262069</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>1.236210</td>\n",
       "      <td>1.268909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934545</td>\n",
       "      <td>1.339510</td>\n",
       "      <td>5.438205</td>\n",
       "      <td>3.869035</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>1.537790</td>\n",
       "      <td>5.224499</td>\n",
       "      <td>3.975790</td>\n",
       "      <td>0.939650</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NOSE_x      NOSE_y      NOSE_z    NOSE_vis  LEFT_EYE_INNER_x  \\\n",
       "count  959.000000  959.000000  959.000000  959.000000        959.000000   \n",
       "mean     0.505858    0.415518   -0.246159    0.943101          0.509002   \n",
       "std      0.169495    0.206963    0.314906    0.048036          0.175483   \n",
       "min     -0.515029   -0.864020   -2.438290    0.698547         -0.572157   \n",
       "25%      0.430587    0.251077   -0.403654    0.921806          0.434221   \n",
       "50%      0.497861    0.397042   -0.207230    0.960206          0.503867   \n",
       "75%      0.565261    0.563995   -0.048967    0.976734          0.568594   \n",
       "max      1.226446    1.296077    1.270609    0.999651          1.227893   \n",
       "\n",
       "       LEFT_EYE_INNER_y  LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  \\\n",
       "count        959.000000        959.000000          959.000000  959.000000   \n",
       "mean           0.405122         -0.245095            0.946450    0.511065   \n",
       "std            0.211302          0.313211            0.045594    0.176174   \n",
       "min           -0.926884         -2.470850            0.713646   -0.586003   \n",
       "25%            0.235587         -0.384058            0.923609    0.433971   \n",
       "50%            0.384749         -0.199818            0.962283    0.506907   \n",
       "75%            0.563723         -0.059908            0.978745    0.572143   \n",
       "max            1.272271          1.262069            0.999706    1.236210   \n",
       "\n",
       "       LEFT_EYE_y  ...  RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  \\\n",
       "count  959.000000  ...      959.000000         959.000000         959.000000   \n",
       "mean     0.404905  ...        0.733155           0.540911           0.875051   \n",
       "std      0.210777  ...        0.114746           0.254270           0.421723   \n",
       "min     -0.926810  ...        0.300710          -0.409310          -0.324660   \n",
       "25%      0.235401  ...        0.664399           0.342956           0.715532   \n",
       "50%      0.381391  ...        0.743797           0.547789           0.849946   \n",
       "75%      0.563686  ...        0.819670           0.747333           0.937381   \n",
       "max      1.268909  ...        0.934545           1.339510           5.438205   \n",
       "\n",
       "       LEFT_FOOT_INDEX_z  LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  \\\n",
       "count         959.000000           959.000000          959.000000   \n",
       "mean            0.117718             0.757308            0.438541   \n",
       "std             0.458191             0.109323            0.252702   \n",
       "min            -1.693826             0.305906           -0.555812   \n",
       "25%            -0.141796             0.706594            0.252431   \n",
       "50%             0.050389             0.778800            0.421433   \n",
       "75%             0.307297             0.832774            0.599478   \n",
       "max             3.869035             0.980343            1.537790   \n",
       "\n",
       "       RIGHT_FOOT_INDEX_y  RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  \\\n",
       "count          959.000000          959.000000            959.000000   \n",
       "mean             0.860734            0.123024              0.729785   \n",
       "std              0.413459            0.467372              0.117507   \n",
       "min             -0.288427           -1.124857              0.287977   \n",
       "25%              0.702412           -0.131425              0.656494   \n",
       "50%              0.840961            0.052675              0.742752   \n",
       "75%              0.931384            0.297200              0.820857   \n",
       "max              5.224499            3.975790              0.939650   \n",
       "\n",
       "           target  \n",
       "count  959.000000  \n",
       "mean     2.058394  \n",
       "std      1.456656  \n",
       "min      0.000000  \n",
       "25%      1.000000  \n",
       "50%      2.000000  \n",
       "75%      3.000000  \n",
       "max      4.000000  \n",
       "\n",
       "[8 rows x 133 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-8204a511db924627a8b8060e43d00c3b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-8204a511db924627a8b8060e43d00c3b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-8204a511db924627a8b8060e43d00c3b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-45b0f0e8af1628084e89ee1ec4750d29\"}, \"mark\": {\"type\": \"bar\", \"size\": 50}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"label\"}, \"tooltip\": [{\"type\": \"quantitative\", \"aggregate\": \"count\", \"title\": \"Count\"}, {\"type\": \"nominal\", \"field\": \"label\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"title\": \"Pose\"}, \"field\": \"label\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"axis\": {\"title\": \"Count\"}}}, \"height\": 300, \"selection\": {\"selector032\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Number of data in each pose\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-45b0f0e8af1628084e89ee1ec4750d29\": [{\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "label_df = pd.DataFrame()\n",
    "label_df['label'] = list(map(lambda x: labelencoder.inverse_transform([x])[0], data['target']))\n",
    "\n",
    "bars = alt.Chart(label_df).mark_bar(size=50).encode(\n",
    "    x=alt.X('label', axis=alt.Axis(title='Pose')),\n",
    "    y=alt.Y(\"count()\", axis=alt.Axis(title='Count')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Count'), 'label'],\n",
    "    color='label'\n",
    ")\n",
    "\n",
    "(bars).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"Number of data in each pose\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traning the baseline model with SVM\n",
    "from sklearn.svm import SVC\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "model = SVC(kernel = 'poly')\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.99      0.97      0.98       196\n",
      "     goddess       0.93      0.84      0.88       164\n",
      "       plank       0.94      0.98      0.96       225\n",
      "        tree       0.97      0.85      0.91       136\n",
      "    warrior2       0.86      0.96      0.91       238\n",
      "\n",
      "    accuracy                           0.93       959\n",
      "   macro avg       0.94      0.92      0.93       959\n",
      "weighted avg       0.93      0.93      0.93       959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = labelencoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm1vgXjEnuhH"
   },
   "source": [
    "# Implementing YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kcdC17YEcgTN"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, x):\n",
    "        return self['x']\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "--> Drop parameter head.stems.0.conv.weight.\n",
      "--> Drop parameter head.stems.0.bn.weight.\n",
      "--> Drop parameter head.stems.0.bn.bias.\n",
      "--> Drop parameter head.stems.0.bn.running_mean.\n",
      "--> Drop parameter head.stems.0.bn.running_var.\n",
      "--> Drop parameter head.stems.0.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.1.conv.weight.\n",
      "--> Drop parameter head.stems.1.bn.weight.\n",
      "--> Drop parameter head.stems.1.bn.bias.\n",
      "--> Drop parameter head.stems.1.bn.running_mean.\n",
      "--> Drop parameter head.stems.1.bn.running_var.\n",
      "--> Drop parameter head.stems.1.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.2.conv.weight.\n",
      "--> Drop parameter head.stems.2.bn.weight.\n",
      "--> Drop parameter head.stems.2.bn.bias.\n",
      "--> Drop parameter head.stems.2.bn.running_mean.\n",
      "--> Drop parameter head.stems.2.bn.running_var.\n",
      "--> Drop parameter head.stems.2.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_preds.0.weight.\n",
      "--> Drop parameter head.cls_preds.0.bias.\n",
      "--> Drop parameter head.cls_preds.1.weight.\n",
      "--> Drop parameter head.cls_preds.1.bias.\n",
      "--> Drop parameter head.cls_preds.2.weight.\n",
      "--> Drop parameter head.cls_preds.2.bias.\n",
      "--> Drop parameter head.reg_preds.0.weight.\n",
      "--> Drop parameter head.reg_preds.0.bias.\n",
      "--> Drop parameter head.reg_preds.1.weight.\n",
      "--> Drop parameter head.reg_preds.1.bias.\n",
      "--> Drop parameter head.reg_preds.2.weight.\n",
      "--> Drop parameter head.reg_preds.2.bias.\n",
      "--> Drop parameter head.obj_preds.0.weight.\n",
      "--> Drop parameter head.obj_preds.0.bias.\n",
      "--> Drop parameter head.obj_preds.1.weight.\n",
      "--> Drop parameter head.obj_preds.1.bias.\n",
      "--> Drop parameter head.obj_preds.2.weight.\n",
      "--> Drop parameter head.obj_preds.2.bias.\n",
      "No param head.fc0a.weight.\n",
      "No param head.fc0a.bias.\n",
      "No param head.fc0b.weight.\n",
      "No param head.fc0b.bias.\n",
      "No param head.fc0c.weight.\n",
      "No param head.fc0c.bias.\n",
      "No param head.fc1.weight.\n",
      "No param head.fc1.bias.\n",
      "No param head.fc2.weight.\n",
      "No param head.fc2.bias.\n",
      "No param head.fc3.weight.\n",
      "No param head.fc3.bias.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_sizes:List[int], input_channels:List[int], num_classes:int, hidden_features:int = 128):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc0a = nn.Linear(input_channels[0]*input_sizes[0]**2,hidden_features)\n",
    "        self.fc0b = nn.Linear(input_channels[1]*input_sizes[1]**2,hidden_features)\n",
    "        self.fc0c = nn.Linear(input_channels[2]*input_sizes[2]**2,hidden_features)\n",
    "        # Concatenate the three outputs into one linear layer\n",
    "        self.fc1 = nn.Linear(len(input_sizes) * hidden_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a = F.relu(self.fc0a(torch.flatten(x[0],1)))\n",
    "        b = F.relu(self.fc0b(torch.flatten(x[1],1)))\n",
    "        c = F.relu(self.fc0c(torch.flatten(x[2],1)))\n",
    "        x = torch.cat([a,b,c], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = 1e-3\n",
    "                m.momentum = 0.03\n",
    "\n",
    "\n",
    "model = get_model(opt,\n",
    "                  # head=IdentityModule(),\n",
    "                  head=ClassificationHead([80,40,20], [64,128,256], 5),\n",
    "                  freeze_layers=True)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 images\n",
      "Input image batch of shape: torch.Size([2, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# Load Images\n",
    "img_dir = 'imgs/'\n",
    "images = [cv2.imread(str(im)) for im in Path(img_dir).glob('*.jpg')]\n",
    "print(f'There are {len(images)} images')\n",
    "inp_imgs = np.zeros([len(images), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images):\n",
    "    img, r = preproc(image, opt.test_size, opt.rgb_means, opt.std)\n",
    "    inp_imgs[b_i] = img\n",
    "\n",
    "inp_imgs = torch.from_numpy(inp_imgs).to(opt.device)\n",
    "print(f'Input image batch of shape: {inp_imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs.\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(inp_imgs)\n",
    "    # print(yolo_outputs)\n",
    "    print(yolo_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TODO: Implement a custom training loop\n",
    "As long as `get_model` is called with `freeze_layers=True`, the early layers (the YOLOX pretrained ones) will be frozen, so training should be fast--only the head needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load Images\n",
    "while not stopping_condition:\n",
    "    # Take input\n",
    "    # Send it through the model\n",
    "    # Calculate the loss\n",
    "    # Run back propagation\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
