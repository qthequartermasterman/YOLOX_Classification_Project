{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIqOUQPKntq-"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJUk_dFhb7RF"
   },
   "source": [
    "Reference Link: https://www.analyticsvidhya.com/blog/2021/10/human-pose-estimation-using-machine-learning-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4VepFNk-cZY5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Preparation\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dtAAkvRYcbY4",
    "outputId": "aeae7d69-f165-4804-cf47-022b77a321d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traning the baseline model with SVM\n",
    "from sklearn.svm import SVC\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "model = SVC(kernel = 'poly')\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'downdog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'downdog'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/qpmqnv0x16q_mzjksztb28n40000gn/T/ipykernel_63333/678569049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisibility\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'downdog'"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = labelencoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm1vgXjEnuhH"
   },
   "source": [
    "# Implementing YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kcdC17YEcgTN"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, x):\n",
    "        return self['x']\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "--> Drop parameter head.stems.0.conv.weight.\n",
      "--> Drop parameter head.stems.0.bn.weight.\n",
      "--> Drop parameter head.stems.0.bn.bias.\n",
      "--> Drop parameter head.stems.0.bn.running_mean.\n",
      "--> Drop parameter head.stems.0.bn.running_var.\n",
      "--> Drop parameter head.stems.0.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.1.conv.weight.\n",
      "--> Drop parameter head.stems.1.bn.weight.\n",
      "--> Drop parameter head.stems.1.bn.bias.\n",
      "--> Drop parameter head.stems.1.bn.running_mean.\n",
      "--> Drop parameter head.stems.1.bn.running_var.\n",
      "--> Drop parameter head.stems.1.bn.num_batches_tracked.\n",
      "--> Drop parameter head.stems.2.conv.weight.\n",
      "--> Drop parameter head.stems.2.bn.weight.\n",
      "--> Drop parameter head.stems.2.bn.bias.\n",
      "--> Drop parameter head.stems.2.bn.running_mean.\n",
      "--> Drop parameter head.stems.2.bn.running_var.\n",
      "--> Drop parameter head.stems.2.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.cls_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.0.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.1.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.0.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.dconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.conv.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.weight.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.bias.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_mean.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.running_var.\n",
      "--> Drop parameter head.reg_convs.2.1.pconv.bn.num_batches_tracked.\n",
      "--> Drop parameter head.cls_preds.0.weight.\n",
      "--> Drop parameter head.cls_preds.0.bias.\n",
      "--> Drop parameter head.cls_preds.1.weight.\n",
      "--> Drop parameter head.cls_preds.1.bias.\n",
      "--> Drop parameter head.cls_preds.2.weight.\n",
      "--> Drop parameter head.cls_preds.2.bias.\n",
      "--> Drop parameter head.reg_preds.0.weight.\n",
      "--> Drop parameter head.reg_preds.0.bias.\n",
      "--> Drop parameter head.reg_preds.1.weight.\n",
      "--> Drop parameter head.reg_preds.1.bias.\n",
      "--> Drop parameter head.reg_preds.2.weight.\n",
      "--> Drop parameter head.reg_preds.2.bias.\n",
      "--> Drop parameter head.obj_preds.0.weight.\n",
      "--> Drop parameter head.obj_preds.0.bias.\n",
      "--> Drop parameter head.obj_preds.1.weight.\n",
      "--> Drop parameter head.obj_preds.1.bias.\n",
      "--> Drop parameter head.obj_preds.2.weight.\n",
      "--> Drop parameter head.obj_preds.2.bias.\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "model = get_model(opt,\n",
    "                  head=IdentityModule(),\n",
    "                  freeze_layers=True)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 images\n",
      "Input image batch of shape: torch.Size([2, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# Load Images\n",
    "img_dir = 'imgs/'\n",
    "images = [cv2.imread(str(im)) for im in Path(img_dir).glob('*.jpg')]\n",
    "print(f'There are {len(images)} images')\n",
    "inp_imgs = np.zeros([len(images), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images):\n",
    "    img, r = preproc(image, opt.test_size, opt.rgb_means, opt.std)\n",
    "    inp_imgs[b_i] = img\n",
    "\n",
    "inp_imgs = torch.from_numpy(inp_imgs).to(opt.device)\n",
    "print(f'Input image batch of shape: {inp_imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep_learning_class/lib/python3.8/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[torch.Size([2, 64, 80, 80]), torch.Size([2, 128, 40, 40]), torch.Size([2, 256, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs.\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(inp_imgs)\n",
    "    # print(yolo_outputs)\n",
    "    print(len(yolo_outputs))\n",
    "    print([t.shape for t in yolo_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## TODO: Implement Network Head for classification (probably some conv layers and a few fully connected layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TODO: Implement a custom training loop\n",
    "As long as `get_model` is called with `freeze_layers=True`, the early layers (the YOLOX pretrained ones) will be frozen, so training should be fast--only the head needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
