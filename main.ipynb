{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Preparation\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "images_arrays = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365975</td>\n",
       "      <td>0.297709</td>\n",
       "      <td>-0.385955</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.375471</td>\n",
       "      <td>0.288381</td>\n",
       "      <td>-0.355241</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.381822</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728874</td>\n",
       "      <td>0.343049</td>\n",
       "      <td>0.770953</td>\n",
       "      <td>-0.013308</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>0.373077</td>\n",
       "      <td>0.595134</td>\n",
       "      <td>0.249858</td>\n",
       "      <td>0.641886</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.517337</td>\n",
       "      <td>0.273278</td>\n",
       "      <td>-0.649164</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.532713</td>\n",
       "      <td>0.258381</td>\n",
       "      <td>-0.619042</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.540860</td>\n",
       "      <td>0.258364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696976</td>\n",
       "      <td>0.481968</td>\n",
       "      <td>0.983146</td>\n",
       "      <td>-0.157763</td>\n",
       "      <td>0.966885</td>\n",
       "      <td>0.516807</td>\n",
       "      <td>0.731737</td>\n",
       "      <td>0.358949</td>\n",
       "      <td>0.615583</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.493811</td>\n",
       "      <td>0.170744</td>\n",
       "      <td>-0.357681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.506060</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>-0.318918</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.513639</td>\n",
       "      <td>0.155501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695904</td>\n",
       "      <td>0.475992</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>0.966968</td>\n",
       "      <td>0.425618</td>\n",
       "      <td>0.903785</td>\n",
       "      <td>0.344804</td>\n",
       "      <td>0.643306</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363192</td>\n",
       "      <td>0.253992</td>\n",
       "      <td>-0.522942</td>\n",
       "      <td>0.999566</td>\n",
       "      <td>0.375098</td>\n",
       "      <td>0.241423</td>\n",
       "      <td>-0.478249</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.383550</td>\n",
       "      <td>0.241243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705867</td>\n",
       "      <td>0.357126</td>\n",
       "      <td>0.674954</td>\n",
       "      <td>0.499544</td>\n",
       "      <td>0.908316</td>\n",
       "      <td>0.388761</td>\n",
       "      <td>0.913819</td>\n",
       "      <td>0.144282</td>\n",
       "      <td>0.663821</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.402617</td>\n",
       "      <td>0.247819</td>\n",
       "      <td>-0.569452</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.394126</td>\n",
       "      <td>0.238084</td>\n",
       "      <td>-0.583032</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.396659</td>\n",
       "      <td>0.234638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721907</td>\n",
       "      <td>0.743622</td>\n",
       "      <td>0.945898</td>\n",
       "      <td>0.123149</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.413644</td>\n",
       "      <td>0.941780</td>\n",
       "      <td>0.413074</td>\n",
       "      <td>0.692597</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOSE_x    NOSE_y    NOSE_z  NOSE_vis  LEFT_EYE_INNER_x  LEFT_EYE_INNER_y  \\\n",
       "0  0.365975  0.297709 -0.385955  0.999924          0.375471          0.288381   \n",
       "1  0.517337  0.273278 -0.649164  0.999928          0.532713          0.258381   \n",
       "2  0.493811  0.170744 -0.357681  0.999934          0.506060          0.155396   \n",
       "3  0.363192  0.253992 -0.522942  0.999566          0.375098          0.241423   \n",
       "4  0.402617  0.247819 -0.569452  0.998938          0.394126          0.238084   \n",
       "\n",
       "   LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  LEFT_EYE_y  ...  \\\n",
       "0         -0.355241            0.999899    0.381822    0.288810  ...   \n",
       "1         -0.619042            0.999905    0.540860    0.258364  ...   \n",
       "2         -0.318918            0.999913    0.513639    0.155501  ...   \n",
       "3         -0.478249            0.999525    0.383550    0.241243  ...   \n",
       "4         -0.583032            0.998867    0.396659    0.234638  ...   \n",
       "\n",
       "   RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  LEFT_FOOT_INDEX_z  \\\n",
       "0        0.728874           0.343049           0.770953          -0.013308   \n",
       "1        0.696976           0.481968           0.983146          -0.157763   \n",
       "2        0.695904           0.475992           0.930432           0.089517   \n",
       "3        0.705867           0.357126           0.674954           0.499544   \n",
       "4        0.721907           0.743622           0.945898           0.123149   \n",
       "\n",
       "   LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  RIGHT_FOOT_INDEX_y  \\\n",
       "0             0.970696            0.373077            0.595134   \n",
       "1             0.966885            0.516807            0.731737   \n",
       "2             0.966968            0.425618            0.903785   \n",
       "3             0.908316            0.388761            0.913819   \n",
       "4             0.913700            0.413644            0.941780   \n",
       "\n",
       "   RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  target  \n",
       "0            0.249858              0.641886       3  \n",
       "1            0.358949              0.615583       3  \n",
       "2            0.344804              0.643306       3  \n",
       "3            0.144282              0.663821       3  \n",
       "4            0.413074              0.692597       3  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying Dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7025789059aa447498a69a8a2ddcdef1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7025789059aa447498a69a8a2ddcdef1\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7025789059aa447498a69a8a2ddcdef1\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-28cf31dbd6aae41f98696110b2d022f6\"}, \"mark\": {\"type\": \"bar\", \"size\": 50}, \"encoding\": {\"color\": {\"field\": \"label\", \"type\": \"nominal\"}, \"tooltip\": [{\"aggregate\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, {\"field\": \"label\", \"type\": \"nominal\"}], \"x\": {\"axis\": {\"title\": \"Pose\"}, \"field\": \"label\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"title\": \"Count\"}, \"type\": \"quantitative\"}}, \"height\": 300, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Number of data in each pose\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-28cf31dbd6aae41f98696110b2d022f6\": [{\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.DataFrame()\n",
    "label_df['label'] = list(map(lambda x: labelencoder.inverse_transform([x])[0], data['target']))\n",
    "\n",
    "bars = alt.Chart(label_df).mark_bar(size=50).encode(\n",
    "    x=alt.X('label', axis=alt.Axis(title='Pose')),\n",
    "    y=alt.Y(\"count()\", axis=alt.Axis(title='Count')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Count'), 'label'],\n",
    "    color='label'\n",
    ")\n",
    "\n",
    "(bars).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"Number of data in each pose\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Yolox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    # def __getattr__(self, x):\n",
    "    #     return self[x]\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack\n"
     ]
    }
   ],
   "source": [
    "from yolox.data_augment import random_perspective\n",
    "path = \"dataset/train/\"\n",
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "num_transformations = 0\n",
    "\n",
    "images_arrays = []\n",
    "target = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "#         print(img)\n",
    "        img = cv2.imread(img)\n",
    "        \n",
    "#         print('transformations')\n",
    "        transformations = [img] + [random_perspective(img, scale=[0.5,1.2])[0] for _ in range(num_transformations)]\n",
    "#         print('imgs')\n",
    "        imgs = [preproc(imgx, opt.test_size, opt.rgb_means, opt.std)[0] for imgx in transformations]\n",
    "#         img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        label = subdir.replace(path, '')\n",
    "        \n",
    "        \n",
    "        images_arrays.extend(imgs)\n",
    "        target.extend([label]*(len(imgs)))\n",
    "\n",
    "print('stack')\n",
    "# inp_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "# for b_i, image in enumerate(images_arrays):\n",
    "#     inp_imgs[b_i] = image\n",
    "inp_imgs = np.stack(images_arrays).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 3, 640, 640)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 ... 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "target = labelencoder.fit_transform(target)\n",
    "\n",
    "# Shuffle them, otherwise our training gets all screwy\n",
    "inp_imgs, target = shuffle(inp_imgs, target)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(images_arrays[1])\n",
    "print(images_arrays[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a baseline model and your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining baseline model\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "baseline_model = SVC(kernel = 'poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_sizes:List[int], input_channels:List[int], num_classes:int,\n",
    "                 hidden_features:int = 128, dropout=.5, num_hidden_layers=1):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc0a = nn.Linear(input_channels[0]*input_sizes[0]**2,hidden_features)\n",
    "        self.fc0b = nn.Linear(input_channels[1]*input_sizes[1]**2,hidden_features)\n",
    "        self.fc0c = nn.Linear(input_channels[2]*input_sizes[2]**2,hidden_features)\n",
    "        # Concatenate the three outputs into one linear layer\n",
    "        self.fc1 = nn.Linear(len(input_sizes) * hidden_features, hidden_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        # self.dropout2 = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.Sequential(*[nn.Sequential(nn.Linear(hidden_features, hidden_features),\n",
    "                                                           nn.ReLU(),\n",
    "                                                           nn.Dropout(dropout),\n",
    "                                                           )\n",
    "                                             for _ in range(num_hidden_layers)\n",
    "                                             ]\n",
    "                                           )\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a = F.relu(self.fc0a(torch.flatten(x[0],1)))\n",
    "        b = F.relu(self.fc0b(torch.flatten(x[1],1)))\n",
    "        c = F.relu(self.fc0c(torch.flatten(x[2],1)))\n",
    "        x = torch.cat([a,b,c], dim=1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.hidden_layers(x)\n",
    "        # x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = 1e-3\n",
    "                m.momentum = 0.03\n",
    "\n",
    "def generate_model(opt, hidden_features=128, dropout=0.5, freeze_layers=True, num_hidden_layers=1):\n",
    "    return get_model(opt,\n",
    "                     head=ClassificationHead([80,40,20], [64,128,256], 5,\n",
    "                                             hidden_features=hidden_features, dropout=dropout,\n",
    "                                             num_hidden_layers=num_hidden_layers),\n",
    "                     freeze_layers=freeze_layers)\n",
    "\n",
    "\n",
    "model = generate_model(opt)\n",
    "model.to(opt.device)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Run a training loop on a training set with both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs (i.e. all tensors are the right shape)\n",
    "# Use only the first 10 images, for speed's sake\n",
    "model = model.to(opt.device)\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(torch.as_tensor(inp_imgs[:10]).to(opt.device))\n",
    "    # print(yolo_outputs)\n",
    "    print(yolo_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Take this pre-generated model off the GPU so we have space\n",
    "try:\n",
    "    del yolo_outputs\n",
    "except: pass\n",
    "try:\n",
    "    del model\n",
    "except: pass\n",
    "try:\n",
    "    del gs\n",
    "except: pass\n",
    "try:\n",
    "    del net\n",
    "except: pass\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            print('cuda', obj.is_cuda)\n",
    "    except:\n",
    "        pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.1615\u001B[0m       \u001B[32m0.3264\u001B[0m        \u001B[35m1.5346\u001B[0m     +  1.4537\n",
      "      2        \u001B[36m1.6570\u001B[0m       0.2708        \u001B[35m1.5254\u001B[0m     +  1.4630\n",
      "      3        \u001B[36m1.5281\u001B[0m       \u001B[32m0.4236\u001B[0m        \u001B[35m1.4407\u001B[0m     +  1.4755\n",
      "      4        \u001B[36m1.5053\u001B[0m       \u001B[32m0.5486\u001B[0m        \u001B[35m1.3852\u001B[0m     +  1.4690\n",
      "      5        \u001B[36m1.4231\u001B[0m       0.5069        \u001B[35m1.2742\u001B[0m     +  1.4628\n",
      "      6        \u001B[36m1.3497\u001B[0m       0.4722        1.3009        1.4549\n",
      "      7        \u001B[36m1.2771\u001B[0m       0.5417        \u001B[35m1.2052\u001B[0m     +  1.4479\n",
      "      8        \u001B[36m1.1360\u001B[0m       \u001B[32m0.6111\u001B[0m        \u001B[35m1.0904\u001B[0m     +  1.4672\n",
      "      9        \u001B[36m0.9850\u001B[0m       0.6042        \u001B[35m1.0172\u001B[0m     +  1.4625\n",
      "     10        \u001B[36m0.8913\u001B[0m       0.6042        1.0211        1.4690\n",
      "     11        \u001B[36m0.7399\u001B[0m       \u001B[32m0.6597\u001B[0m        \u001B[35m0.9809\u001B[0m     +  1.4448\n",
      "     12        \u001B[36m0.6700\u001B[0m       0.6250        \u001B[35m0.9258\u001B[0m     +  1.4643\n",
      "     13        \u001B[36m0.5519\u001B[0m       0.6528        0.9384        1.4660\n",
      "     14        \u001B[36m0.5237\u001B[0m       0.6458        \u001B[35m0.9197\u001B[0m     +  1.4492\n",
      "     15        \u001B[36m0.5069\u001B[0m       0.6389        0.9761        1.4730\n",
      "     16        \u001B[36m0.4305\u001B[0m       0.6181        1.0433        1.4562\n",
      "     17        \u001B[36m0.4100\u001B[0m       0.6597        1.0442        1.4468\n",
      "     18        \u001B[36m0.3076\u001B[0m       0.6597        1.1149        1.4469\n",
      "     19        \u001B[36m0.3073\u001B[0m       \u001B[32m0.6806\u001B[0m        1.0195        1.4473\n",
      "     20        \u001B[36m0.2477\u001B[0m       \u001B[32m0.6875\u001B[0m        0.9403        1.4472\n",
      "     21        0.2732       \u001B[32m0.7014\u001B[0m        \u001B[35m0.8722\u001B[0m     +  1.4468\n",
      "     22        \u001B[36m0.1431\u001B[0m       0.7014        0.9941        1.4575\n",
      "     23        \u001B[36m0.1400\u001B[0m       0.6736        1.2866        1.4452\n",
      "     24        \u001B[36m0.1102\u001B[0m       \u001B[32m0.7083\u001B[0m        1.1194        1.4478\n",
      "     25        \u001B[36m0.1037\u001B[0m       0.7083        1.1866        1.4463\n",
      "     26        \u001B[36m0.0985\u001B[0m       \u001B[32m0.7153\u001B[0m        1.1230        1.4447\n",
      "     27        \u001B[36m0.0808\u001B[0m       \u001B[32m0.7222\u001B[0m        1.1967        1.4561\n",
      "     28        \u001B[36m0.0634\u001B[0m       0.7153        1.2200        1.4449\n",
      "     29        \u001B[36m0.0541\u001B[0m       0.7014        1.2249        1.4530\n",
      "     30        0.0791       0.6597        1.4834        1.4453\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.561 total time=  54.9s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0965\u001B[0m       \u001B[32m0.3103\u001B[0m        \u001B[35m1.5648\u001B[0m     +  1.4670\n",
      "      2        \u001B[36m1.6950\u001B[0m       0.2759        \u001B[35m1.5577\u001B[0m     +  1.4942\n",
      "      3        \u001B[36m1.6166\u001B[0m       \u001B[32m0.3241\u001B[0m        \u001B[35m1.5265\u001B[0m     +  1.4855\n",
      "      4        \u001B[36m1.5714\u001B[0m       \u001B[32m0.3724\u001B[0m        \u001B[35m1.4896\u001B[0m     +  1.5030\n",
      "      5        \u001B[36m1.5710\u001B[0m       0.2897        1.5247        1.4921\n",
      "      6        1.5731       0.3103        1.5331        1.4586\n",
      "      7        \u001B[36m1.5320\u001B[0m       0.3241        \u001B[35m1.4233\u001B[0m     +  1.4535\n",
      "      8        \u001B[36m1.4848\u001B[0m       0.3448        1.4449        1.4656\n",
      "      9        \u001B[36m1.4577\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.3826\u001B[0m     +  1.4548\n",
      "     10        \u001B[36m1.4488\u001B[0m       \u001B[32m0.4207\u001B[0m        \u001B[35m1.3141\u001B[0m     +  1.4704\n",
      "     11        \u001B[36m1.4443\u001B[0m       0.3793        1.3592        1.4650\n",
      "     12        \u001B[36m1.3128\u001B[0m       \u001B[32m0.4552\u001B[0m        \u001B[35m1.1982\u001B[0m     +  1.4508\n",
      "     13        \u001B[36m1.2720\u001B[0m       \u001B[32m0.5172\u001B[0m        1.2795        1.4725\n",
      "     14        \u001B[36m1.2030\u001B[0m       \u001B[32m0.5241\u001B[0m        1.2729        1.4680\n",
      "     15        \u001B[36m1.1985\u001B[0m       \u001B[32m0.5655\u001B[0m        \u001B[35m1.1916\u001B[0m     +  1.4526\n",
      "     16        \u001B[36m1.1706\u001B[0m       0.4621        \u001B[35m1.1202\u001B[0m     +  1.4709\n",
      "     17        \u001B[36m1.0684\u001B[0m       0.5655        \u001B[35m1.0672\u001B[0m     +  1.4719\n",
      "     18        \u001B[36m0.9965\u001B[0m       \u001B[32m0.5724\u001B[0m        \u001B[35m0.9420\u001B[0m     +  1.4637\n",
      "     19        \u001B[36m0.9514\u001B[0m       \u001B[32m0.6759\u001B[0m        0.9713        1.4728\n",
      "     20        \u001B[36m0.8448\u001B[0m       0.6414        1.0308        1.4512\n",
      "     21        \u001B[36m0.7473\u001B[0m       0.6621        1.0787        1.4493\n",
      "     22        \u001B[36m0.7163\u001B[0m       \u001B[32m0.6897\u001B[0m        \u001B[35m0.8830\u001B[0m     +  1.4517\n",
      "     23        \u001B[36m0.6403\u001B[0m       0.6552        \u001B[35m0.8693\u001B[0m     +  1.4704\n",
      "     24        \u001B[36m0.5800\u001B[0m       \u001B[32m0.7379\u001B[0m        \u001B[35m0.7782\u001B[0m     +  1.4733\n",
      "     25        \u001B[36m0.4520\u001B[0m       0.5931        0.9858        1.4680\n",
      "     26        \u001B[36m0.3762\u001B[0m       0.6966        \u001B[35m0.7699\u001B[0m     +  1.4507\n",
      "     27        \u001B[36m0.2869\u001B[0m       0.6966        0.7739        1.4698\n",
      "     28        \u001B[36m0.2653\u001B[0m       0.6966        0.7750        1.4499\n",
      "     29        \u001B[36m0.2579\u001B[0m       0.6552        0.8548        1.4506\n",
      "     30        \u001B[36m0.2233\u001B[0m       0.7034        0.8515        1.4506\n",
      "     31        0.2311       0.6621        1.1503        1.4519\n",
      "     32        \u001B[36m0.1565\u001B[0m       0.6897        1.0333        1.4517\n",
      "     33        0.1731       0.7310        0.9774        1.5012\n",
      "     34        \u001B[36m0.1403\u001B[0m       0.6966        0.9940        1.4582\n",
      "     35        \u001B[36m0.1202\u001B[0m       0.7379        0.8504        1.4625\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.161 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.3995\u001B[0m       \u001B[32m0.2966\u001B[0m        \u001B[35m1.5370\u001B[0m     +  1.4625\n",
      "      2        \u001B[36m1.5840\u001B[0m       \u001B[32m0.3586\u001B[0m        \u001B[35m1.5152\u001B[0m     +  1.4748\n",
      "      3        \u001B[36m1.5742\u001B[0m       \u001B[32m0.3793\u001B[0m        \u001B[35m1.4834\u001B[0m     +  1.4763\n",
      "      4        1.5804       0.3655        \u001B[35m1.4474\u001B[0m     +  1.4827\n",
      "      5        \u001B[36m1.5098\u001B[0m       0.3793        1.4504        1.4688\n",
      "      6        1.5361       0.3448        \u001B[35m1.4311\u001B[0m     +  1.4520\n",
      "      7        1.5314       \u001B[32m0.4345\u001B[0m        \u001B[35m1.4270\u001B[0m     +  1.4741\n",
      "      8        \u001B[36m1.4440\u001B[0m       0.4069        \u001B[35m1.3503\u001B[0m     +  1.4782\n",
      "      9        \u001B[36m1.4162\u001B[0m       0.3931        \u001B[35m1.3427\u001B[0m     +  1.4732\n",
      "     10        1.4399       0.4207        1.3615        1.4801\n",
      "     11        \u001B[36m1.2599\u001B[0m       \u001B[32m0.4759\u001B[0m        1.3771        1.4563\n",
      "     12        1.3094       0.4138        \u001B[35m1.2895\u001B[0m     +  1.4505\n",
      "     13        \u001B[36m1.2400\u001B[0m       0.4345        \u001B[35m1.2147\u001B[0m     +  1.4666\n",
      "     14        \u001B[36m1.1415\u001B[0m       0.4345        1.3150        1.4756\n",
      "     15        \u001B[36m1.0817\u001B[0m       \u001B[32m0.5310\u001B[0m        \u001B[35m1.1008\u001B[0m     +  1.4476\n",
      "     16        \u001B[36m1.0499\u001B[0m       \u001B[32m0.6345\u001B[0m        1.1094        1.4717\n",
      "     17        \u001B[36m0.8747\u001B[0m       0.5793        1.1531        1.4514\n",
      "     18        0.8858       0.5517        \u001B[35m1.0849\u001B[0m     +  1.4531\n",
      "     19        \u001B[36m0.7178\u001B[0m       \u001B[32m0.6483\u001B[0m        \u001B[35m1.0699\u001B[0m     +  1.4697\n",
      "     20        \u001B[36m0.6401\u001B[0m       \u001B[32m0.6690\u001B[0m        1.0868        1.4767\n",
      "     21        \u001B[36m0.5329\u001B[0m       0.6000        1.3027        1.4507\n",
      "     22        \u001B[36m0.4736\u001B[0m       0.6621        1.1684        1.4500\n",
      "     23        \u001B[36m0.3965\u001B[0m       \u001B[32m0.6897\u001B[0m        1.0800        1.4506\n",
      "     24        \u001B[36m0.3278\u001B[0m       \u001B[32m0.6966\u001B[0m        \u001B[35m1.0168\u001B[0m     +  1.4509\n",
      "     25        \u001B[36m0.2772\u001B[0m       \u001B[32m0.7034\u001B[0m        1.1478        1.4683\n",
      "     26        0.3341       0.6897        1.0632        1.4529\n",
      "     27        \u001B[36m0.2368\u001B[0m       0.6966        1.0693        1.4495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001B[36m0.2082\u001B[0m       0.6690        1.3189        1.4526\n",
      "     29        \u001B[36m0.1613\u001B[0m       0.6828        1.2722        1.4503\n",
      "     30        \u001B[36m0.1447\u001B[0m       0.6966        1.5304        1.4682\n",
      "     31        0.1632       0.6690        1.7649        1.4715\n",
      "     32        0.1740       0.6897        1.2104        1.4695\n",
      "     33        0.1492       0.6690        1.4053        1.4666\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=1;, score=-0.958 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0393\u001B[0m       \u001B[32m0.2778\u001B[0m        \u001B[35m1.5839\u001B[0m     +  1.4658\n",
      "      2        \u001B[36m1.6608\u001B[0m       \u001B[32m0.3194\u001B[0m        \u001B[35m1.5792\u001B[0m     +  1.4671\n",
      "      3        \u001B[36m1.5976\u001B[0m       0.3194        \u001B[35m1.5480\u001B[0m     +  1.4685\n",
      "      4        1.5991       0.3194        \u001B[35m1.5456\u001B[0m     +  1.4686\n",
      "      5        \u001B[36m1.5687\u001B[0m       \u001B[32m0.3611\u001B[0m        \u001B[35m1.5208\u001B[0m     +  1.4750\n",
      "      6        \u001B[36m1.5225\u001B[0m       0.3611        \u001B[35m1.5027\u001B[0m     +  1.4712\n",
      "      7        1.5300       \u001B[32m0.4514\u001B[0m        \u001B[35m1.4758\u001B[0m     +  1.4790\n",
      "      8        \u001B[36m1.4713\u001B[0m       0.3958        \u001B[35m1.4386\u001B[0m     +  1.4930\n",
      "      9        \u001B[36m1.4623\u001B[0m       0.4236        \u001B[35m1.3820\u001B[0m     +  1.4950\n",
      "     10        \u001B[36m1.3873\u001B[0m       0.4514        1.3876        1.4850\n",
      "     11        \u001B[36m1.3427\u001B[0m       \u001B[32m0.4931\u001B[0m        \u001B[35m1.2962\u001B[0m     +  1.4567\n",
      "     12        \u001B[36m1.2598\u001B[0m       \u001B[32m0.5764\u001B[0m        \u001B[35m1.2293\u001B[0m     +  1.4878\n",
      "     13        \u001B[36m1.1623\u001B[0m       0.5278        \u001B[35m1.1551\u001B[0m     +  1.4757\n",
      "     14        \u001B[36m1.0746\u001B[0m       \u001B[32m0.6181\u001B[0m        \u001B[35m1.0746\u001B[0m     +  1.4705\n",
      "     15        \u001B[36m0.9802\u001B[0m       0.6181        \u001B[35m1.0623\u001B[0m     +  1.4789\n",
      "     16        \u001B[36m0.9155\u001B[0m       \u001B[32m0.6806\u001B[0m        \u001B[35m1.0236\u001B[0m     +  1.4970\n",
      "     17        \u001B[36m0.8659\u001B[0m       0.6528        \u001B[35m1.0199\u001B[0m     +  1.4869\n",
      "     18        \u001B[36m0.7599\u001B[0m       0.6111        \u001B[35m0.9903\u001B[0m     +  1.4815\n",
      "     19        \u001B[36m0.7340\u001B[0m       0.6389        1.0197        1.4958\n",
      "     20        \u001B[36m0.6270\u001B[0m       0.6667        \u001B[35m0.9760\u001B[0m     +  1.4630\n",
      "     21        \u001B[36m0.5709\u001B[0m       0.6528        \u001B[35m0.9559\u001B[0m     +  1.4770\n",
      "     22        0.6074       0.6597        \u001B[35m0.9394\u001B[0m     +  1.4953\n",
      "     23        \u001B[36m0.5621\u001B[0m       0.6250        0.9897        1.4821\n",
      "     24        \u001B[36m0.4987\u001B[0m       \u001B[32m0.6875\u001B[0m        \u001B[35m0.8813\u001B[0m     +  1.4638\n",
      "     25        \u001B[36m0.4313\u001B[0m       0.6597        0.9013        1.4911\n",
      "     26        \u001B[36m0.3786\u001B[0m       0.6528        0.9492        1.4695\n",
      "     27        \u001B[36m0.3783\u001B[0m       0.6528        0.9570        1.4632\n",
      "     28        \u001B[36m0.2996\u001B[0m       0.6319        1.1023        1.4622\n",
      "     29        \u001B[36m0.2905\u001B[0m       0.6319        1.1206        1.4621\n",
      "     30        \u001B[36m0.2892\u001B[0m       0.6667        1.1036        1.4622\n",
      "     31        \u001B[36m0.2639\u001B[0m       0.6042        1.2796        1.4618\n",
      "     32        \u001B[36m0.2426\u001B[0m       0.6458        1.0768        1.4614\n",
      "     33        \u001B[36m0.2291\u001B[0m       0.6875        1.1188        1.4768\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-0.937 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9755\u001B[0m       \u001B[32m0.2966\u001B[0m        \u001B[35m1.5846\u001B[0m     +  1.5066\n",
      "      2        \u001B[36m1.6363\u001B[0m       \u001B[32m0.3172\u001B[0m        \u001B[35m1.5833\u001B[0m     +  1.4803\n",
      "      3        \u001B[36m1.6186\u001B[0m       0.2621        \u001B[35m1.5822\u001B[0m     +  1.4735\n",
      "      4        1.6242       \u001B[32m0.3379\u001B[0m        \u001B[35m1.5644\u001B[0m     +  1.4856\n",
      "      5        \u001B[36m1.5770\u001B[0m       0.3379        \u001B[35m1.5530\u001B[0m     +  1.4754\n",
      "      6        1.6036       0.3310        \u001B[35m1.5393\u001B[0m     +  1.4767\n",
      "      7        \u001B[36m1.5500\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.5230\u001B[0m     +  1.4928\n",
      "      8        \u001B[36m1.5413\u001B[0m       0.3793        \u001B[35m1.4838\u001B[0m     +  1.4762\n",
      "      9        \u001B[36m1.5033\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.4682\u001B[0m     +  1.4906\n",
      "     10        \u001B[36m1.4667\u001B[0m       \u001B[32m0.4207\u001B[0m        \u001B[35m1.4356\u001B[0m     +  1.4838\n",
      "     11        \u001B[36m1.4407\u001B[0m       \u001B[32m0.4345\u001B[0m        \u001B[35m1.4161\u001B[0m     +  1.4763\n",
      "     12        1.4491       0.4000        \u001B[35m1.4066\u001B[0m     +  1.4937\n",
      "     13        \u001B[36m1.4125\u001B[0m       \u001B[32m0.4552\u001B[0m        \u001B[35m1.3712\u001B[0m     +  1.4964\n",
      "     14        \u001B[36m1.3042\u001B[0m       \u001B[32m0.4828\u001B[0m        \u001B[35m1.3185\u001B[0m     +  1.5012\n",
      "     15        \u001B[36m1.2532\u001B[0m       0.4345        \u001B[35m1.2748\u001B[0m     +  1.4826\n",
      "     16        \u001B[36m1.2260\u001B[0m       \u001B[32m0.5034\u001B[0m        \u001B[35m1.2533\u001B[0m     +  1.4833\n",
      "     17        \u001B[36m1.1369\u001B[0m       0.4414        \u001B[35m1.2296\u001B[0m     +  1.4908\n",
      "     18        \u001B[36m1.0685\u001B[0m       \u001B[32m0.5310\u001B[0m        1.2359        1.4916\n",
      "     19        \u001B[36m1.0299\u001B[0m       \u001B[32m0.5724\u001B[0m        1.2546        1.4596\n",
      "     20        \u001B[36m0.9856\u001B[0m       \u001B[32m0.5862\u001B[0m        \u001B[35m1.1929\u001B[0m     +  1.4585\n",
      "     21        \u001B[36m0.9414\u001B[0m       0.5793        \u001B[35m1.1388\u001B[0m     +  1.4793\n",
      "     22        \u001B[36m0.8591\u001B[0m       \u001B[32m0.6276\u001B[0m        1.1942        1.4762\n",
      "     23        \u001B[36m0.8447\u001B[0m       0.5931        1.1773        1.4580\n",
      "     24        \u001B[36m0.7806\u001B[0m       0.5655        1.2189        1.4586\n",
      "     25        \u001B[36m0.7333\u001B[0m       0.5931        \u001B[35m1.1049\u001B[0m     +  1.4610\n",
      "     26        \u001B[36m0.7245\u001B[0m       0.6069        1.1240        1.4855\n",
      "     27        \u001B[36m0.6276\u001B[0m       \u001B[32m0.6552\u001B[0m        \u001B[35m1.0903\u001B[0m     +  1.4713\n",
      "     28        \u001B[36m0.6126\u001B[0m       0.6345        \u001B[35m1.0315\u001B[0m     +  1.4807\n",
      "     29        \u001B[36m0.5792\u001B[0m       0.6000        1.0859        1.4779\n",
      "     30        \u001B[36m0.4954\u001B[0m       \u001B[32m0.6966\u001B[0m        \u001B[35m1.0298\u001B[0m     +  1.4612\n",
      "     31        0.5080       0.6000        1.2499        1.4809\n",
      "     32        0.5912       0.6552        1.0324        1.4618\n",
      "     33        \u001B[36m0.4790\u001B[0m       0.6207        1.0558        1.4640\n",
      "     34        \u001B[36m0.4385\u001B[0m       0.6414        \u001B[35m0.9965\u001B[0m     +  1.4600\n",
      "     35        \u001B[36m0.4186\u001B[0m       \u001B[32m0.7172\u001B[0m        1.1116        1.4939\n",
      "     36        \u001B[36m0.4060\u001B[0m       0.6828        1.0420        1.4926\n",
      "     37        \u001B[36m0.3905\u001B[0m       0.6690        1.1718        1.4729\n",
      "     38        \u001B[36m0.3494\u001B[0m       0.7034        1.0098        1.4668\n",
      "     39        \u001B[36m0.3346\u001B[0m       0.6552        1.0854        1.4605\n",
      "     40        0.3464       0.6483        1.3218        1.4596\n",
      "     41        \u001B[36m0.2941\u001B[0m       0.6690        1.0772        1.4607\n",
      "     42        \u001B[36m0.2344\u001B[0m       0.6759        1.2034        1.4653\n",
      "     43        \u001B[36m0.2269\u001B[0m       0.6690        1.2655        1.4614\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.538 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0754\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5951\u001B[0m     +  1.4879\n",
      "      2        \u001B[36m1.6154\u001B[0m       0.2414        1.6029        1.4946\n",
      "      3        \u001B[36m1.6137\u001B[0m       0.2483        1.6005        1.4791\n",
      "      4        \u001B[36m1.6084\u001B[0m       0.2483        1.6031        1.4782\n",
      "      5        1.6244       0.2414        1.5997        1.4722\n",
      "      6        \u001B[36m1.6009\u001B[0m       0.2483        1.5996        1.4717\n",
      "      7        1.6123       0.2483        1.5962        1.4714\n",
      "      8        1.6298       0.2483        1.6044        1.4758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        1.6162       0.2483        1.6013        1.4735\n",
      "     10        1.6158       0.2483        \u001B[35m1.5939\u001B[0m     +  1.4740\n",
      "     11        \u001B[36m1.5977\u001B[0m       0.2483        1.5965        1.5011\n",
      "     12        \u001B[36m1.5914\u001B[0m       0.2483        \u001B[35m1.5768\u001B[0m     +  1.4723\n",
      "     13        \u001B[36m1.5691\u001B[0m       \u001B[32m0.2552\u001B[0m        \u001B[35m1.5329\u001B[0m     +  1.4926\n",
      "     14        1.5749       0.2483        1.5356        1.5007\n",
      "     15        \u001B[36m1.5338\u001B[0m       0.2483        \u001B[35m1.5276\u001B[0m     +  1.4822\n",
      "     16        1.5389       \u001B[32m0.2690\u001B[0m        \u001B[35m1.4970\u001B[0m     +  1.4912\n",
      "     17        \u001B[36m1.4951\u001B[0m       0.2690        \u001B[35m1.4415\u001B[0m     +  1.4930\n",
      "     18        1.5080       0.2621        1.5102        1.5034\n",
      "     19        1.5024       \u001B[32m0.2966\u001B[0m        1.4460        1.4864\n",
      "     20        \u001B[36m1.4027\u001B[0m       0.2759        \u001B[35m1.4215\u001B[0m     +  1.4816\n",
      "     21        1.4252       0.2828        \u001B[35m1.3734\u001B[0m     +  1.5136\n",
      "     22        1.4312       \u001B[32m0.3172\u001B[0m        \u001B[35m1.3228\u001B[0m     +  1.5158\n",
      "     23        \u001B[36m1.3789\u001B[0m       0.2966        1.3897        1.5631\n",
      "     24        \u001B[36m1.3511\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.2686\u001B[0m     +  1.5318\n",
      "     25        \u001B[36m1.2525\u001B[0m       \u001B[32m0.4138\u001B[0m        1.2946        1.5354\n",
      "     26        \u001B[36m1.2228\u001B[0m       0.3931        1.2913        1.5398\n",
      "     27        1.2285       0.3793        \u001B[35m1.2359\u001B[0m     +  1.5247\n",
      "     28        \u001B[36m1.1562\u001B[0m       0.4138        \u001B[35m1.1564\u001B[0m     +  1.5306\n",
      "     29        1.1571       \u001B[32m0.4483\u001B[0m        \u001B[35m1.1471\u001B[0m     +  1.5044\n",
      "     30        \u001B[36m1.1416\u001B[0m       0.4345        1.1710        1.5073\n",
      "     31        1.1452       0.4000        \u001B[35m1.1417\u001B[0m     +  1.4885\n",
      "     32        \u001B[36m1.0640\u001B[0m       \u001B[32m0.4759\u001B[0m        1.1743        1.4965\n",
      "     33        1.0723       \u001B[32m0.5034\u001B[0m        1.2297        1.4752\n",
      "     34        \u001B[36m0.9996\u001B[0m       0.4966        \u001B[35m1.1229\u001B[0m     +  1.4712\n",
      "     35        \u001B[36m0.9198\u001B[0m       \u001B[32m0.5241\u001B[0m        \u001B[35m1.1193\u001B[0m     +  1.4914\n",
      "     36        \u001B[36m0.9015\u001B[0m       0.4966        1.1322        1.5126\n",
      "     37        \u001B[36m0.8802\u001B[0m       0.4690        1.1708        1.4958\n",
      "     38        \u001B[36m0.8315\u001B[0m       0.5241        1.2101        1.4780\n",
      "     39        0.8350       \u001B[32m0.5517\u001B[0m        1.3602        1.4706\n",
      "     40        \u001B[36m0.8115\u001B[0m       0.5310        1.2107        1.4710\n",
      "     41        0.8119       0.5103        1.2352        1.4704\n",
      "     42        \u001B[36m0.7621\u001B[0m       0.5034        1.3133        1.4709\n",
      "     43        0.7775       \u001B[32m0.5655\u001B[0m        1.2126        1.4708\n",
      "     44        0.7843       0.5241        1.3166        1.4710\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.497 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8219\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.5906\u001B[0m     +  1.4936\n",
      "      2        \u001B[36m1.6125\u001B[0m       0.2431        \u001B[35m1.5895\u001B[0m     +  1.4763\n",
      "      3        \u001B[36m1.5913\u001B[0m       0.2292        \u001B[35m1.5846\u001B[0m     +  1.4925\n",
      "      4        1.5948       \u001B[32m0.2708\u001B[0m        \u001B[35m1.5789\u001B[0m     +  1.4784\n",
      "      5        \u001B[36m1.5830\u001B[0m       0.2431        \u001B[35m1.5703\u001B[0m     +  1.4795\n",
      "      6        \u001B[36m1.5736\u001B[0m       0.2431        \u001B[35m1.5632\u001B[0m     +  1.4993\n",
      "      7        \u001B[36m1.5666\u001B[0m       0.2431        \u001B[35m1.5551\u001B[0m     +  1.4971\n",
      "      8        \u001B[36m1.5586\u001B[0m       0.2431        \u001B[35m1.5404\u001B[0m     +  1.4765\n",
      "      9        \u001B[36m1.5310\u001B[0m       0.2500        \u001B[35m1.5230\u001B[0m     +  1.4831\n",
      "     10        \u001B[36m1.5147\u001B[0m       0.2500        \u001B[35m1.5040\u001B[0m     +  1.4815\n",
      "     11        \u001B[36m1.4855\u001B[0m       0.2500        \u001B[35m1.4798\u001B[0m     +  1.4788\n",
      "     12        \u001B[36m1.4578\u001B[0m       0.2500        \u001B[35m1.4513\u001B[0m     +  1.4957\n",
      "     13        \u001B[36m1.4404\u001B[0m       0.2500        1.4561        1.4913\n",
      "     14        \u001B[36m1.3862\u001B[0m       0.2500        \u001B[35m1.4196\u001B[0m     +  1.4657\n",
      "     15        \u001B[36m1.3659\u001B[0m       0.2639        \u001B[35m1.3916\u001B[0m     +  1.4839\n",
      "     16        \u001B[36m1.3205\u001B[0m       \u001B[32m0.3542\u001B[0m        \u001B[35m1.3614\u001B[0m     +  1.4848\n",
      "     17        \u001B[36m1.2705\u001B[0m       0.3472        \u001B[35m1.3336\u001B[0m     +  1.4864\n",
      "     18        \u001B[36m1.2098\u001B[0m       \u001B[32m0.3958\u001B[0m        \u001B[35m1.2847\u001B[0m     +  1.4839\n",
      "     19        \u001B[36m1.1921\u001B[0m       \u001B[32m0.5139\u001B[0m        \u001B[35m1.2611\u001B[0m     +  1.4864\n",
      "     20        \u001B[36m1.1051\u001B[0m       \u001B[32m0.5417\u001B[0m        \u001B[35m1.2092\u001B[0m     +  1.4903\n",
      "     21        \u001B[36m1.0493\u001B[0m       \u001B[32m0.5694\u001B[0m        \u001B[35m1.1870\u001B[0m     +  1.4835\n",
      "     22        \u001B[36m0.9753\u001B[0m       0.5139        1.1983        1.4951\n",
      "     23        \u001B[36m0.9366\u001B[0m       \u001B[32m0.5764\u001B[0m        \u001B[35m1.1672\u001B[0m     +  1.4762\n",
      "     24        \u001B[36m0.9086\u001B[0m       0.5694        1.1842        1.4881\n",
      "     25        \u001B[36m0.8587\u001B[0m       0.5347        1.1677        1.4544\n",
      "     26        \u001B[36m0.8071\u001B[0m       0.5208        1.1909        1.4546\n",
      "     27        \u001B[36m0.7893\u001B[0m       0.5208        1.1988        1.4549\n",
      "     28        \u001B[36m0.7842\u001B[0m       0.5556        \u001B[35m1.1626\u001B[0m     +  1.4553\n",
      "     29        \u001B[36m0.7395\u001B[0m       0.5278        1.2672        1.4710\n",
      "     30        \u001B[36m0.6853\u001B[0m       0.5625        1.2685        1.4528\n",
      "     31        \u001B[36m0.6554\u001B[0m       0.4931        1.3898        1.4556\n",
      "     32        \u001B[36m0.5958\u001B[0m       0.5625        1.3148        1.4542\n",
      "     33        \u001B[36m0.5790\u001B[0m       0.5625        1.3623        1.4530\n",
      "     34        \u001B[36m0.5660\u001B[0m       \u001B[32m0.5903\u001B[0m        1.4886        1.4541\n",
      "     35        \u001B[36m0.5489\u001B[0m       0.4931        1.4004        1.4546\n",
      "     36        \u001B[36m0.5250\u001B[0m       0.4444        1.6389        1.4571\n",
      "     37        \u001B[36m0.4899\u001B[0m       0.5556        1.2163        1.4569\n",
      "     38        \u001B[36m0.4750\u001B[0m       0.5833        \u001B[35m1.1405\u001B[0m     +  1.4577\n",
      "     39        \u001B[36m0.4346\u001B[0m       0.5764        1.4267        1.4852\n",
      "     40        0.4574       0.5625        1.4125        1.4617\n",
      "     41        \u001B[36m0.3806\u001B[0m       0.5694        1.3870        1.4601\n",
      "     42        \u001B[36m0.3607\u001B[0m       \u001B[32m0.6042\u001B[0m        1.5300        1.4593\n",
      "     43        0.4091       0.6042        1.5212        1.4566\n",
      "     44        0.3691       0.5625        1.3136        1.4582\n",
      "     45        \u001B[36m0.3322\u001B[0m       0.5694        1.4901        1.4568\n",
      "     46        0.4217       0.5833        1.5532        1.4576\n",
      "     47        0.3413       0.5764        1.5223        1.4606\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.439 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8286\u001B[0m       \u001B[32m0.2138\u001B[0m        \u001B[35m1.6139\u001B[0m     +  1.4775\n",
      "      2        \u001B[36m1.6944\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.6072\u001B[0m     +  1.5098\n",
      "      3        \u001B[36m1.6256\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5983\u001B[0m     +  1.5160\n",
      "      4        1.6315       0.2414        \u001B[35m1.5951\u001B[0m     +  1.5048\n",
      "      5        1.6317       0.2483        \u001B[35m1.5879\u001B[0m     +  1.5114\n",
      "      6        \u001B[36m1.6178\u001B[0m       \u001B[32m0.2621\u001B[0m        1.5898        1.5071\n",
      "      7        \u001B[36m1.5961\u001B[0m       \u001B[32m0.2690\u001B[0m        \u001B[35m1.5849\u001B[0m     +  1.4750\n",
      "      8        \u001B[36m1.5914\u001B[0m       0.2483        \u001B[35m1.5751\u001B[0m     +  1.5097\n",
      "      9        \u001B[36m1.5816\u001B[0m       \u001B[32m0.2897\u001B[0m        1.5800        1.5154\n",
      "     10        1.5822       \u001B[32m0.3310\u001B[0m        \u001B[35m1.5504\u001B[0m     +  1.4864\n",
      "     11        \u001B[36m1.5735\u001B[0m       0.2621        1.5506        1.5069\n",
      "     12        \u001B[36m1.5513\u001B[0m       \u001B[32m0.5034\u001B[0m        \u001B[35m1.5308\u001B[0m     +  1.4765\n",
      "     13        \u001B[36m1.5363\u001B[0m       0.4000        \u001B[35m1.5145\u001B[0m     +  1.5069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001B[36m1.5156\u001B[0m       0.4552        \u001B[35m1.4804\u001B[0m     +  1.5070\n",
      "     15        \u001B[36m1.5096\u001B[0m       0.3793        \u001B[35m1.4517\u001B[0m     +  1.5082\n",
      "     16        \u001B[36m1.4729\u001B[0m       0.3931        \u001B[35m1.4301\u001B[0m     +  1.5110\n",
      "     17        \u001B[36m1.4029\u001B[0m       0.4000        \u001B[35m1.3839\u001B[0m     +  1.5091\n",
      "     18        \u001B[36m1.3891\u001B[0m       0.4276        \u001B[35m1.3340\u001B[0m     +  1.5026\n",
      "     19        \u001B[36m1.2906\u001B[0m       0.4966        \u001B[35m1.3144\u001B[0m     +  1.5040\n",
      "     20        \u001B[36m1.2535\u001B[0m       0.4414        \u001B[35m1.2749\u001B[0m     +  1.5068\n",
      "     21        \u001B[36m1.1852\u001B[0m       0.5034        \u001B[35m1.2565\u001B[0m     +  1.5006\n",
      "     22        \u001B[36m1.1384\u001B[0m       \u001B[32m0.5241\u001B[0m        \u001B[35m1.2365\u001B[0m     +  1.5039\n",
      "     23        \u001B[36m1.0528\u001B[0m       \u001B[32m0.5655\u001B[0m        \u001B[35m1.1382\u001B[0m     +  1.5015\n",
      "     24        \u001B[36m0.9882\u001B[0m       \u001B[32m0.5931\u001B[0m        \u001B[35m1.1382\u001B[0m     +  1.5104\n",
      "     25        \u001B[36m0.9548\u001B[0m       \u001B[32m0.6207\u001B[0m        1.1603        1.5168\n",
      "     26        \u001B[36m0.8746\u001B[0m       0.5586        \u001B[35m1.1070\u001B[0m     +  1.4767\n",
      "     27        \u001B[36m0.8335\u001B[0m       \u001B[32m0.6414\u001B[0m        1.1363        1.5210\n",
      "     28        \u001B[36m0.7925\u001B[0m       0.6276        \u001B[35m1.0395\u001B[0m     +  1.5130\n",
      "     29        \u001B[36m0.7221\u001B[0m       \u001B[32m0.6897\u001B[0m        \u001B[35m1.0005\u001B[0m     +  1.4979\n",
      "     30        0.7278       0.6621        1.1987        1.5082\n",
      "     31        \u001B[36m0.6490\u001B[0m       0.6207        1.1329        1.4956\n",
      "     32        0.6642       0.6828        \u001B[35m0.9243\u001B[0m     +  1.4891\n",
      "     33        0.6521       0.6828        1.0761        1.5072\n",
      "     34        \u001B[36m0.6196\u001B[0m       0.6345        1.0340        1.4807\n",
      "     35        \u001B[36m0.5166\u001B[0m       0.6345        1.2058        1.4884\n",
      "     36        0.5338       0.6690        1.0400        1.4816\n",
      "     37        0.5305       0.6690        0.9335        1.4789\n",
      "     38        \u001B[36m0.5054\u001B[0m       \u001B[32m0.6966\u001B[0m        1.0467        1.4762\n",
      "     39        \u001B[36m0.4599\u001B[0m       \u001B[32m0.7034\u001B[0m        1.2353        1.4806\n",
      "     40        0.4748       0.6552        1.1753        1.4776\n",
      "     41        \u001B[36m0.4081\u001B[0m       \u001B[32m0.7172\u001B[0m        0.9512        1.4796\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.282 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8012\u001B[0m       \u001B[32m0.1586\u001B[0m        \u001B[35m1.6151\u001B[0m     +  1.4934\n",
      "      2        \u001B[36m1.6492\u001B[0m       \u001B[32m0.1655\u001B[0m        1.6184        1.5150\n",
      "      3        \u001B[36m1.6379\u001B[0m       \u001B[32m0.2966\u001B[0m        \u001B[35m1.6003\u001B[0m     +  1.4785\n",
      "      4        \u001B[36m1.6092\u001B[0m       0.2828        1.6015        1.5177\n",
      "      5        \u001B[36m1.6053\u001B[0m       \u001B[32m0.3172\u001B[0m        \u001B[35m1.5871\u001B[0m     +  1.4963\n",
      "      6        1.6113       \u001B[32m0.3448\u001B[0m        \u001B[35m1.5768\u001B[0m     +  1.5196\n",
      "      7        \u001B[36m1.5763\u001B[0m       0.3448        \u001B[35m1.5545\u001B[0m     +  1.5101\n",
      "      8        \u001B[36m1.5500\u001B[0m       0.2552        \u001B[35m1.5407\u001B[0m     +  1.5111\n",
      "      9        \u001B[36m1.5278\u001B[0m       \u001B[32m0.3517\u001B[0m        \u001B[35m1.5236\u001B[0m     +  1.5136\n",
      "     10        \u001B[36m1.5252\u001B[0m       0.3172        \u001B[35m1.5056\u001B[0m     +  1.5214\n",
      "     11        \u001B[36m1.4888\u001B[0m       \u001B[32m0.3655\u001B[0m        \u001B[35m1.4707\u001B[0m     +  1.5079\n",
      "     12        \u001B[36m1.4652\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.4523\u001B[0m     +  1.5199\n",
      "     13        \u001B[36m1.4374\u001B[0m       0.3862        \u001B[35m1.4090\u001B[0m     +  1.5068\n",
      "     14        \u001B[36m1.4283\u001B[0m       \u001B[32m0.4069\u001B[0m        1.4225        1.5152\n",
      "     15        \u001B[36m1.3917\u001B[0m       0.3931        \u001B[35m1.3997\u001B[0m     +  1.5157\n",
      "     16        \u001B[36m1.3685\u001B[0m       0.3931        \u001B[35m1.3920\u001B[0m     +  1.4995\n",
      "     17        \u001B[36m1.3056\u001B[0m       \u001B[32m0.4414\u001B[0m        \u001B[35m1.3808\u001B[0m     +  1.5168\n",
      "     18        \u001B[36m1.2588\u001B[0m       0.4276        \u001B[35m1.3416\u001B[0m     +  1.5068\n",
      "     19        \u001B[36m1.2095\u001B[0m       \u001B[32m0.4828\u001B[0m        \u001B[35m1.3054\u001B[0m     +  1.5176\n",
      "     20        \u001B[36m1.1498\u001B[0m       \u001B[32m0.5034\u001B[0m        \u001B[35m1.2639\u001B[0m     +  1.5057\n",
      "     21        \u001B[36m1.0689\u001B[0m       0.4828        \u001B[35m1.2459\u001B[0m     +  1.4956\n",
      "     22        \u001B[36m1.0628\u001B[0m       \u001B[32m0.5310\u001B[0m        \u001B[35m1.2342\u001B[0m     +  1.5031\n",
      "     23        \u001B[36m1.0371\u001B[0m       \u001B[32m0.5379\u001B[0m        \u001B[35m1.2132\u001B[0m     +  1.5148\n",
      "     24        \u001B[36m0.9662\u001B[0m       0.4966        1.2189        1.5135\n",
      "     25        \u001B[36m0.9261\u001B[0m       0.5103        1.2915        1.4981\n",
      "     26        \u001B[36m0.8838\u001B[0m       0.5379        1.3041        1.4764\n",
      "     27        \u001B[36m0.8304\u001B[0m       0.4966        1.4047        1.5013\n",
      "     28        \u001B[36m0.7911\u001B[0m       0.4483        1.7688        1.4789\n",
      "     29        \u001B[36m0.7512\u001B[0m       0.4690        1.9380        1.4752\n",
      "     30        \u001B[36m0.7377\u001B[0m       0.4483        1.9081        1.4764\n",
      "     31        \u001B[36m0.7035\u001B[0m       0.4897        1.4965        1.4944\n",
      "     32        \u001B[36m0.6277\u001B[0m       0.4897        2.3830        1.4987\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.529 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.3416\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.5596\u001B[0m     +  1.6874\n",
      "      2        \u001B[36m1.6708\u001B[0m       \u001B[32m0.2639\u001B[0m        \u001B[35m1.5518\u001B[0m     +  1.7242\n",
      "      3        \u001B[36m1.6221\u001B[0m       \u001B[32m0.3819\u001B[0m        \u001B[35m1.4994\u001B[0m     +  1.7249\n",
      "      4        \u001B[36m1.5567\u001B[0m       \u001B[32m0.4444\u001B[0m        1.5037        1.7205\n",
      "      5        \u001B[36m1.5222\u001B[0m       0.3889        \u001B[35m1.4880\u001B[0m     +  1.6970\n",
      "      6        \u001B[36m1.5108\u001B[0m       0.4167        \u001B[35m1.4159\u001B[0m     +  1.7111\n",
      "      7        \u001B[36m1.4719\u001B[0m       0.4444        \u001B[35m1.3850\u001B[0m     +  1.7282\n",
      "      8        \u001B[36m1.4135\u001B[0m       0.4306        \u001B[35m1.3371\u001B[0m     +  1.7228\n",
      "      9        \u001B[36m1.3801\u001B[0m       0.3819        1.4151        1.7144\n",
      "     10        \u001B[36m1.3657\u001B[0m       \u001B[32m0.5069\u001B[0m        \u001B[35m1.3096\u001B[0m     +  1.7067\n",
      "     11        \u001B[36m1.2437\u001B[0m       0.4167        \u001B[35m1.2931\u001B[0m     +  1.7218\n",
      "     12        \u001B[36m1.1196\u001B[0m       \u001B[32m0.5417\u001B[0m        \u001B[35m1.1719\u001B[0m     +  1.7183\n",
      "     13        \u001B[36m1.0104\u001B[0m       \u001B[32m0.5833\u001B[0m        \u001B[35m1.0765\u001B[0m     +  1.7035\n",
      "     14        \u001B[36m0.9247\u001B[0m       \u001B[32m0.5903\u001B[0m        1.0882        1.7155\n",
      "     15        \u001B[36m0.8429\u001B[0m       \u001B[32m0.6250\u001B[0m        \u001B[35m0.9938\u001B[0m     +  1.6783\n",
      "     16        \u001B[36m0.6929\u001B[0m       0.6181        1.0056        1.7101\n",
      "     17        \u001B[36m0.6880\u001B[0m       0.6250        \u001B[35m0.9654\u001B[0m     +  1.6773\n",
      "     18        \u001B[36m0.5612\u001B[0m       \u001B[32m0.6458\u001B[0m        1.0055        1.7045\n",
      "     19        \u001B[36m0.4505\u001B[0m       \u001B[32m0.6528\u001B[0m        1.0656        1.6768\n",
      "     20        \u001B[36m0.3933\u001B[0m       \u001B[32m0.7014\u001B[0m        0.9959        1.6765\n",
      "     21        \u001B[36m0.2804\u001B[0m       0.6944        1.0428        1.6834\n",
      "     22        \u001B[36m0.2672\u001B[0m       0.6597        1.0710        1.6811\n",
      "     23        \u001B[36m0.2255\u001B[0m       0.6875        1.1420        1.6816\n",
      "     24        \u001B[36m0.2089\u001B[0m       0.6458        1.4842        1.6809\n",
      "     25        \u001B[36m0.1822\u001B[0m       0.6250        1.5137        1.6969\n",
      "     26        \u001B[36m0.1685\u001B[0m       0.6111        1.5622        1.7017\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.434 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.8112\u001B[0m       \u001B[32m0.3517\u001B[0m        \u001B[35m1.5283\u001B[0m     +  1.7032\n",
      "      2        \u001B[36m1.6576\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.4552\u001B[0m     +  1.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001B[36m1.5050\u001B[0m       \u001B[32m0.4759\u001B[0m        \u001B[35m1.3621\u001B[0m     +  1.7151\n",
      "      4        \u001B[36m1.3622\u001B[0m       \u001B[32m0.4966\u001B[0m        \u001B[35m1.2770\u001B[0m     +  1.7137\n",
      "      5        \u001B[36m1.2191\u001B[0m       \u001B[32m0.5586\u001B[0m        \u001B[35m1.1027\u001B[0m     +  1.7229\n",
      "      6        \u001B[36m0.9496\u001B[0m       \u001B[32m0.6276\u001B[0m        \u001B[35m0.9572\u001B[0m     +  1.7107\n",
      "      7        \u001B[36m0.7431\u001B[0m       \u001B[32m0.6414\u001B[0m        \u001B[35m0.8478\u001B[0m     +  1.7121\n",
      "      8        \u001B[36m0.4802\u001B[0m       0.6138        0.9590        1.7210\n",
      "      9        0.4871       \u001B[32m0.6690\u001B[0m        \u001B[35m0.8256\u001B[0m     +  1.6851\n",
      "     10        \u001B[36m0.3314\u001B[0m       \u001B[32m0.6966\u001B[0m        \u001B[35m0.8064\u001B[0m     +  1.7200\n",
      "     11        \u001B[36m0.2197\u001B[0m       0.6828        \u001B[35m0.8033\u001B[0m     +  1.7177\n",
      "     12        \u001B[36m0.1631\u001B[0m       0.6483        1.0153        1.7179\n",
      "     13        \u001B[36m0.1257\u001B[0m       0.6690        1.0237        1.6813\n",
      "     14        \u001B[36m0.1157\u001B[0m       0.6483        1.2337        1.6805\n",
      "     15        \u001B[36m0.0868\u001B[0m       0.6759        1.0654        1.6811\n",
      "     16        \u001B[36m0.0565\u001B[0m       0.6966        1.1580        1.6843\n",
      "     17        0.0914       0.6897        1.0927        1.6808\n",
      "     18        0.0614       0.6759        1.2127        1.6828\n",
      "     19        \u001B[36m0.0343\u001B[0m       0.6759        1.1855        1.6819\n",
      "     20        0.0472       \u001B[32m0.7103\u001B[0m        1.0576        1.6810\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.288 total time=  52.2s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.7261\u001B[0m       \u001B[32m0.3241\u001B[0m        \u001B[35m1.5957\u001B[0m     +  1.6888\n",
      "      2        \u001B[36m1.6979\u001B[0m       \u001B[32m0.3724\u001B[0m        \u001B[35m1.5069\u001B[0m     +  1.7040\n",
      "      3        \u001B[36m1.5403\u001B[0m       0.3724        \u001B[35m1.4700\u001B[0m     +  1.7275\n",
      "      4        \u001B[36m1.4762\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.3169\u001B[0m     +  1.7173\n",
      "      5        \u001B[36m1.3302\u001B[0m       \u001B[32m0.4966\u001B[0m        \u001B[35m1.2127\u001B[0m     +  1.7209\n",
      "      6        \u001B[36m1.1275\u001B[0m       0.4690        \u001B[35m1.1932\u001B[0m     +  1.7152\n",
      "      7        \u001B[36m0.9585\u001B[0m       \u001B[32m0.6621\u001B[0m        \u001B[35m0.9594\u001B[0m     +  1.7137\n",
      "      8        \u001B[36m0.6815\u001B[0m       0.5931        1.0143        1.7074\n",
      "      9        \u001B[36m0.5240\u001B[0m       0.5517        1.1935        1.6819\n",
      "     10        \u001B[36m0.4177\u001B[0m       0.6138        1.0703        1.6904\n",
      "     11        \u001B[36m0.3646\u001B[0m       0.6138        1.1257        1.6863\n",
      "     12        \u001B[36m0.2954\u001B[0m       0.6552        0.9702        1.6877\n",
      "     13        \u001B[36m0.1978\u001B[0m       0.6414        1.0657        1.6853\n",
      "     14        \u001B[36m0.1665\u001B[0m       0.6552        1.1409        1.6876\n",
      "     15        \u001B[36m0.1048\u001B[0m       \u001B[32m0.6897\u001B[0m        0.9726        1.6856\n",
      "     16        \u001B[36m0.0988\u001B[0m       \u001B[32m0.6966\u001B[0m        1.0217        1.6854\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.931 total time=  41.0s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.4611\u001B[0m       \u001B[32m0.2778\u001B[0m        \u001B[35m1.5795\u001B[0m     +  1.7075\n",
      "      2        \u001B[36m1.6557\u001B[0m       0.2431        1.5865        1.7218\n",
      "      3        \u001B[36m1.6153\u001B[0m       0.2708        \u001B[35m1.5752\u001B[0m     +  1.6838\n",
      "      4        \u001B[36m1.6104\u001B[0m       \u001B[32m0.3194\u001B[0m        \u001B[35m1.5507\u001B[0m     +  1.7170\n",
      "      5        \u001B[36m1.5751\u001B[0m       0.3194        \u001B[35m1.5186\u001B[0m     +  1.7153\n",
      "      6        \u001B[36m1.5628\u001B[0m       \u001B[32m0.3472\u001B[0m        \u001B[35m1.4935\u001B[0m     +  1.7096\n",
      "      7        \u001B[36m1.4904\u001B[0m       \u001B[32m0.3681\u001B[0m        \u001B[35m1.4261\u001B[0m     +  1.7161\n",
      "      8        \u001B[36m1.4244\u001B[0m       \u001B[32m0.3889\u001B[0m        \u001B[35m1.3670\u001B[0m     +  1.7202\n",
      "      9        \u001B[36m1.3605\u001B[0m       \u001B[32m0.4653\u001B[0m        \u001B[35m1.2715\u001B[0m     +  1.7180\n",
      "     10        \u001B[36m1.2585\u001B[0m       \u001B[32m0.5625\u001B[0m        \u001B[35m1.2038\u001B[0m     +  1.7189\n",
      "     11        \u001B[36m1.1850\u001B[0m       0.4861        1.2229        1.7170\n",
      "     12        \u001B[36m0.9871\u001B[0m       0.5208        \u001B[35m1.1440\u001B[0m     +  1.6799\n",
      "     13        \u001B[36m0.9071\u001B[0m       \u001B[32m0.5903\u001B[0m        \u001B[35m1.0347\u001B[0m     +  1.7124\n",
      "     14        \u001B[36m0.7089\u001B[0m       \u001B[32m0.6597\u001B[0m        \u001B[35m1.0045\u001B[0m     +  1.7084\n",
      "     15        \u001B[36m0.5873\u001B[0m       0.6250        \u001B[35m0.9576\u001B[0m     +  1.7103\n",
      "     16        \u001B[36m0.5168\u001B[0m       \u001B[32m0.6667\u001B[0m        0.9903        1.7106\n",
      "     17        \u001B[36m0.3856\u001B[0m       \u001B[32m0.6875\u001B[0m        \u001B[35m0.9371\u001B[0m     +  1.6815\n",
      "     18        \u001B[36m0.3181\u001B[0m       0.6875        0.9775        1.7087\n",
      "     19        \u001B[36m0.1736\u001B[0m       0.6736        1.2822        1.6809\n",
      "     20        \u001B[36m0.1463\u001B[0m       \u001B[32m0.7222\u001B[0m        1.2612        1.6780\n",
      "     21        \u001B[36m0.1332\u001B[0m       0.6806        1.3023        1.6780\n",
      "     22        \u001B[36m0.1001\u001B[0m       0.6528        1.5755        1.6790\n",
      "     23        0.1308       0.6875        1.4465        1.6838\n",
      "     24        0.1508       0.6806        1.5838        1.6785\n",
      "     25        0.1721       0.6250        1.9291        1.6778\n",
      "     26        0.1061       0.6458        1.7438        1.6786\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-2.003 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0265\u001B[0m       \u001B[32m0.3793\u001B[0m        \u001B[35m1.5725\u001B[0m     +  1.6954\n",
      "      2        \u001B[36m1.6002\u001B[0m       0.3241        \u001B[35m1.5062\u001B[0m     +  1.7237\n",
      "      3        \u001B[36m1.5743\u001B[0m       0.3172        \u001B[35m1.4905\u001B[0m     +  1.7181\n",
      "      4        \u001B[36m1.5636\u001B[0m       0.3793        1.5008        1.7226\n",
      "      5        \u001B[36m1.5026\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.4044\u001B[0m     +  1.6942\n",
      "      6        \u001B[36m1.4204\u001B[0m       \u001B[32m0.4483\u001B[0m        \u001B[35m1.3284\u001B[0m     +  1.7126\n",
      "      7        \u001B[36m1.3073\u001B[0m       \u001B[32m0.4759\u001B[0m        \u001B[35m1.2264\u001B[0m     +  1.7170\n",
      "      8        \u001B[36m1.2684\u001B[0m       0.4621        \u001B[35m1.1802\u001B[0m     +  1.7149\n",
      "      9        \u001B[36m1.0907\u001B[0m       \u001B[32m0.5379\u001B[0m        \u001B[35m1.0548\u001B[0m     +  1.7220\n",
      "     10        \u001B[36m1.0106\u001B[0m       \u001B[32m0.5448\u001B[0m        \u001B[35m1.0402\u001B[0m     +  1.7202\n",
      "     11        \u001B[36m0.8204\u001B[0m       \u001B[32m0.6276\u001B[0m        \u001B[35m0.9472\u001B[0m     +  1.7164\n",
      "     12        \u001B[36m0.7419\u001B[0m       \u001B[32m0.6759\u001B[0m        \u001B[35m0.8639\u001B[0m     +  1.7177\n",
      "     13        \u001B[36m0.5368\u001B[0m       0.6621        0.9144        1.7295\n",
      "     14        \u001B[36m0.4946\u001B[0m       0.5862        1.3489        1.6879\n",
      "     15        \u001B[36m0.3465\u001B[0m       \u001B[32m0.7172\u001B[0m        0.8897        1.6878\n",
      "     16        \u001B[36m0.2508\u001B[0m       0.6414        1.2349        1.6856\n",
      "     17        \u001B[36m0.1882\u001B[0m       0.6897        1.1291        1.6880\n",
      "     18        \u001B[36m0.1704\u001B[0m       0.6207        1.6935        1.6875\n",
      "     19        \u001B[36m0.1324\u001B[0m       0.6690        1.2723        1.6855\n",
      "     20        0.1358       \u001B[32m0.7241\u001B[0m        1.2627        1.6885\n",
      "     21        \u001B[36m0.0711\u001B[0m       0.6966        1.4806        1.6880\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.433 total time=  55.7s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.2240\u001B[0m       \u001B[32m0.2759\u001B[0m        \u001B[35m1.6141\u001B[0m     +  1.6995\n",
      "      2        \u001B[36m1.7066\u001B[0m       \u001B[32m0.3379\u001B[0m        \u001B[35m1.5916\u001B[0m     +  1.7118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001B[36m1.6149\u001B[0m       \u001B[32m0.3448\u001B[0m        \u001B[35m1.5471\u001B[0m     +  1.7128\n",
      "      4        \u001B[36m1.6120\u001B[0m       0.3379        \u001B[35m1.5340\u001B[0m     +  1.7260\n",
      "      5        \u001B[36m1.5620\u001B[0m       0.3448        \u001B[35m1.4708\u001B[0m     +  1.7165\n",
      "      6        \u001B[36m1.5573\u001B[0m       0.3448        1.4820        1.7156\n",
      "      7        \u001B[36m1.4869\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.4637\u001B[0m     +  1.6889\n",
      "      8        \u001B[36m1.4383\u001B[0m       0.3931        \u001B[35m1.3191\u001B[0m     +  1.7176\n",
      "      9        \u001B[36m1.3574\u001B[0m       0.3793        1.4109        1.7189\n",
      "     10        \u001B[36m1.2949\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.2186\u001B[0m     +  1.6869\n",
      "     11        \u001B[36m1.1789\u001B[0m       0.3517        1.3132        1.7319\n",
      "     12        \u001B[36m1.0622\u001B[0m       \u001B[32m0.4690\u001B[0m        \u001B[35m1.1271\u001B[0m     +  1.7205\n",
      "     13        \u001B[36m0.8935\u001B[0m       \u001B[32m0.4759\u001B[0m        1.1989        1.7327\n",
      "     14        \u001B[36m0.8090\u001B[0m       \u001B[32m0.5586\u001B[0m        1.1631        1.7048\n",
      "     15        \u001B[36m0.6878\u001B[0m       \u001B[32m0.5793\u001B[0m        \u001B[35m1.0185\u001B[0m     +  1.6887\n",
      "     16        \u001B[36m0.5395\u001B[0m       \u001B[32m0.5862\u001B[0m        1.1128        1.7240\n",
      "     17        \u001B[36m0.4217\u001B[0m       \u001B[32m0.6414\u001B[0m        1.0888        1.6959\n",
      "     18        \u001B[36m0.3497\u001B[0m       \u001B[32m0.6621\u001B[0m        \u001B[35m0.9775\u001B[0m     +  1.6882\n",
      "     19        \u001B[36m0.1966\u001B[0m       \u001B[32m0.6828\u001B[0m        1.0486        1.7135\n",
      "     20        0.2010       0.6483        1.3455        1.6888\n",
      "     21        \u001B[36m0.1462\u001B[0m       \u001B[32m0.7172\u001B[0m        1.1601        1.6876\n",
      "     22        \u001B[36m0.1150\u001B[0m       0.6759        1.3616        1.6875\n",
      "     23        \u001B[36m0.0595\u001B[0m       0.6897        1.7920        1.6890\n",
      "     24        0.0602       0.6759        1.6148        1.6890\n",
      "     25        \u001B[36m0.0532\u001B[0m       0.6897        1.6267        1.6902\n",
      "     26        \u001B[36m0.0513\u001B[0m       0.6759        1.8035        1.6882\n",
      "     27        \u001B[36m0.0434\u001B[0m       0.6483        2.3584        1.6872\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.473 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9376\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.5955\u001B[0m     +  1.7011\n",
      "      2        \u001B[36m1.6225\u001B[0m       0.2431        \u001B[35m1.5943\u001B[0m     +  1.7274\n",
      "      3        1.6237       0.2431        \u001B[35m1.5929\u001B[0m     +  1.7158\n",
      "      4        \u001B[36m1.6089\u001B[0m       \u001B[32m0.2500\u001B[0m        \u001B[35m1.5911\u001B[0m     +  1.7282\n",
      "      5        \u001B[36m1.5962\u001B[0m       0.2014        \u001B[35m1.5845\u001B[0m     +  1.7226\n",
      "      6        1.6049       0.2292        \u001B[35m1.5835\u001B[0m     +  1.7141\n",
      "      7        \u001B[36m1.5839\u001B[0m       0.2431        1.5852        1.7279\n",
      "      8        \u001B[36m1.5781\u001B[0m       0.2500        \u001B[35m1.5698\u001B[0m     +  1.6890\n",
      "      9        1.5835       \u001B[32m0.2569\u001B[0m        1.5739        1.7198\n",
      "     10        \u001B[36m1.5550\u001B[0m       0.2431        \u001B[35m1.5479\u001B[0m     +  1.6996\n",
      "     11        \u001B[36m1.5083\u001B[0m       0.2569        \u001B[35m1.5219\u001B[0m     +  1.7141\n",
      "     12        \u001B[36m1.4626\u001B[0m       0.2500        \u001B[35m1.4930\u001B[0m     +  1.7160\n",
      "     13        \u001B[36m1.4504\u001B[0m       \u001B[32m0.2708\u001B[0m        \u001B[35m1.4598\u001B[0m     +  1.7227\n",
      "     14        \u001B[36m1.3798\u001B[0m       \u001B[32m0.2917\u001B[0m        \u001B[35m1.4334\u001B[0m     +  1.7227\n",
      "     15        \u001B[36m1.3351\u001B[0m       0.2917        \u001B[35m1.4227\u001B[0m     +  1.7108\n",
      "     16        \u001B[36m1.2576\u001B[0m       \u001B[32m0.3681\u001B[0m        \u001B[35m1.4109\u001B[0m     +  1.7039\n",
      "     17        \u001B[36m1.2225\u001B[0m       \u001B[32m0.4375\u001B[0m        \u001B[35m1.3694\u001B[0m     +  1.7228\n",
      "     18        \u001B[36m1.1187\u001B[0m       0.3472        1.4118        1.7203\n",
      "     19        \u001B[36m1.0637\u001B[0m       \u001B[32m0.5000\u001B[0m        \u001B[35m1.2909\u001B[0m     +  1.6848\n",
      "     20        \u001B[36m0.9658\u001B[0m       \u001B[32m0.5694\u001B[0m        \u001B[35m1.2303\u001B[0m     +  1.7210\n",
      "     21        \u001B[36m0.8563\u001B[0m       \u001B[32m0.5903\u001B[0m        1.4300        1.7090\n",
      "     22        \u001B[36m0.7505\u001B[0m       \u001B[32m0.6181\u001B[0m        1.4368        1.6810\n",
      "     23        \u001B[36m0.6678\u001B[0m       0.6111        1.2538        1.6810\n",
      "     24        \u001B[36m0.5999\u001B[0m       0.6111        1.4222        1.6828\n",
      "     25        0.6108       0.5556        1.3626        1.6842\n",
      "     26        \u001B[36m0.5756\u001B[0m       0.5764        1.4208        1.6798\n",
      "     27        \u001B[36m0.5530\u001B[0m       0.5486        1.4806        1.6796\n",
      "     28        \u001B[36m0.5095\u001B[0m       0.5139        1.3380        1.6809\n",
      "     29        \u001B[36m0.4900\u001B[0m       0.4514        2.4949        1.6791\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.246 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9606\u001B[0m       \u001B[32m0.1655\u001B[0m        \u001B[35m1.5998\u001B[0m     +  1.6942\n",
      "      2        \u001B[36m1.6619\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.5997\u001B[0m     +  1.7222\n",
      "      3        \u001B[36m1.6350\u001B[0m       \u001B[32m0.2552\u001B[0m        \u001B[35m1.5862\u001B[0m     +  1.7278\n",
      "      4        1.6388       \u001B[32m0.2897\u001B[0m        1.5889        1.7132\n",
      "      5        \u001B[36m1.6218\u001B[0m       \u001B[32m0.2966\u001B[0m        1.5863        1.6871\n",
      "      6        \u001B[36m1.6144\u001B[0m       0.2552        1.6030        1.6908\n",
      "      7        \u001B[36m1.6013\u001B[0m       0.2690        \u001B[35m1.5711\u001B[0m     +  1.6870\n",
      "      8        \u001B[36m1.5740\u001B[0m       0.2276        \u001B[35m1.5565\u001B[0m     +  1.7301\n",
      "      9        \u001B[36m1.5546\u001B[0m       0.2621        1.5609        1.7171\n",
      "     10        1.5683       0.2690        \u001B[35m1.5304\u001B[0m     +  1.6888\n",
      "     11        \u001B[36m1.5268\u001B[0m       \u001B[32m0.4345\u001B[0m        \u001B[35m1.5217\u001B[0m     +  1.7323\n",
      "     12        \u001B[36m1.4714\u001B[0m       0.3034        \u001B[35m1.4560\u001B[0m     +  1.7324\n",
      "     13        \u001B[36m1.4401\u001B[0m       0.4138        \u001B[35m1.4017\u001B[0m     +  1.7177\n",
      "     14        \u001B[36m1.3750\u001B[0m       \u001B[32m0.5172\u001B[0m        \u001B[35m1.3412\u001B[0m     +  1.7282\n",
      "     15        \u001B[36m1.2884\u001B[0m       \u001B[32m0.5448\u001B[0m        1.3506        1.7154\n",
      "     16        \u001B[36m1.1636\u001B[0m       0.5310        \u001B[35m1.2081\u001B[0m     +  1.6887\n",
      "     17        \u001B[36m1.0834\u001B[0m       \u001B[32m0.5724\u001B[0m        \u001B[35m1.1365\u001B[0m     +  1.7250\n",
      "     18        \u001B[36m1.0311\u001B[0m       0.5655        \u001B[35m1.0909\u001B[0m     +  1.7197\n",
      "     19        \u001B[36m0.9150\u001B[0m       0.4759        \u001B[35m1.0850\u001B[0m     +  1.7277\n",
      "     20        \u001B[36m0.8919\u001B[0m       0.5241        \u001B[35m1.0344\u001B[0m     +  1.7353\n",
      "     21        \u001B[36m0.8451\u001B[0m       \u001B[32m0.6000\u001B[0m        \u001B[35m0.9817\u001B[0m     +  1.7210\n",
      "     22        \u001B[36m0.7406\u001B[0m       \u001B[32m0.6690\u001B[0m        \u001B[35m0.9540\u001B[0m     +  1.7274\n",
      "     23        \u001B[36m0.6032\u001B[0m       0.5724        1.0096        1.7263\n",
      "     24        \u001B[36m0.5456\u001B[0m       0.5931        1.0311        1.6921\n",
      "     25        \u001B[36m0.4937\u001B[0m       0.6345        \u001B[35m0.8995\u001B[0m     +  1.6913\n",
      "     26        \u001B[36m0.4143\u001B[0m       0.6138        1.0285        1.7304\n",
      "     27        \u001B[36m0.3404\u001B[0m       0.6483        0.9906        1.7087\n",
      "     28        \u001B[36m0.3109\u001B[0m       \u001B[32m0.6828\u001B[0m        1.0858        1.7016\n",
      "     29        0.3241       0.5034        1.6219        1.6930\n",
      "     30        \u001B[36m0.2390\u001B[0m       \u001B[32m0.6897\u001B[0m        0.9433        1.6984\n",
      "     31        \u001B[36m0.1929\u001B[0m       \u001B[32m0.7172\u001B[0m        0.9946        1.6961\n",
      "     32        \u001B[36m0.1911\u001B[0m       0.6207        1.3101        1.6913\n",
      "     33        \u001B[36m0.1539\u001B[0m       \u001B[32m0.7310\u001B[0m        1.1032        1.6946\n",
      "     34        \u001B[36m0.1206\u001B[0m       0.6552        1.3790        1.6928\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.513 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8324\u001B[0m       \u001B[32m0.2069\u001B[0m        \u001B[35m1.5971\u001B[0m     +  1.7017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001B[36m1.6412\u001B[0m       0.2000        1.6063        1.7147\n",
      "      3        \u001B[36m1.6329\u001B[0m       \u001B[32m0.2138\u001B[0m        1.6009        1.6940\n",
      "      4        \u001B[36m1.6146\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.5866\u001B[0m     +  1.6945\n",
      "      5        1.6172       0.2276        1.6003        1.7176\n",
      "      6        \u001B[36m1.5887\u001B[0m       0.2276        1.5926        1.6911\n",
      "      7        \u001B[36m1.5882\u001B[0m       \u001B[32m0.3172\u001B[0m        \u001B[35m1.5809\u001B[0m     +  1.6894\n",
      "      8        \u001B[36m1.5754\u001B[0m       \u001B[32m0.3517\u001B[0m        \u001B[35m1.5760\u001B[0m     +  1.7087\n",
      "      9        \u001B[36m1.5288\u001B[0m       0.3448        \u001B[35m1.5440\u001B[0m     +  1.7202\n",
      "     10        \u001B[36m1.4875\u001B[0m       0.3448        \u001B[35m1.5250\u001B[0m     +  1.7280\n",
      "     11        \u001B[36m1.4542\u001B[0m       0.3241        1.5280        1.7164\n",
      "     12        \u001B[36m1.4301\u001B[0m       0.3448        \u001B[35m1.4258\u001B[0m     +  1.6904\n",
      "     13        \u001B[36m1.4029\u001B[0m       0.3172        \u001B[35m1.4240\u001B[0m     +  1.7172\n",
      "     14        \u001B[36m1.3759\u001B[0m       0.3448        \u001B[35m1.3822\u001B[0m     +  1.7183\n",
      "     15        \u001B[36m1.3527\u001B[0m       0.3103        \u001B[35m1.3556\u001B[0m     +  1.7232\n",
      "     16        \u001B[36m1.3381\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.3523\u001B[0m     +  1.7172\n",
      "     17        \u001B[36m1.2651\u001B[0m       \u001B[32m0.4138\u001B[0m        \u001B[35m1.3507\u001B[0m     +  1.7168\n",
      "     18        \u001B[36m1.2607\u001B[0m       0.3931        \u001B[35m1.3258\u001B[0m     +  1.7150\n",
      "     19        \u001B[36m1.2456\u001B[0m       \u001B[32m0.4276\u001B[0m        1.3609        1.7089\n",
      "     20        \u001B[36m1.2241\u001B[0m       0.4069        1.3749        1.6892\n",
      "     21        1.2659       0.3586        1.3622        1.6890\n",
      "     22        1.3028       0.4069        \u001B[35m1.3196\u001B[0m     +  1.6921\n",
      "     23        \u001B[36m1.2087\u001B[0m       \u001B[32m0.4345\u001B[0m        \u001B[35m1.2832\u001B[0m     +  1.7268\n",
      "     24        \u001B[36m1.1920\u001B[0m       0.4276        1.2925        1.7196\n",
      "     25        1.1966       0.4138        1.3062        1.6897\n",
      "     26        \u001B[36m1.1815\u001B[0m       0.4138        1.3073        1.6885\n",
      "     27        \u001B[36m1.1432\u001B[0m       \u001B[32m0.4552\u001B[0m        1.3229        1.6908\n",
      "     28        \u001B[36m1.1424\u001B[0m       0.4276        1.3230        1.6920\n",
      "     29        \u001B[36m1.0799\u001B[0m       0.4483        1.3249        1.6895\n",
      "     30        \u001B[36m1.0717\u001B[0m       0.3586        1.3178        1.6904\n",
      "     31        1.0830       \u001B[32m0.4966\u001B[0m        \u001B[35m1.2780\u001B[0m     +  1.6904\n",
      "     32        \u001B[36m1.0373\u001B[0m       0.4621        \u001B[35m1.2325\u001B[0m     +  1.7253\n",
      "     33        \u001B[36m0.9642\u001B[0m       \u001B[32m0.5172\u001B[0m        1.2468        1.7241\n",
      "     34        \u001B[36m0.9284\u001B[0m       \u001B[32m0.5241\u001B[0m        1.2684        1.6910\n",
      "     35        0.9487       0.5172        \u001B[35m1.2240\u001B[0m     +  1.6926\n",
      "     36        \u001B[36m0.9079\u001B[0m       0.4966        1.3677        1.7181\n",
      "     37        0.9518       \u001B[32m0.5379\u001B[0m        1.3125        1.6925\n",
      "     38        \u001B[36m0.8895\u001B[0m       \u001B[32m0.5448\u001B[0m        1.3312        1.6916\n",
      "     39        \u001B[36m0.8398\u001B[0m       0.4828        1.4298        1.6932\n",
      "     40        \u001B[36m0.8380\u001B[0m       0.5241        1.2298        1.6945\n",
      "     41        \u001B[36m0.8363\u001B[0m       \u001B[32m0.5862\u001B[0m        \u001B[35m1.1679\u001B[0m     +  1.6903\n",
      "     42        \u001B[36m0.7926\u001B[0m       0.4828        1.5464        1.7217\n",
      "     43        0.8190       0.5448        1.1844        1.6943\n",
      "     44        \u001B[36m0.7320\u001B[0m       0.5310        1.4452        1.6914\n",
      "     45        0.7336       0.4897        2.4069        1.6905\n",
      "     46        \u001B[36m0.7260\u001B[0m       0.5241        1.3956        1.6940\n",
      "     47        \u001B[36m0.6813\u001B[0m       0.5448        1.2891        1.6994\n",
      "     48        \u001B[36m0.6679\u001B[0m       0.5724        1.4768        1.6987\n",
      "     49        \u001B[36m0.6245\u001B[0m       \u001B[32m0.6069\u001B[0m        1.2230        1.6953\n",
      "     50        \u001B[36m0.5938\u001B[0m       0.5172        1.8755        1.6941\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.650 total time= 2.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m3.2725\u001B[0m       \u001B[32m0.2708\u001B[0m        \u001B[35m1.5163\u001B[0m     +  2.0989\n",
      "      2        \u001B[36m1.6605\u001B[0m       \u001B[32m0.4028\u001B[0m        \u001B[35m1.4408\u001B[0m     +  2.1446\n",
      "      3        \u001B[36m1.5708\u001B[0m       0.3472        \u001B[35m1.4222\u001B[0m     +  2.1399\n",
      "      4        \u001B[36m1.4752\u001B[0m       \u001B[32m0.4306\u001B[0m        \u001B[35m1.3507\u001B[0m     +  2.1340\n",
      "      5        \u001B[36m1.3165\u001B[0m       \u001B[32m0.5347\u001B[0m        \u001B[35m1.2043\u001B[0m     +  2.1348\n",
      "      6        \u001B[36m1.1101\u001B[0m       \u001B[32m0.5972\u001B[0m        \u001B[35m1.0709\u001B[0m     +  2.1494\n",
      "      7        \u001B[36m0.9188\u001B[0m       \u001B[32m0.6181\u001B[0m        \u001B[35m1.0170\u001B[0m     +  2.1294\n",
      "      8        \u001B[36m0.7459\u001B[0m       \u001B[32m0.6458\u001B[0m        \u001B[35m0.9288\u001B[0m     +  2.1397\n",
      "      9        \u001B[36m0.5068\u001B[0m       0.6389        1.0107        2.1413\n",
      "     10        \u001B[36m0.4278\u001B[0m       \u001B[32m0.6597\u001B[0m        0.9581        2.1130\n",
      "     11        \u001B[36m0.2897\u001B[0m       \u001B[32m0.6806\u001B[0m        1.0886        2.1098\n",
      "     12        \u001B[36m0.2409\u001B[0m       0.6458        1.2556        2.1101\n",
      "     13        \u001B[36m0.2399\u001B[0m       0.6458        1.0874        2.1094\n",
      "     14        \u001B[36m0.2042\u001B[0m       \u001B[32m0.6944\u001B[0m        1.0530        2.1095\n",
      "     15        \u001B[36m0.1799\u001B[0m       0.6042        1.5690        2.1102\n",
      "     16        \u001B[36m0.1254\u001B[0m       0.6111        1.8584        2.1101\n",
      "     17        \u001B[36m0.1177\u001B[0m       0.6875        1.2846        2.1126\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.349 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m3.5719\u001B[0m       \u001B[32m0.3586\u001B[0m        \u001B[35m1.4861\u001B[0m     +  2.1187\n",
      "      2        \u001B[36m1.6369\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.4158\u001B[0m     +  2.1464\n",
      "      3        \u001B[36m1.4377\u001B[0m       \u001B[32m0.5172\u001B[0m        \u001B[35m1.2808\u001B[0m     +  2.1530\n",
      "      4        \u001B[36m1.1840\u001B[0m       \u001B[32m0.5517\u001B[0m        \u001B[35m1.1047\u001B[0m     +  2.1437\n",
      "      5        \u001B[36m0.9591\u001B[0m       \u001B[32m0.6621\u001B[0m        \u001B[35m0.9483\u001B[0m     +  2.1421\n",
      "      6        \u001B[36m0.6722\u001B[0m       0.6276        \u001B[35m0.9082\u001B[0m     +  2.1476\n",
      "      7        \u001B[36m0.4564\u001B[0m       0.6552        \u001B[35m0.8904\u001B[0m     +  2.1520\n",
      "      8        \u001B[36m0.2329\u001B[0m       \u001B[32m0.6828\u001B[0m        \u001B[35m0.8811\u001B[0m     +  2.1486\n",
      "      9        \u001B[36m0.1552\u001B[0m       0.6690        1.1114        2.1372\n",
      "     10        \u001B[36m0.1005\u001B[0m       \u001B[32m0.6897\u001B[0m        1.1454        2.1136\n",
      "     11        \u001B[36m0.0985\u001B[0m       \u001B[32m0.6966\u001B[0m        1.0634        2.1139\n",
      "     12        \u001B[36m0.0660\u001B[0m       0.6621        1.1190        2.1157\n",
      "     13        \u001B[36m0.0462\u001B[0m       0.6828        1.4442        2.1229\n",
      "     14        0.0486       \u001B[32m0.7103\u001B[0m        1.0654        2.1199\n",
      "     15        \u001B[36m0.0433\u001B[0m       0.6621        1.3778        2.1180\n",
      "     16        \u001B[36m0.0353\u001B[0m       0.7103        1.2668        2.1182\n",
      "     17        0.0857       0.6966        1.2282        2.1198\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-2.078 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m3.2982\u001B[0m       \u001B[32m0.3793\u001B[0m        \u001B[35m1.5616\u001B[0m     +  2.1216\n",
      "      2        \u001B[36m1.6612\u001B[0m       0.3655        \u001B[35m1.5140\u001B[0m     +  2.1568\n",
      "      3        \u001B[36m1.5622\u001B[0m       0.3724        \u001B[35m1.4256\u001B[0m     +  2.1397\n",
      "      4        \u001B[36m1.4495\u001B[0m       \u001B[32m0.4138\u001B[0m        \u001B[35m1.3537\u001B[0m     +  2.1520\n",
      "      5        \u001B[36m1.3296\u001B[0m       0.3793        1.3581        2.1406\n",
      "      6        \u001B[36m1.2628\u001B[0m       \u001B[32m0.4483\u001B[0m        \u001B[35m1.1941\u001B[0m     +  2.1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001B[36m1.1556\u001B[0m       0.3862        1.3960        2.1542\n",
      "      8        \u001B[36m1.0126\u001B[0m       \u001B[32m0.5379\u001B[0m        \u001B[35m1.1871\u001B[0m     +  2.1162\n",
      "      9        \u001B[36m0.7026\u001B[0m       \u001B[32m0.6414\u001B[0m        \u001B[35m1.0628\u001B[0m     +  2.1492\n",
      "     10        \u001B[36m0.5837\u001B[0m       0.6207        \u001B[35m1.0584\u001B[0m     +  2.1492\n",
      "     11        \u001B[36m0.3282\u001B[0m       \u001B[32m0.6621\u001B[0m        \u001B[35m0.9932\u001B[0m     +  2.1418\n",
      "     12        \u001B[36m0.2462\u001B[0m       0.6138        1.2176        2.1417\n",
      "     13        \u001B[36m0.1602\u001B[0m       0.6414        1.1818        2.1163\n",
      "     14        \u001B[36m0.1249\u001B[0m       \u001B[32m0.6690\u001B[0m        1.2112        2.1163\n",
      "     15        \u001B[36m0.0701\u001B[0m       \u001B[32m0.6966\u001B[0m        1.1501        2.1186\n",
      "     16        \u001B[36m0.0560\u001B[0m       0.6966        1.2109        2.1174\n",
      "     17        \u001B[36m0.0490\u001B[0m       0.6621        1.3839        2.1174\n",
      "     18        \u001B[36m0.0218\u001B[0m       0.6621        1.5865        2.1174\n",
      "     19        0.0220       0.6414        1.5820        2.1174\n",
      "     20        \u001B[36m0.0077\u001B[0m       0.6345        1.5545        2.1206\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=1;, score=-2.276 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.6713\u001B[0m       \u001B[32m0.2153\u001B[0m        \u001B[35m1.6030\u001B[0m     +  2.1146\n",
      "      2        \u001B[36m1.6742\u001B[0m       \u001B[32m0.2986\u001B[0m        \u001B[35m1.5578\u001B[0m     +  2.1533\n",
      "      3        \u001B[36m1.6516\u001B[0m       \u001B[32m0.3056\u001B[0m        \u001B[35m1.5245\u001B[0m     +  2.1357\n",
      "      4        \u001B[36m1.5462\u001B[0m       \u001B[32m0.3472\u001B[0m        \u001B[35m1.4856\u001B[0m     +  2.1438\n",
      "      5        1.5651       \u001B[32m0.3681\u001B[0m        \u001B[35m1.4400\u001B[0m     +  2.1430\n",
      "      6        \u001B[36m1.4725\u001B[0m       \u001B[32m0.3819\u001B[0m        \u001B[35m1.3631\u001B[0m     +  2.1317\n",
      "      7        \u001B[36m1.3580\u001B[0m       \u001B[32m0.4236\u001B[0m        \u001B[35m1.2726\u001B[0m     +  2.1563\n",
      "      8        \u001B[36m1.2144\u001B[0m       \u001B[32m0.4861\u001B[0m        \u001B[35m1.1969\u001B[0m     +  2.1544\n",
      "      9        \u001B[36m1.0322\u001B[0m       \u001B[32m0.5278\u001B[0m        \u001B[35m1.1355\u001B[0m     +  2.1483\n",
      "     10        \u001B[36m0.7963\u001B[0m       \u001B[32m0.6042\u001B[0m        \u001B[35m1.0168\u001B[0m     +  2.1517\n",
      "     11        \u001B[36m0.6029\u001B[0m       \u001B[32m0.6319\u001B[0m        \u001B[35m0.9692\u001B[0m     +  2.1456\n",
      "     12        \u001B[36m0.4603\u001B[0m       \u001B[32m0.6389\u001B[0m        1.0023        2.1568\n",
      "     13        \u001B[36m0.3294\u001B[0m       \u001B[32m0.6944\u001B[0m        1.0158        2.1490\n",
      "     14        \u001B[36m0.2796\u001B[0m       0.6458        1.1079        2.1164\n",
      "     15        \u001B[36m0.1788\u001B[0m       0.6806        1.1147        2.1148\n",
      "     16        \u001B[36m0.1519\u001B[0m       0.6806        1.2308        2.1128\n",
      "     17        0.1779       0.6250        1.1932        2.1138\n",
      "     18        \u001B[36m0.1437\u001B[0m       0.6806        1.3763        2.1131\n",
      "     19        \u001B[36m0.1329\u001B[0m       0.6250        1.6066        2.1138\n",
      "     20        0.1898       0.6319        1.7057        2.1141\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.453 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.5503\u001B[0m       \u001B[32m0.3448\u001B[0m        \u001B[35m1.5753\u001B[0m     +  2.1260\n",
      "      2        \u001B[36m1.6399\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.5641\u001B[0m     +  2.1469\n",
      "      3        \u001B[36m1.6171\u001B[0m       0.3862        \u001B[35m1.5122\u001B[0m     +  2.1570\n",
      "      4        \u001B[36m1.5791\u001B[0m       0.3241        \u001B[35m1.4957\u001B[0m     +  2.1512\n",
      "      5        \u001B[36m1.5514\u001B[0m       0.3586        \u001B[35m1.3936\u001B[0m     +  2.1581\n",
      "      6        \u001B[36m1.4588\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.3710\u001B[0m     +  2.1541\n",
      "      7        \u001B[36m1.4038\u001B[0m       \u001B[32m0.4276\u001B[0m        \u001B[35m1.2723\u001B[0m     +  2.1600\n",
      "      8        \u001B[36m1.3000\u001B[0m       \u001B[32m0.4483\u001B[0m        \u001B[35m1.2101\u001B[0m     +  2.1701\n",
      "      9        \u001B[36m1.1528\u001B[0m       \u001B[32m0.5103\u001B[0m        \u001B[35m1.1320\u001B[0m     +  2.1581\n",
      "     10        \u001B[36m0.9999\u001B[0m       \u001B[32m0.5862\u001B[0m        \u001B[35m1.0044\u001B[0m     +  2.1691\n",
      "     11        \u001B[36m0.8785\u001B[0m       \u001B[32m0.6414\u001B[0m        \u001B[35m0.9470\u001B[0m     +  2.1715\n",
      "     12        \u001B[36m0.7181\u001B[0m       \u001B[32m0.6483\u001B[0m        \u001B[35m0.8815\u001B[0m     +  2.1615\n",
      "     13        \u001B[36m0.5972\u001B[0m       \u001B[32m0.6897\u001B[0m        \u001B[35m0.8078\u001B[0m     +  2.1511\n",
      "     14        \u001B[36m0.4888\u001B[0m       0.6138        1.0038        2.1612\n",
      "     15        \u001B[36m0.4366\u001B[0m       0.6207        1.1185        2.1336\n",
      "     16        \u001B[36m0.3894\u001B[0m       0.6897        0.9114        2.1234\n",
      "     17        \u001B[36m0.2469\u001B[0m       \u001B[32m0.6966\u001B[0m        0.9488        2.1208\n",
      "     18        \u001B[36m0.1710\u001B[0m       0.6276        1.3004        2.1234\n",
      "     19        0.1778       0.5586        1.8055        2.1512\n",
      "     20        \u001B[36m0.1292\u001B[0m       \u001B[32m0.7103\u001B[0m        1.1062        2.1270\n",
      "     21        \u001B[36m0.0911\u001B[0m       0.6828        1.5966        2.1220\n",
      "     22        \u001B[36m0.0677\u001B[0m       0.6759        1.3306        2.1229\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.375 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.3595\u001B[0m       \u001B[32m0.2690\u001B[0m        \u001B[35m1.5869\u001B[0m     +  2.1313\n",
      "      2        \u001B[36m1.6886\u001B[0m       \u001B[32m0.2897\u001B[0m        \u001B[35m1.5854\u001B[0m     +  2.1708\n",
      "      3        \u001B[36m1.6357\u001B[0m       \u001B[32m0.3655\u001B[0m        \u001B[35m1.5233\u001B[0m     +  2.1604\n",
      "      4        \u001B[36m1.5905\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.4892\u001B[0m     +  2.1437\n",
      "      5        \u001B[36m1.5347\u001B[0m       0.3724        \u001B[35m1.4550\u001B[0m     +  2.1669\n",
      "      6        \u001B[36m1.4470\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.3953\u001B[0m     +  2.1657\n",
      "      7        \u001B[36m1.3456\u001B[0m       \u001B[32m0.4000\u001B[0m        1.4121        2.1551\n",
      "      8        \u001B[36m1.2976\u001B[0m       \u001B[32m0.4828\u001B[0m        \u001B[35m1.1694\u001B[0m     +  2.1205\n",
      "      9        \u001B[36m1.2046\u001B[0m       \u001B[32m0.5241\u001B[0m        1.2090        2.1622\n",
      "     10        \u001B[36m1.0134\u001B[0m       0.4621        1.2188        2.1269\n",
      "     11        \u001B[36m0.9004\u001B[0m       0.4966        1.3580        2.1211\n",
      "     12        \u001B[36m0.7697\u001B[0m       0.5172        1.2082        2.1213\n",
      "     13        \u001B[36m0.7003\u001B[0m       \u001B[32m0.5517\u001B[0m        1.2919        2.1242\n",
      "     14        \u001B[36m0.5314\u001B[0m       \u001B[32m0.5655\u001B[0m        1.3094        2.1213\n",
      "     15        \u001B[36m0.3162\u001B[0m       \u001B[32m0.6000\u001B[0m        \u001B[35m1.1421\u001B[0m     +  2.1265\n",
      "     16        \u001B[36m0.2210\u001B[0m       0.5793        1.4110        2.1498\n",
      "     17        \u001B[36m0.1451\u001B[0m       \u001B[32m0.7241\u001B[0m        \u001B[35m1.0956\u001B[0m     +  2.1198\n",
      "     18        \u001B[36m0.1039\u001B[0m       0.7103        1.2262        2.1542\n",
      "     19        \u001B[36m0.0932\u001B[0m       0.6966        1.1800        2.1273\n",
      "     20        \u001B[36m0.0623\u001B[0m       \u001B[32m0.7310\u001B[0m        1.2872        2.1257\n",
      "     21        \u001B[36m0.0564\u001B[0m       \u001B[32m0.7448\u001B[0m        1.3790        2.1377\n",
      "     22        \u001B[36m0.0266\u001B[0m       0.6759        1.6212        2.1221\n",
      "     23        \u001B[36m0.0219\u001B[0m       0.6759        1.6283        2.1385\n",
      "     24        0.0323       0.6828        1.6381        2.1266\n",
      "     25        0.0489       0.7103        1.4906        2.1215\n",
      "     26        0.0455       0.6345        1.8778        2.1231\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.284 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.2140\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.6097\u001B[0m     +  2.1167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001B[36m1.7012\u001B[0m       0.2222        \u001B[35m1.5944\u001B[0m     +  2.1544\n",
      "      3        \u001B[36m1.6502\u001B[0m       0.2431        \u001B[35m1.5823\u001B[0m     +  2.1491\n",
      "      4        \u001B[36m1.6123\u001B[0m       \u001B[32m0.2986\u001B[0m        \u001B[35m1.5686\u001B[0m     +  2.1422\n",
      "      5        \u001B[36m1.5970\u001B[0m       0.2569        1.5746        2.1448\n",
      "      6        \u001B[36m1.5766\u001B[0m       0.2500        \u001B[35m1.5459\u001B[0m     +  2.1224\n",
      "      7        \u001B[36m1.5386\u001B[0m       0.2847        \u001B[35m1.5123\u001B[0m     +  2.1495\n",
      "      8        \u001B[36m1.5050\u001B[0m       \u001B[32m0.3403\u001B[0m        \u001B[35m1.4731\u001B[0m     +  2.1505\n",
      "      9        \u001B[36m1.4541\u001B[0m       \u001B[32m0.3750\u001B[0m        \u001B[35m1.4272\u001B[0m     +  2.1451\n",
      "     10        \u001B[36m1.3702\u001B[0m       \u001B[32m0.4167\u001B[0m        \u001B[35m1.3375\u001B[0m     +  2.1510\n",
      "     11        \u001B[36m1.2433\u001B[0m       0.3611        \u001B[35m1.2785\u001B[0m     +  2.1474\n",
      "     12        \u001B[36m1.1500\u001B[0m       \u001B[32m0.4583\u001B[0m        \u001B[35m1.2429\u001B[0m     +  2.1433\n",
      "     13        \u001B[36m1.0502\u001B[0m       \u001B[32m0.5069\u001B[0m        1.2559        2.1582\n",
      "     14        \u001B[36m0.9219\u001B[0m       0.4722        1.3886        2.1334\n",
      "     15        \u001B[36m0.8429\u001B[0m       \u001B[32m0.5972\u001B[0m        \u001B[35m1.1604\u001B[0m     +  2.1139\n",
      "     16        \u001B[36m0.7511\u001B[0m       \u001B[32m0.6111\u001B[0m        \u001B[35m1.1122\u001B[0m     +  2.1491\n",
      "     17        \u001B[36m0.5320\u001B[0m       \u001B[32m0.6458\u001B[0m        \u001B[35m1.0493\u001B[0m     +  2.1518\n",
      "     18        \u001B[36m0.4886\u001B[0m       0.6250        1.2614        2.1474\n",
      "     19        \u001B[36m0.3964\u001B[0m       0.5903        1.5144        2.1148\n",
      "     20        0.5764       0.6389        1.1927        2.1146\n",
      "     21        \u001B[36m0.3078\u001B[0m       \u001B[32m0.6528\u001B[0m        1.1196        2.1164\n",
      "     22        \u001B[36m0.1721\u001B[0m       \u001B[32m0.6736\u001B[0m        1.3457        2.1162\n",
      "     23        0.1970       \u001B[32m0.7083\u001B[0m        1.1273        2.1183\n",
      "     24        \u001B[36m0.1231\u001B[0m       0.7083        1.2211        2.1200\n",
      "     25        \u001B[36m0.0757\u001B[0m       \u001B[32m0.7153\u001B[0m        1.3168        2.1163\n",
      "     26        \u001B[36m0.0506\u001B[0m       0.7014        1.3536        2.1172\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.110 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0029\u001B[0m       \u001B[32m0.1586\u001B[0m        \u001B[35m1.5953\u001B[0m     +  2.1219\n",
      "      2        \u001B[36m1.6766\u001B[0m       \u001B[32m0.2345\u001B[0m        1.6041        2.1495\n",
      "      3        \u001B[36m1.6231\u001B[0m       \u001B[32m0.2414\u001B[0m        \u001B[35m1.5829\u001B[0m     +  2.1245\n",
      "      4        \u001B[36m1.6038\u001B[0m       \u001B[32m0.4276\u001B[0m        \u001B[35m1.5603\u001B[0m     +  2.1480\n",
      "      5        \u001B[36m1.6019\u001B[0m       0.3517        1.5715        2.1560\n",
      "      6        \u001B[36m1.5929\u001B[0m       0.3379        \u001B[35m1.5239\u001B[0m     +  2.1249\n",
      "      7        \u001B[36m1.5703\u001B[0m       0.3793        \u001B[35m1.5213\u001B[0m     +  2.1508\n",
      "      8        \u001B[36m1.5077\u001B[0m       0.4000        \u001B[35m1.4233\u001B[0m     +  2.1563\n",
      "      9        \u001B[36m1.4799\u001B[0m       0.3655        \u001B[35m1.3453\u001B[0m     +  2.1601\n",
      "     10        \u001B[36m1.3337\u001B[0m       0.3793        \u001B[35m1.2702\u001B[0m     +  2.1530\n",
      "     11        \u001B[36m1.2563\u001B[0m       \u001B[32m0.4552\u001B[0m        \u001B[35m1.1949\u001B[0m     +  2.1570\n",
      "     12        \u001B[36m1.1811\u001B[0m       0.4483        1.2121        2.1680\n",
      "     13        \u001B[36m1.0377\u001B[0m       \u001B[32m0.5172\u001B[0m        \u001B[35m1.0361\u001B[0m     +  2.1257\n",
      "     14        \u001B[36m0.9435\u001B[0m       \u001B[32m0.5517\u001B[0m        \u001B[35m1.0354\u001B[0m     +  2.1541\n",
      "     15        \u001B[36m0.8468\u001B[0m       0.5310        \u001B[35m1.0172\u001B[0m     +  2.1538\n",
      "     16        \u001B[36m0.7126\u001B[0m       \u001B[32m0.5793\u001B[0m        \u001B[35m0.9613\u001B[0m     +  2.1559\n",
      "     17        0.7377       0.5793        1.2037        2.1651\n",
      "     18        \u001B[36m0.6845\u001B[0m       \u001B[32m0.6483\u001B[0m        \u001B[35m0.8767\u001B[0m     +  2.1351\n",
      "     19        \u001B[36m0.4511\u001B[0m       \u001B[32m0.6552\u001B[0m        1.0752        2.1724\n",
      "     20        \u001B[36m0.4117\u001B[0m       0.6069        1.1299        2.1392\n",
      "     21        \u001B[36m0.3367\u001B[0m       0.6483        1.4061        2.1261\n",
      "     22        \u001B[36m0.2612\u001B[0m       0.6414        1.2731        2.1294\n",
      "     23        \u001B[36m0.2426\u001B[0m       \u001B[32m0.6621\u001B[0m        1.1126        2.1285\n",
      "     24        \u001B[36m0.1123\u001B[0m       \u001B[32m0.6759\u001B[0m        1.2998        2.1257\n",
      "     25        \u001B[36m0.0659\u001B[0m       \u001B[32m0.6897\u001B[0m        1.3245        2.1287\n",
      "     26        \u001B[36m0.0483\u001B[0m       \u001B[32m0.6966\u001B[0m        1.2927        2.1306\n",
      "     27        0.1037       \u001B[32m0.7034\u001B[0m        1.4268        2.1292\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.538 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9228\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5918\u001B[0m     +  2.1410\n",
      "      2        \u001B[36m1.6559\u001B[0m       \u001B[32m0.2759\u001B[0m        \u001B[35m1.5857\u001B[0m     +  2.1542\n",
      "      3        \u001B[36m1.6251\u001B[0m       \u001B[32m0.3517\u001B[0m        1.5867        2.1525\n",
      "      4        1.6409       0.3448        \u001B[35m1.5805\u001B[0m     +  2.1263\n",
      "      5        \u001B[36m1.6116\u001B[0m       0.2966        \u001B[35m1.5637\u001B[0m     +  2.1588\n",
      "      6        \u001B[36m1.5848\u001B[0m       0.3448        \u001B[35m1.5477\u001B[0m     +  2.1546\n",
      "      7        \u001B[36m1.5644\u001B[0m       0.3517        \u001B[35m1.5285\u001B[0m     +  2.1677\n",
      "      8        \u001B[36m1.5205\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.4497\u001B[0m     +  2.1577\n",
      "      9        \u001B[36m1.4720\u001B[0m       0.3655        \u001B[35m1.3494\u001B[0m     +  2.1653\n",
      "     10        \u001B[36m1.3344\u001B[0m       0.3379        1.3770        2.1708\n",
      "     11        \u001B[36m1.2540\u001B[0m       0.3724        \u001B[35m1.2786\u001B[0m     +  2.1395\n",
      "     12        \u001B[36m1.1872\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.2526\u001B[0m     +  2.1645\n",
      "     13        \u001B[36m1.0400\u001B[0m       \u001B[32m0.4483\u001B[0m        \u001B[35m1.2120\u001B[0m     +  2.1667\n",
      "     14        \u001B[36m0.9609\u001B[0m       \u001B[32m0.4828\u001B[0m        1.2517        2.1696\n",
      "     15        0.9735       0.4483        1.3441        2.1383\n",
      "     16        1.0568       \u001B[32m0.5517\u001B[0m        1.2400        2.1281\n",
      "     17        \u001B[36m0.8105\u001B[0m       \u001B[32m0.5931\u001B[0m        \u001B[35m1.1613\u001B[0m     +  2.1275\n",
      "     18        \u001B[36m0.5996\u001B[0m       0.5172        1.4624        2.1590\n",
      "     19        \u001B[36m0.4958\u001B[0m       \u001B[32m0.6207\u001B[0m        \u001B[35m1.1540\u001B[0m     +  2.1273\n",
      "     20        0.5064       0.5862        1.3933        2.1611\n",
      "     21        \u001B[36m0.4042\u001B[0m       \u001B[32m0.6621\u001B[0m        \u001B[35m1.0658\u001B[0m     +  2.1259\n",
      "     22        \u001B[36m0.2794\u001B[0m       \u001B[32m0.7241\u001B[0m        1.1459        2.1548\n",
      "     23        \u001B[36m0.1931\u001B[0m       0.6897        1.1322        2.1251\n",
      "     24        \u001B[36m0.1169\u001B[0m       0.7034        1.4898        2.1300\n",
      "     25        \u001B[36m0.0806\u001B[0m       0.6483        1.5466        2.1375\n",
      "     26        0.0845       0.7034        1.5763        2.1318\n",
      "     27        0.0886       0.6759        1.7927        2.1324\n",
      "     28        \u001B[36m0.0441\u001B[0m       0.6690        2.0171        2.1288\n",
      "     29        0.1005       0.6621        1.9659        2.1309\n",
      "     30        0.1367       0.7241        1.7628        2.1299\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=True, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.743 total time= 1.8min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.1568\u001B[0m       \u001B[32m0.2569\u001B[0m        \u001B[35m1.5748\u001B[0m     +  2.9501\n",
      "      2        \u001B[36m1.5951\u001B[0m       \u001B[32m0.3125\u001B[0m        1.5801        2.9232\n",
      "      3        1.5952       \u001B[32m0.3194\u001B[0m        \u001B[35m1.5121\u001B[0m     +  2.9084\n",
      "      4        \u001B[36m1.5034\u001B[0m       \u001B[32m0.4028\u001B[0m        \u001B[35m1.3751\u001B[0m     +  2.9229\n",
      "      5        \u001B[36m1.3906\u001B[0m       \u001B[32m0.5486\u001B[0m        \u001B[35m1.1474\u001B[0m     +  2.9159\n",
      "      6        \u001B[36m1.1825\u001B[0m       0.5486        \u001B[35m1.0940\u001B[0m     +  2.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001B[36m1.1590\u001B[0m       \u001B[32m0.5972\u001B[0m        1.1794        2.9275\n",
      "      8        \u001B[36m0.9921\u001B[0m       0.5903        \u001B[35m0.9572\u001B[0m     +  2.9128\n",
      "      9        \u001B[36m0.8228\u001B[0m       \u001B[32m0.7222\u001B[0m        \u001B[35m0.7708\u001B[0m     +  2.9266\n",
      "     10        \u001B[36m0.7698\u001B[0m       0.6111        1.3118        2.9257\n",
      "     11        \u001B[36m0.6143\u001B[0m       0.7014        0.9711        2.9112\n",
      "     12        \u001B[36m0.6028\u001B[0m       0.6875        1.0010        2.9085\n",
      "     13        \u001B[36m0.5591\u001B[0m       \u001B[32m0.7361\u001B[0m        1.1486        2.9044\n",
      "     14        \u001B[36m0.5071\u001B[0m       0.6875        2.4543        2.9174\n",
      "     15        \u001B[36m0.3991\u001B[0m       0.6875        0.8819        2.9089\n",
      "     16        \u001B[36m0.3399\u001B[0m       0.6875        1.3096        2.9117\n",
      "     17        \u001B[36m0.2950\u001B[0m       0.6736        1.4194        2.9124\n",
      "     18        \u001B[36m0.2739\u001B[0m       0.6944        1.2881        2.9115\n",
      "     19        \u001B[36m0.2258\u001B[0m       \u001B[32m0.7847\u001B[0m        \u001B[35m0.7627\u001B[0m     +  2.9125\n",
      "     20        \u001B[36m0.1939\u001B[0m       0.7778        0.9981        2.9251\n",
      "     21        \u001B[36m0.1306\u001B[0m       \u001B[32m0.8194\u001B[0m        0.7660        2.9116\n",
      "     22        0.1374       0.7847        \u001B[35m0.7615\u001B[0m     +  2.9123\n",
      "     23        \u001B[36m0.1150\u001B[0m       0.7639        0.9179        2.9371\n",
      "     24        \u001B[36m0.1028\u001B[0m       0.8125        1.0038        2.9104\n",
      "     25        \u001B[36m0.0904\u001B[0m       0.7986        1.0340        2.9114\n",
      "     26        0.1230       0.6875        1.3499        2.9130\n",
      "     27        0.1066       0.7569        1.2434        2.9332\n",
      "     28        \u001B[36m0.0681\u001B[0m       0.7639        1.1235        2.9196\n",
      "     29        0.0919       0.7847        1.0774        2.9157\n",
      "     30        \u001B[36m0.0600\u001B[0m       0.7917        1.2609        2.9170\n",
      "     31        0.0628       0.7431        1.3453        2.9194\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.324 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.1475\u001B[0m       \u001B[32m0.3379\u001B[0m        \u001B[35m1.5779\u001B[0m     +  2.9480\n",
      "      2        \u001B[36m1.5817\u001B[0m       \u001B[32m0.4345\u001B[0m        \u001B[35m1.4741\u001B[0m     +  2.9351\n",
      "      3        \u001B[36m1.4203\u001B[0m       \u001B[32m0.4828\u001B[0m        \u001B[35m1.1605\u001B[0m     +  2.9376\n",
      "      4        \u001B[36m1.1835\u001B[0m       \u001B[32m0.6207\u001B[0m        \u001B[35m1.0812\u001B[0m     +  2.9303\n",
      "      5        \u001B[36m0.9671\u001B[0m       \u001B[32m0.7103\u001B[0m        \u001B[35m0.7849\u001B[0m     +  2.9434\n",
      "      6        \u001B[36m0.7935\u001B[0m       0.7034        \u001B[35m0.7759\u001B[0m     +  2.9396\n",
      "      7        \u001B[36m0.6777\u001B[0m       \u001B[32m0.7724\u001B[0m        \u001B[35m0.7349\u001B[0m     +  2.9445\n",
      "      8        \u001B[36m0.5565\u001B[0m       0.7655        \u001B[35m0.7027\u001B[0m     +  2.9382\n",
      "      9        \u001B[36m0.4674\u001B[0m       \u001B[32m0.8000\u001B[0m        \u001B[35m0.5894\u001B[0m     +  2.9359\n",
      "     10        \u001B[36m0.4314\u001B[0m       0.8000        \u001B[35m0.5743\u001B[0m     +  2.9385\n",
      "     11        \u001B[36m0.3832\u001B[0m       0.7862        0.6512        2.9334\n",
      "     12        \u001B[36m0.2298\u001B[0m       0.7793        0.9072        2.9194\n",
      "     13        0.2601       0.7586        0.7424        2.9150\n",
      "     14        \u001B[36m0.2003\u001B[0m       \u001B[32m0.8069\u001B[0m        0.8552        2.9133\n",
      "     15        0.2018       0.7586        1.0626        2.9127\n",
      "     16        \u001B[36m0.1348\u001B[0m       \u001B[32m0.8138\u001B[0m        0.5984        2.9254\n",
      "     17        \u001B[36m0.1118\u001B[0m       \u001B[32m0.8207\u001B[0m        0.8459        2.9135\n",
      "     18        0.1236       0.8000        0.8490        2.9172\n",
      "     19        0.1651       0.7862        0.8923        2.9186\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-0.671 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0953\u001B[0m       \u001B[32m0.3172\u001B[0m        \u001B[35m1.5470\u001B[0m     +  2.9378\n",
      "      2        \u001B[36m1.6053\u001B[0m       \u001B[32m0.4690\u001B[0m        \u001B[35m1.3704\u001B[0m     +  2.9369\n",
      "      3        \u001B[36m1.4846\u001B[0m       \u001B[32m0.5103\u001B[0m        \u001B[35m1.2483\u001B[0m     +  2.9356\n",
      "      4        \u001B[36m1.2813\u001B[0m       0.3310        2.4627        2.9416\n",
      "      5        \u001B[36m1.1331\u001B[0m       \u001B[32m0.6069\u001B[0m        \u001B[35m0.9871\u001B[0m     +  2.9175\n",
      "      6        \u001B[36m1.0449\u001B[0m       \u001B[32m0.6897\u001B[0m        \u001B[35m0.7912\u001B[0m     +  2.9349\n",
      "      7        \u001B[36m0.8167\u001B[0m       \u001B[32m0.7655\u001B[0m        \u001B[35m0.7168\u001B[0m     +  2.9416\n",
      "      8        \u001B[36m0.6666\u001B[0m       0.6276        1.1436        2.9397\n",
      "      9        \u001B[36m0.4874\u001B[0m       0.6345        1.2435        2.9226\n",
      "     10        \u001B[36m0.4730\u001B[0m       0.7448        1.2813        2.9238\n",
      "     11        \u001B[36m0.3196\u001B[0m       0.6759        3.1201        2.9238\n",
      "     12        \u001B[36m0.2955\u001B[0m       \u001B[32m0.8207\u001B[0m        1.0189        2.9265\n",
      "     13        \u001B[36m0.2203\u001B[0m       0.8069        0.9709        2.9240\n",
      "     14        0.2363       \u001B[32m0.8483\u001B[0m        0.7949        2.9236\n",
      "     15        \u001B[36m0.1822\u001B[0m       0.7862        1.4789        2.9238\n",
      "     16        0.2193       0.7586        1.1617        2.9154\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=1;, score=-1.275 total time=  56.1s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9001\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.5931\u001B[0m     +  2.9302\n",
      "      2        \u001B[36m1.6368\u001B[0m       \u001B[32m0.2639\u001B[0m        1.6009        2.9407\n",
      "      3        \u001B[36m1.6025\u001B[0m       0.2431        \u001B[35m1.5793\u001B[0m     +  2.9204\n",
      "      4        \u001B[36m1.5872\u001B[0m       0.2431        \u001B[35m1.5338\u001B[0m     +  2.9346\n",
      "      5        \u001B[36m1.5429\u001B[0m       \u001B[32m0.4236\u001B[0m        \u001B[35m1.4573\u001B[0m     +  2.9416\n",
      "      6        \u001B[36m1.5355\u001B[0m       0.2569        1.8158        2.9413\n",
      "      7        \u001B[36m1.4553\u001B[0m       0.4236        \u001B[35m1.3351\u001B[0m     +  2.9331\n",
      "      8        \u001B[36m1.4144\u001B[0m       0.3611        1.3576        2.9308\n",
      "      9        \u001B[36m1.3701\u001B[0m       0.3194        1.9344        2.9167\n",
      "     10        \u001B[36m1.3465\u001B[0m       \u001B[32m0.5000\u001B[0m        \u001B[35m1.3203\u001B[0m     +  2.9174\n",
      "     11        \u001B[36m1.2882\u001B[0m       \u001B[32m0.5833\u001B[0m        \u001B[35m1.2705\u001B[0m     +  2.9387\n",
      "     12        \u001B[36m1.2311\u001B[0m       0.5208        1.4039        2.9324\n",
      "     13        \u001B[36m1.1450\u001B[0m       0.5833        \u001B[35m1.2639\u001B[0m     +  2.9074\n",
      "     14        \u001B[36m1.1366\u001B[0m       0.5556        \u001B[35m1.2134\u001B[0m     +  2.9329\n",
      "     15        \u001B[36m1.0259\u001B[0m       0.4722        1.6557        2.9437\n",
      "     16        \u001B[36m0.9911\u001B[0m       \u001B[32m0.5972\u001B[0m        1.3316        2.9388\n",
      "     17        \u001B[36m0.9679\u001B[0m       \u001B[32m0.6181\u001B[0m        \u001B[35m1.0770\u001B[0m     +  2.9325\n",
      "     18        \u001B[36m0.8714\u001B[0m       0.6111        1.1161        2.9297\n",
      "     19        \u001B[36m0.8335\u001B[0m       \u001B[32m0.6528\u001B[0m        \u001B[35m1.0719\u001B[0m     +  2.9074\n",
      "     20        \u001B[36m0.7842\u001B[0m       0.6319        1.3824        2.9470\n",
      "     21        \u001B[36m0.7838\u001B[0m       \u001B[32m0.6597\u001B[0m        1.1086        2.9284\n",
      "     22        \u001B[36m0.7529\u001B[0m       0.6458        1.5146        2.9159\n",
      "     23        0.7529       0.6458        1.1719        2.9153\n",
      "     24        \u001B[36m0.6659\u001B[0m       0.6597        1.1887        2.9173\n",
      "     25        \u001B[36m0.6606\u001B[0m       0.6458        1.3077        2.9114\n",
      "     26        \u001B[36m0.6467\u001B[0m       0.6458        1.3732        2.9116\n",
      "     27        \u001B[36m0.6414\u001B[0m       0.6528        1.4929        2.9231\n",
      "     28        \u001B[36m0.6168\u001B[0m       \u001B[32m0.6944\u001B[0m        1.3932        2.9200\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.462 total time= 1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.7449\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.5730\u001B[0m     +  2.9388\n",
      "      2        \u001B[36m1.5910\u001B[0m       \u001B[32m0.2483\u001B[0m        1.5739        2.9460\n",
      "      3        \u001B[36m1.5676\u001B[0m       \u001B[32m0.2690\u001B[0m        \u001B[35m1.5375\u001B[0m     +  2.9293\n",
      "      4        \u001B[36m1.5661\u001B[0m       \u001B[32m0.2897\u001B[0m        \u001B[35m1.5178\u001B[0m     +  2.9453\n",
      "      5        \u001B[36m1.5350\u001B[0m       \u001B[32m0.3379\u001B[0m        1.6355        2.9445\n",
      "      6        \u001B[36m1.4898\u001B[0m       \u001B[32m0.3448\u001B[0m        \u001B[35m1.3898\u001B[0m     +  2.9303\n",
      "      7        \u001B[36m1.4504\u001B[0m       0.3172        1.3899        2.9419\n",
      "      8        \u001B[36m1.3240\u001B[0m       0.3448        \u001B[35m1.3178\u001B[0m     +  2.9201\n",
      "      9        \u001B[36m1.2884\u001B[0m       0.3241        1.4209        2.9446\n",
      "     10        \u001B[36m1.2689\u001B[0m       \u001B[32m0.4276\u001B[0m        1.3434        2.9278\n",
      "     11        \u001B[36m1.1888\u001B[0m       0.4069        \u001B[35m1.2356\u001B[0m     +  2.9233\n",
      "     12        \u001B[36m1.1043\u001B[0m       0.3862        \u001B[35m1.2077\u001B[0m     +  2.9370\n",
      "     13        \u001B[36m1.0313\u001B[0m       \u001B[32m0.4414\u001B[0m        1.2121        2.9467\n",
      "     14        \u001B[36m0.9917\u001B[0m       0.4276        2.0050        2.9226\n",
      "     15        \u001B[36m0.9528\u001B[0m       \u001B[32m0.6414\u001B[0m        \u001B[35m1.0741\u001B[0m     +  2.9163\n",
      "     16        \u001B[36m0.8554\u001B[0m       0.5655        1.2398        2.9411\n",
      "     17        \u001B[36m0.8350\u001B[0m       \u001B[32m0.6483\u001B[0m        1.2255        2.9189\n",
      "     18        \u001B[36m0.8170\u001B[0m       0.6069        1.3921        2.9178\n",
      "     19        \u001B[36m0.8079\u001B[0m       0.6483        1.5913        2.9250\n",
      "     20        \u001B[36m0.7974\u001B[0m       0.6069        1.4392        2.9226\n",
      "     21        \u001B[36m0.7643\u001B[0m       0.5793        1.4051        2.9248\n",
      "     22        \u001B[36m0.6991\u001B[0m       0.6276        1.1748        2.9233\n",
      "     23        \u001B[36m0.6767\u001B[0m       0.6483        1.1092        2.9221\n",
      "     24        \u001B[36m0.6027\u001B[0m       \u001B[32m0.6897\u001B[0m        1.1076        2.9350\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.193 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8312\u001B[0m       \u001B[32m0.2897\u001B[0m        \u001B[35m1.5993\u001B[0m     +  2.9495\n",
      "      2        \u001B[36m1.6357\u001B[0m       0.2483        1.6067        2.9536\n",
      "      3        \u001B[36m1.6314\u001B[0m       \u001B[32m0.2966\u001B[0m        \u001B[35m1.5889\u001B[0m     +  2.9251\n",
      "      4        \u001B[36m1.6184\u001B[0m       0.2414        1.5954        2.9431\n",
      "      5        \u001B[36m1.5881\u001B[0m       0.2897        \u001B[35m1.5582\u001B[0m     +  2.9330\n",
      "      6        \u001B[36m1.5841\u001B[0m       0.2483        \u001B[35m1.5117\u001B[0m     +  2.9498\n",
      "      7        \u001B[36m1.5152\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.3930\u001B[0m     +  2.9415\n",
      "      8        \u001B[36m1.4821\u001B[0m       0.3172        1.4179        2.9438\n",
      "      9        \u001B[36m1.4231\u001B[0m       \u001B[32m0.4207\u001B[0m        \u001B[35m1.2293\u001B[0m     +  2.9354\n",
      "     10        \u001B[36m1.3321\u001B[0m       \u001B[32m0.4621\u001B[0m        \u001B[35m1.1771\u001B[0m     +  2.9480\n",
      "     11        \u001B[36m1.2530\u001B[0m       \u001B[32m0.4759\u001B[0m        1.1848        2.9485\n",
      "     12        1.2900       0.3862        1.3565        2.9306\n",
      "     13        \u001B[36m1.1576\u001B[0m       \u001B[32m0.5172\u001B[0m        \u001B[35m0.9822\u001B[0m     +  2.9256\n",
      "     14        \u001B[36m1.1087\u001B[0m       0.5172        1.1593        2.9406\n",
      "     15        \u001B[36m0.9980\u001B[0m       0.5103        1.1290        2.9257\n",
      "     16        \u001B[36m0.9935\u001B[0m       \u001B[32m0.5931\u001B[0m        1.2655        2.9248\n",
      "     17        \u001B[36m0.9714\u001B[0m       \u001B[32m0.6345\u001B[0m        \u001B[35m0.9526\u001B[0m     +  2.9253\n",
      "     18        \u001B[36m0.8691\u001B[0m       0.4759        1.3258        2.9367\n",
      "     19        \u001B[36m0.8284\u001B[0m       0.6276        1.1020        2.9236\n",
      "     20        \u001B[36m0.7490\u001B[0m       \u001B[32m0.6897\u001B[0m        0.9737        2.9236\n",
      "     21        \u001B[36m0.6063\u001B[0m       \u001B[32m0.7034\u001B[0m        0.9697        2.9492\n",
      "     22        0.6456       0.6966        1.0987        2.9250\n",
      "     23        0.6332       0.6828        0.9948        2.9239\n",
      "     24        \u001B[36m0.5344\u001B[0m       0.7034        0.9757        2.9256\n",
      "     25        \u001B[36m0.4303\u001B[0m       \u001B[32m0.7517\u001B[0m        0.9732        2.9244\n",
      "     26        \u001B[36m0.3515\u001B[0m       \u001B[32m0.7655\u001B[0m        1.0114        2.9272\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=2;, score=-1.275 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.7144\u001B[0m       \u001B[32m0.2361\u001B[0m        \u001B[35m1.6087\u001B[0m     +  2.9405\n",
      "      2        \u001B[36m1.6267\u001B[0m       0.2361        1.6117        2.9414\n",
      "      3        1.6339       0.2361        \u001B[35m1.6010\u001B[0m     +  2.9219\n",
      "      4        \u001B[36m1.6261\u001B[0m       \u001B[32m0.2431\u001B[0m        \u001B[35m1.5985\u001B[0m     +  2.9411\n",
      "      5        1.6265       \u001B[32m0.2500\u001B[0m        \u001B[35m1.5929\u001B[0m     +  2.9422\n",
      "      6        \u001B[36m1.6187\u001B[0m       \u001B[32m0.2569\u001B[0m        1.5972        2.9453\n",
      "      7        \u001B[36m1.5997\u001B[0m       0.2361        1.5941        2.9277\n",
      "      8        1.6151       0.2361        1.6000        2.9207\n",
      "      9        1.5998       0.2431        \u001B[35m1.5920\u001B[0m     +  2.9191\n",
      "     10        \u001B[36m1.5888\u001B[0m       0.2361        \u001B[35m1.5490\u001B[0m     +  2.9355\n",
      "     11        \u001B[36m1.5398\u001B[0m       0.2431        1.5866        2.9415\n",
      "     12        \u001B[36m1.5258\u001B[0m       \u001B[32m0.3125\u001B[0m        1.5494        2.9218\n",
      "     13        \u001B[36m1.5225\u001B[0m       \u001B[32m0.4097\u001B[0m        \u001B[35m1.4414\u001B[0m     +  2.9278\n",
      "     14        1.5681       0.3264        1.4911        2.9345\n",
      "     15        \u001B[36m1.4965\u001B[0m       \u001B[32m0.4375\u001B[0m        1.4717        2.9301\n",
      "     16        \u001B[36m1.4535\u001B[0m       0.3681        1.4463        2.9355\n",
      "     17        \u001B[36m1.3850\u001B[0m       0.3542        \u001B[35m1.4239\u001B[0m     +  2.9240\n",
      "     18        1.3853       0.3750        \u001B[35m1.4144\u001B[0m     +  2.9382\n",
      "     19        1.4377       0.3819        \u001B[35m1.3908\u001B[0m     +  2.9322\n",
      "     20        \u001B[36m1.3429\u001B[0m       0.4028        \u001B[35m1.3316\u001B[0m     +  2.9361\n",
      "     21        \u001B[36m1.3030\u001B[0m       0.4097        \u001B[35m1.3131\u001B[0m     +  2.9362\n",
      "     22        \u001B[36m1.2912\u001B[0m       0.3889        \u001B[35m1.2915\u001B[0m     +  2.9481\n",
      "     23        \u001B[36m1.2664\u001B[0m       0.4167        \u001B[35m1.2804\u001B[0m     +  2.9597\n",
      "     24        \u001B[36m1.2559\u001B[0m       0.3472        1.4356        2.9670\n",
      "     25        \u001B[36m1.2189\u001B[0m       0.4097        \u001B[35m1.2582\u001B[0m     +  2.9211\n",
      "     26        \u001B[36m1.2039\u001B[0m       0.3542        1.4586        2.9337\n",
      "     27        \u001B[36m1.1947\u001B[0m       0.3681        1.3701        2.9244\n",
      "     28        \u001B[36m1.1502\u001B[0m       0.3889        1.3853        2.9204\n",
      "     29        \u001B[36m1.1408\u001B[0m       0.4167        \u001B[35m1.1968\u001B[0m     +  2.9203\n",
      "     30        1.1511       0.3542        1.9815        2.9358\n",
      "     31        1.1785       0.4028        1.2343        2.9187\n",
      "     32        \u001B[36m1.1279\u001B[0m       0.3958        1.2428        2.9185\n",
      "     33        1.1553       0.3472        1.9698        2.9192\n",
      "     34        1.1609       \u001B[32m0.4861\u001B[0m        1.2972        2.9187\n",
      "     35        \u001B[36m1.1031\u001B[0m       \u001B[32m0.5278\u001B[0m        \u001B[35m1.1576\u001B[0m     +  2.9226\n",
      "     36        \u001B[36m1.0479\u001B[0m       0.5278        1.1744        2.9591\n",
      "     37        1.1209       0.4653        1.4916        2.9383\n",
      "     38        1.0766       0.5139        1.3965        2.9207\n",
      "     39        1.0916       0.4792        1.3942        2.9201\n",
      "     40        \u001B[36m1.0412\u001B[0m       \u001B[32m0.5417\u001B[0m        \u001B[35m1.1207\u001B[0m     +  2.9213\n",
      "     41        1.0432       0.3750        1.6173        2.9452\n",
      "     42        \u001B[36m1.0373\u001B[0m       0.4931        1.8872        2.9238\n",
      "     43        \u001B[36m0.9135\u001B[0m       0.5347        1.4463        2.9320\n",
      "     44        0.9259       0.5347        1.1494        2.9270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     45        \u001B[36m0.8846\u001B[0m       0.5139        2.0304        2.9231\n",
      "     46        \u001B[36m0.8779\u001B[0m       0.4167        1.3668        2.9297\n",
      "     47        0.8781       \u001B[32m0.5486\u001B[0m        1.2161        2.9240\n",
      "     48        0.8960       \u001B[32m0.5625\u001B[0m        \u001B[35m1.0367\u001B[0m     +  2.9221\n",
      "     49        \u001B[36m0.8187\u001B[0m       0.5486        1.5515        2.9402\n",
      "     50        \u001B[36m0.8004\u001B[0m       0.5486        1.7426        2.9220\n",
      "     51        0.8721       0.5625        1.1154        2.9188\n",
      "     52        0.8225       \u001B[32m0.5764\u001B[0m        1.3729        2.9200\n",
      "     53        0.8138       0.5694        1.1443        2.9158\n",
      "     54        \u001B[36m0.7613\u001B[0m       \u001B[32m0.5833\u001B[0m        1.1395        2.9107\n",
      "     55        0.7786       0.5486        1.3300        2.9253\n",
      "     56        0.8451       0.5694        1.3181        2.9374\n",
      "     57        0.8539       0.5625        1.0768        2.9256\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-2.196 total time= 3.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.7323\u001B[0m       \u001B[32m0.1586\u001B[0m        \u001B[35m1.6064\u001B[0m     +  2.9393\n",
      "      2        \u001B[36m1.6283\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.5998\u001B[0m     +  2.9493\n",
      "      3        1.6411       0.2069        1.6000        2.9544\n",
      "      4        \u001B[36m1.6078\u001B[0m       0.2276        \u001B[35m1.5907\u001B[0m     +  2.9407\n",
      "      5        \u001B[36m1.5944\u001B[0m       \u001B[32m0.3034\u001B[0m        \u001B[35m1.5656\u001B[0m     +  2.9458\n",
      "      6        \u001B[36m1.5820\u001B[0m       0.2690        1.5846        2.9619\n",
      "      7        \u001B[36m1.5785\u001B[0m       0.2483        2.1829        2.9488\n",
      "      8        \u001B[36m1.5566\u001B[0m       0.2759        \u001B[35m1.4928\u001B[0m     +  2.9616\n",
      "      9        \u001B[36m1.5077\u001B[0m       0.2690        \u001B[35m1.4181\u001B[0m     +  2.9518\n",
      "     10        \u001B[36m1.5027\u001B[0m       \u001B[32m0.3310\u001B[0m        1.5255        2.9421\n",
      "     11        \u001B[36m1.4552\u001B[0m       0.2483        2.9604        2.9343\n",
      "     12        1.4648       0.3241        1.4333        2.9291\n",
      "     13        \u001B[36m1.4411\u001B[0m       0.3310        1.4702        2.9302\n",
      "     14        \u001B[36m1.3585\u001B[0m       \u001B[32m0.3586\u001B[0m        \u001B[35m1.4103\u001B[0m     +  2.9267\n",
      "     15        1.3810       \u001B[32m0.4000\u001B[0m        1.4201        2.9443\n",
      "     16        \u001B[36m1.3289\u001B[0m       0.3310        1.7238        2.9290\n",
      "     17        \u001B[36m1.3115\u001B[0m       0.3862        1.4404        2.9268\n",
      "     18        \u001B[36m1.2976\u001B[0m       \u001B[32m0.4276\u001B[0m        \u001B[35m1.2906\u001B[0m     +  2.9334\n",
      "     19        \u001B[36m1.2360\u001B[0m       0.3724        2.3301        2.9550\n",
      "     20        1.2390       \u001B[32m0.4483\u001B[0m        1.3071        2.9391\n",
      "     21        \u001B[36m1.1915\u001B[0m       0.4483        1.2967        2.9267\n",
      "     22        \u001B[36m1.1542\u001B[0m       \u001B[32m0.4690\u001B[0m        \u001B[35m1.2794\u001B[0m     +  2.9387\n",
      "     23        \u001B[36m1.1250\u001B[0m       0.4483        1.3370        2.9419\n",
      "     24        1.1324       0.4621        1.2936        2.9410\n",
      "     25        \u001B[36m1.0625\u001B[0m       \u001B[32m0.4966\u001B[0m        \u001B[35m1.2397\u001B[0m     +  2.9381\n",
      "     26        1.1577       0.4621        1.3868        2.9503\n",
      "     27        1.1905       0.2552        1.5561        2.9359\n",
      "     28        1.2359       0.3241       18.1754        2.9468\n",
      "     29        1.1779       0.4345        1.4841        2.9460\n",
      "     30        1.0860       0.4966        1.4372        2.9498\n",
      "     31        \u001B[36m0.9752\u001B[0m       \u001B[32m0.5034\u001B[0m        1.3818        2.9367\n",
      "     32        \u001B[36m0.9060\u001B[0m       \u001B[32m0.5448\u001B[0m        1.4857        2.9590\n",
      "     33        \u001B[36m0.8791\u001B[0m       0.5310        1.5056        2.9530\n",
      "     34        \u001B[36m0.8547\u001B[0m       0.5241        1.3729        2.9385\n",
      "     35        0.8547       0.4966        \u001B[35m1.2077\u001B[0m     +  2.9489\n",
      "     36        0.8907       0.4345        3.8148        2.9548\n",
      "     37        0.9318       0.4828        1.7264        2.9528\n",
      "     38        0.8911       0.4828        1.6032        2.9431\n",
      "     39        \u001B[36m0.8427\u001B[0m       \u001B[32m0.5517\u001B[0m        1.4432        2.9469\n",
      "     40        \u001B[36m0.7846\u001B[0m       0.5448        1.3078        2.9543\n",
      "     41        \u001B[36m0.7837\u001B[0m       0.5172        1.5390        2.9584\n",
      "     42        \u001B[36m0.7749\u001B[0m       0.5241        1.4694        2.9501\n",
      "     43        0.7751       \u001B[32m0.5724\u001B[0m        1.4239        2.9425\n",
      "     44        \u001B[36m0.7605\u001B[0m       0.5517        1.6069        2.9659\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.457 total time= 2.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.7553\u001B[0m       \u001B[32m0.2276\u001B[0m        \u001B[35m1.5966\u001B[0m     +  2.9357\n",
      "      2        \u001B[36m1.6167\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5917\u001B[0m     +  2.9572\n",
      "      3        \u001B[36m1.6031\u001B[0m       0.2483        1.5946        2.9587\n",
      "      4        \u001B[36m1.5992\u001B[0m       0.2345        1.5959        2.9476\n",
      "      5        \u001B[36m1.5963\u001B[0m       0.2069        1.5971        2.9368\n",
      "      6        1.6150       0.2483        1.5928        2.9385\n",
      "      7        1.6015       \u001B[32m0.2828\u001B[0m        \u001B[35m1.5899\u001B[0m     +  2.9208\n",
      "      8        1.6013       0.2138        1.5946        2.9475\n",
      "      9        1.6009       0.2345        1.5966        2.9303\n",
      "     10        1.5983       0.2069        \u001B[35m1.5826\u001B[0m     +  2.9303\n",
      "     11        \u001B[36m1.5954\u001B[0m       0.2345        1.5864        2.9584\n",
      "     12        \u001B[36m1.5764\u001B[0m       \u001B[32m0.3517\u001B[0m        \u001B[35m1.5590\u001B[0m     +  2.9518\n",
      "     13        \u001B[36m1.5419\u001B[0m       0.2690        \u001B[35m1.4637\u001B[0m     +  2.9582\n",
      "     14        \u001B[36m1.4768\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.4623\u001B[0m     +  2.9623\n",
      "     15        \u001B[36m1.4397\u001B[0m       \u001B[32m0.4138\u001B[0m        \u001B[35m1.3546\u001B[0m     +  2.9565\n",
      "     16        \u001B[36m1.4303\u001B[0m       0.3379        1.3698        2.9522\n",
      "     17        \u001B[36m1.4042\u001B[0m       0.3310        1.3678        2.9438\n",
      "     18        \u001B[36m1.3557\u001B[0m       0.3655        \u001B[35m1.2949\u001B[0m     +  2.9342\n",
      "     19        \u001B[36m1.2900\u001B[0m       \u001B[32m0.4966\u001B[0m        1.3054        2.9502\n",
      "     20        1.2987       0.4138        1.3268        2.9475\n",
      "     21        \u001B[36m1.2600\u001B[0m       0.4483        1.3438        2.9500\n",
      "     22        \u001B[36m1.2455\u001B[0m       \u001B[32m0.5103\u001B[0m        1.5395        2.9728\n",
      "     23        \u001B[36m1.1964\u001B[0m       0.4759        \u001B[35m1.2509\u001B[0m     +  2.9459\n",
      "     24        \u001B[36m1.1388\u001B[0m       \u001B[32m0.5517\u001B[0m        \u001B[35m1.1667\u001B[0m     +  2.9652\n",
      "     25        \u001B[36m1.0976\u001B[0m       0.5517        1.1900        2.9459\n",
      "     26        \u001B[36m1.0462\u001B[0m       0.4759        1.2137        2.9306\n",
      "     27        1.0752       0.4897        1.7025        2.9531\n",
      "     28        \u001B[36m1.0304\u001B[0m       \u001B[32m0.5586\u001B[0m        \u001B[35m1.1651\u001B[0m     +  2.9697\n",
      "     29        \u001B[36m1.0128\u001B[0m       0.5448        1.2015        2.9588\n",
      "     30        \u001B[36m0.9819\u001B[0m       0.5241        1.2269        2.9315\n",
      "     31        \u001B[36m0.9395\u001B[0m       0.5517        \u001B[35m1.1485\u001B[0m     +  2.9297\n",
      "     32        0.9784       0.5310        1.3153        2.9769\n",
      "     33        \u001B[36m0.9264\u001B[0m       0.5241        1.2708        2.9528\n",
      "     34        \u001B[36m0.8706\u001B[0m       \u001B[32m0.5724\u001B[0m        \u001B[35m1.1101\u001B[0m     +  2.9551\n",
      "     35        0.8943       0.5517        1.1695        2.9514\n",
      "     36        \u001B[36m0.8460\u001B[0m       0.5586        1.1248        2.9335\n",
      "     37        \u001B[36m0.7847\u001B[0m       \u001B[32m0.6000\u001B[0m        \u001B[35m1.0014\u001B[0m     +  2.9317\n",
      "     38        \u001B[36m0.7599\u001B[0m       0.5517        1.2203        2.9378\n",
      "     39        \u001B[36m0.7037\u001B[0m       0.5862        1.1039        2.9271\n",
      "     40        \u001B[36m0.6942\u001B[0m       0.5655        1.0691        2.9275\n",
      "     41        0.8028       0.5448        1.6237        2.9375\n",
      "     42        0.8157       0.4897        1.3180        2.9367\n",
      "     43        \u001B[36m0.6767\u001B[0m       0.5310        1.2383        2.9353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     44        \u001B[36m0.5868\u001B[0m       0.5517        1.1950        2.9336\n",
      "     45        \u001B[36m0.5635\u001B[0m       0.5586        1.3352        2.9402\n",
      "     46        \u001B[36m0.5342\u001B[0m       0.5655        1.2868        2.9304\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=64, module__num_hidden_layers=3;, score=-1.453 total time= 2.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.4363\u001B[0m       \u001B[32m0.3194\u001B[0m        \u001B[35m1.5685\u001B[0m     +  3.1295\n",
      "      2        \u001B[36m1.6253\u001B[0m       \u001B[32m0.4028\u001B[0m        \u001B[35m1.4503\u001B[0m     +  3.1653\n",
      "      3        \u001B[36m1.4653\u001B[0m       \u001B[32m0.5278\u001B[0m        \u001B[35m1.2148\u001B[0m     +  3.1733\n",
      "      4        \u001B[36m1.2410\u001B[0m       \u001B[32m0.6389\u001B[0m        \u001B[35m1.0937\u001B[0m     +  3.1714\n",
      "      5        \u001B[36m0.9924\u001B[0m       \u001B[32m0.7222\u001B[0m        \u001B[35m0.8558\u001B[0m     +  3.1662\n",
      "      6        \u001B[36m0.7498\u001B[0m       \u001B[32m0.7431\u001B[0m        \u001B[35m0.8511\u001B[0m     +  3.1775\n",
      "      7        \u001B[36m0.5793\u001B[0m       0.6806        1.1339        3.1687\n",
      "      8        \u001B[36m0.4681\u001B[0m       0.6528        1.1438        3.1495\n",
      "      9        \u001B[36m0.3918\u001B[0m       0.7222        0.9927        3.1347\n",
      "     10        \u001B[36m0.3297\u001B[0m       0.6597        1.2883        3.1412\n",
      "     11        0.3315       0.7222        1.1594        3.1441\n",
      "     12        \u001B[36m0.1285\u001B[0m       \u001B[32m0.8194\u001B[0m        \u001B[35m0.8293\u001B[0m     +  3.1507\n",
      "     13        \u001B[36m0.0951\u001B[0m       0.8194        \u001B[35m0.7233\u001B[0m     +  3.1645\n",
      "     14        \u001B[36m0.0792\u001B[0m       0.7292        1.6970        3.1635\n",
      "     15        0.1037       0.8125        0.8289        3.1417\n",
      "     16        \u001B[36m0.0300\u001B[0m       0.7917        0.7659        3.1475\n",
      "     17        0.0350       0.8056        0.8582        3.1514\n",
      "     18        0.0592       0.6944        1.7527        3.1347\n",
      "     19        0.3372       0.6458        1.9429        3.1469\n",
      "     20        0.0831       0.7361        1.1281        3.1423\n",
      "     21        0.0505       0.7708        1.1046        3.1463\n",
      "     22        \u001B[36m0.0240\u001B[0m       0.7361        1.0697        3.1478\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.920 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.5543\u001B[0m       \u001B[32m0.1793\u001B[0m        \u001B[35m1.6092\u001B[0m     +  3.1887\n",
      "      2        \u001B[36m1.6334\u001B[0m       \u001B[32m0.3862\u001B[0m        \u001B[35m1.4766\u001B[0m     +  3.1693\n",
      "      3        \u001B[36m1.5187\u001B[0m       0.3517        2.0347        3.1677\n",
      "      4        \u001B[36m1.3256\u001B[0m       \u001B[32m0.6138\u001B[0m        \u001B[35m1.0696\u001B[0m     +  3.1491\n",
      "      5        \u001B[36m1.0465\u001B[0m       \u001B[32m0.6345\u001B[0m        \u001B[35m0.9741\u001B[0m     +  3.1731\n",
      "      6        \u001B[36m0.8178\u001B[0m       \u001B[32m0.6897\u001B[0m        0.9787        3.1695\n",
      "      7        \u001B[36m0.6687\u001B[0m       0.6000        1.4756        3.1636\n",
      "      8        \u001B[36m0.5302\u001B[0m       0.6897        \u001B[35m0.9458\u001B[0m     +  3.1499\n",
      "      9        \u001B[36m0.3617\u001B[0m       \u001B[32m0.7862\u001B[0m        \u001B[35m0.6680\u001B[0m     +  3.1690\n",
      "     10        \u001B[36m0.1942\u001B[0m       \u001B[32m0.8138\u001B[0m        0.6937        3.1668\n",
      "     11        \u001B[36m0.1050\u001B[0m       \u001B[32m0.8207\u001B[0m        0.7933        3.1538\n",
      "     12        0.1694       0.7310        1.0629        3.1614\n",
      "     13        0.2094       0.7448        1.0029        3.1466\n",
      "     14        0.1371       0.6483        1.3633        3.1532\n",
      "     15        \u001B[36m0.0614\u001B[0m       0.7862        1.2638        3.1483\n",
      "     16        \u001B[36m0.0330\u001B[0m       \u001B[32m0.8414\u001B[0m        1.0295        3.1497\n",
      "     17        0.0411       \u001B[32m0.8483\u001B[0m        1.0967        3.1531\n",
      "     18        \u001B[36m0.0092\u001B[0m       0.8414        1.1440        3.1495\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-1.036 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.6202\u001B[0m       \u001B[32m0.2690\u001B[0m        \u001B[35m1.5602\u001B[0m     +  3.1597\n",
      "      2        \u001B[36m1.5468\u001B[0m       \u001B[32m0.3793\u001B[0m        1.7808        3.1798\n",
      "      3        \u001B[36m1.3805\u001B[0m       \u001B[32m0.4483\u001B[0m        \u001B[35m1.2581\u001B[0m     +  3.1736\n",
      "      4        \u001B[36m1.0779\u001B[0m       0.3793        3.1057        3.1763\n",
      "      5        \u001B[36m0.9034\u001B[0m       \u001B[32m0.6897\u001B[0m        1.3900        3.1761\n",
      "      6        \u001B[36m0.6577\u001B[0m       \u001B[32m0.7724\u001B[0m        \u001B[35m1.1550\u001B[0m     +  3.1793\n",
      "      7        \u001B[36m0.6248\u001B[0m       \u001B[32m0.7793\u001B[0m        \u001B[35m0.9087\u001B[0m     +  3.1854\n",
      "      8        \u001B[36m0.5277\u001B[0m       0.7793        \u001B[35m0.5857\u001B[0m     +  3.1967\n",
      "      9        \u001B[36m0.3320\u001B[0m       0.6621        1.0627        3.1708\n",
      "     10        \u001B[36m0.2334\u001B[0m       0.7655        0.9687        3.1674\n",
      "     11        \u001B[36m0.1966\u001B[0m       0.7793        0.7381        3.1521\n",
      "     12        0.2500       0.7517        1.0918        3.1545\n",
      "     13        \u001B[36m0.1212\u001B[0m       \u001B[32m0.8276\u001B[0m        1.0680        3.1543\n",
      "     14        \u001B[36m0.0873\u001B[0m       \u001B[32m0.8345\u001B[0m        1.0486        3.1593\n",
      "     15        0.1400       0.7931        0.8221        3.1503\n",
      "     16        0.0884       0.8069        1.0430        3.1550\n",
      "     17        0.1052       0.7931        0.9069        3.1530\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=1;, score=-0.992 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9669\u001B[0m       \u001B[32m0.2500\u001B[0m        \u001B[35m1.6002\u001B[0m     +  3.1581\n",
      "      2        \u001B[36m1.6877\u001B[0m       0.2083        1.6253        3.1697\n",
      "      3        \u001B[36m1.6449\u001B[0m       0.2500        \u001B[35m1.5875\u001B[0m     +  3.1432\n",
      "      4        \u001B[36m1.6317\u001B[0m       \u001B[32m0.2847\u001B[0m        1.6174        3.1669\n",
      "      5        \u001B[36m1.6238\u001B[0m       0.2292        1.5905        3.1627\n",
      "      6        \u001B[36m1.6101\u001B[0m       0.2431        1.5929        3.1552\n",
      "      7        \u001B[36m1.5776\u001B[0m       0.2431        \u001B[35m1.5112\u001B[0m     +  3.1530\n",
      "      8        1.5872       0.2431        1.5852        3.1660\n",
      "      9        \u001B[36m1.5026\u001B[0m       0.2431        \u001B[35m1.4372\u001B[0m     +  3.1457\n",
      "     10        \u001B[36m1.4695\u001B[0m       0.2431        1.4921        3.1595\n",
      "     11        \u001B[36m1.4101\u001B[0m       \u001B[32m0.3125\u001B[0m        \u001B[35m1.3899\u001B[0m     +  3.1493\n",
      "     12        1.4462       0.2917        1.4375        3.1568\n",
      "     13        1.4490       \u001B[32m0.3819\u001B[0m        1.4835        3.1472\n",
      "     14        \u001B[36m1.3500\u001B[0m       \u001B[32m0.5139\u001B[0m        \u001B[35m1.3727\u001B[0m     +  3.1623\n",
      "     15        \u001B[36m1.2100\u001B[0m       0.4722        1.7897        3.1593\n",
      "     16        \u001B[36m1.1960\u001B[0m       0.4514        1.4783        3.1438\n",
      "     17        \u001B[36m1.1499\u001B[0m       0.4444        1.4943        3.1450\n",
      "     18        \u001B[36m1.1263\u001B[0m       0.3889        2.0409        3.1458\n",
      "     19        \u001B[36m1.0974\u001B[0m       \u001B[32m0.5694\u001B[0m        \u001B[35m1.0877\u001B[0m     +  3.1452\n",
      "     20        1.1184       0.3194        1.4995        3.1623\n",
      "     21        1.3002       0.4236        1.3277        3.1490\n",
      "     22        1.1677       0.5486        1.3827        3.1513\n",
      "     23        \u001B[36m0.9931\u001B[0m       \u001B[32m0.5764\u001B[0m        1.1614        3.1504\n",
      "     24        \u001B[36m0.9155\u001B[0m       0.5625        1.2213        3.1453\n",
      "     25        \u001B[36m0.8951\u001B[0m       \u001B[32m0.5972\u001B[0m        1.2232        3.1431\n",
      "     26        \u001B[36m0.8648\u001B[0m       0.5903        1.2768        3.1417\n",
      "     27        \u001B[36m0.8491\u001B[0m       0.5694        1.2703        3.1435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001B[36m0.8455\u001B[0m       0.5347        1.9624        3.1419\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.254 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0371\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5828\u001B[0m     +  3.1510\n",
      "      2        \u001B[36m1.6709\u001B[0m       0.2483        1.6029        3.1780\n",
      "      3        \u001B[36m1.6408\u001B[0m       0.2483        1.5839        3.1494\n",
      "      4        \u001B[36m1.6207\u001B[0m       \u001B[32m0.2828\u001B[0m        1.5850        3.1641\n",
      "      5        \u001B[36m1.5864\u001B[0m       \u001B[32m0.2897\u001B[0m        \u001B[35m1.4807\u001B[0m     +  3.1644\n",
      "      6        \u001B[36m1.5739\u001B[0m       0.2345        1.5085        3.1717\n",
      "      7        \u001B[36m1.4964\u001B[0m       \u001B[32m0.3103\u001B[0m        1.6537        3.1520\n",
      "      8        \u001B[36m1.4425\u001B[0m       \u001B[32m0.3241\u001B[0m        \u001B[35m1.4451\u001B[0m     +  3.1564\n",
      "      9        \u001B[36m1.4191\u001B[0m       \u001B[32m0.3517\u001B[0m        1.5116        3.1721\n",
      "     10        \u001B[36m1.3479\u001B[0m       \u001B[32m0.3862\u001B[0m        1.4457        3.1501\n",
      "     11        1.3689       \u001B[32m0.4345\u001B[0m        \u001B[35m1.3574\u001B[0m     +  3.1508\n",
      "     12        \u001B[36m1.2570\u001B[0m       \u001B[32m0.4414\u001B[0m        \u001B[35m1.2387\u001B[0m     +  3.1666\n",
      "     13        \u001B[36m1.2343\u001B[0m       \u001B[32m0.4897\u001B[0m        1.3878        3.1794\n",
      "     14        \u001B[36m1.1877\u001B[0m       \u001B[32m0.5103\u001B[0m        1.3844        3.1527\n",
      "     15        \u001B[36m1.1715\u001B[0m       0.4759        1.2889        3.1501\n",
      "     16        \u001B[36m1.0561\u001B[0m       0.5034        1.2495        3.1564\n",
      "     17        \u001B[36m1.0111\u001B[0m       \u001B[32m0.5172\u001B[0m        1.2766        3.1507\n",
      "     18        1.0121       0.3241        3.5848        3.1564\n",
      "     19        1.0723       0.4897        1.6330        3.1619\n",
      "     20        \u001B[36m0.9629\u001B[0m       \u001B[32m0.5310\u001B[0m        1.2962        3.1730\n",
      "     21        \u001B[36m0.9510\u001B[0m       0.5103        1.4946        3.1606\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.244 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8723\u001B[0m       \u001B[32m0.2345\u001B[0m        \u001B[35m1.5919\u001B[0m     +  3.1575\n",
      "      2        \u001B[36m1.6226\u001B[0m       \u001B[32m0.2414\u001B[0m        \u001B[35m1.5279\u001B[0m     +  3.1679\n",
      "      3        \u001B[36m1.6202\u001B[0m       \u001B[32m0.2483\u001B[0m        1.5575        3.1753\n",
      "      4        \u001B[36m1.5681\u001B[0m       \u001B[32m0.2552\u001B[0m        1.9237        3.1622\n",
      "      5        \u001B[36m1.5116\u001B[0m       \u001B[32m0.3793\u001B[0m        \u001B[35m1.4278\u001B[0m     +  3.1539\n",
      "      6        \u001B[36m1.4248\u001B[0m       \u001B[32m0.4000\u001B[0m        \u001B[35m1.2745\u001B[0m     +  3.1916\n",
      "      7        \u001B[36m1.3214\u001B[0m       \u001B[32m0.4069\u001B[0m        1.3222        3.1707\n",
      "      8        \u001B[36m1.2209\u001B[0m       \u001B[32m0.4897\u001B[0m        1.4060        3.1514\n",
      "      9        \u001B[36m1.1084\u001B[0m       \u001B[32m0.6138\u001B[0m        \u001B[35m1.1709\u001B[0m     +  3.1597\n",
      "     10        \u001B[36m0.9630\u001B[0m       \u001B[32m0.6345\u001B[0m        \u001B[35m0.9053\u001B[0m     +  3.1790\n",
      "     11        \u001B[36m0.8267\u001B[0m       0.6138        0.9974        3.1783\n",
      "     12        \u001B[36m0.6890\u001B[0m       \u001B[32m0.7172\u001B[0m        1.0590        3.1537\n",
      "     13        \u001B[36m0.5462\u001B[0m       0.4414        1.4135        3.1539\n",
      "     14        \u001B[36m0.5288\u001B[0m       0.6138        3.2852        3.1669\n",
      "     15        0.5632       0.6207        1.4556        3.1585\n",
      "     16        \u001B[36m0.4762\u001B[0m       \u001B[32m0.7793\u001B[0m        \u001B[35m0.7645\u001B[0m     +  3.1610\n",
      "     17        \u001B[36m0.2535\u001B[0m       \u001B[32m0.7931\u001B[0m        0.8715        3.1760\n",
      "     18        0.2600       \u001B[32m0.8069\u001B[0m        0.8953        3.1747\n",
      "     19        \u001B[36m0.2320\u001B[0m       0.7862        0.8794        3.1669\n",
      "     20        0.2438       0.7724        0.9762        3.1570\n",
      "     21        0.2987       0.7241        1.0686        3.1710\n",
      "     22        \u001B[36m0.1678\u001B[0m       0.7655        1.0990        3.1550\n",
      "     23        \u001B[36m0.0612\u001B[0m       0.8069        1.0557        3.1529\n",
      "     24        \u001B[36m0.0341\u001B[0m       \u001B[32m0.8276\u001B[0m        1.1045        3.1570\n",
      "     25        \u001B[36m0.0242\u001B[0m       \u001B[32m0.8414\u001B[0m        1.2819        3.1579\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=2;, score=-1.395 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8098\u001B[0m       \u001B[32m0.2083\u001B[0m        \u001B[35m1.6043\u001B[0m     +  3.1528\n",
      "      2        \u001B[36m1.6720\u001B[0m       0.2083        \u001B[35m1.6017\u001B[0m     +  3.1795\n",
      "      3        \u001B[36m1.6171\u001B[0m       \u001B[32m0.2153\u001B[0m        \u001B[35m1.5887\u001B[0m     +  3.1807\n",
      "      4        1.6275       \u001B[32m0.2500\u001B[0m        1.5976        3.1711\n",
      "      5        \u001B[36m1.5942\u001B[0m       0.2431        1.6116        3.1509\n",
      "      6        \u001B[36m1.5733\u001B[0m       \u001B[32m0.2778\u001B[0m        1.5903        3.1493\n",
      "      7        \u001B[36m1.5524\u001B[0m       0.2778        1.6112        3.1505\n",
      "      8        \u001B[36m1.4686\u001B[0m       \u001B[32m0.2847\u001B[0m        1.6629        3.1496\n",
      "      9        \u001B[36m1.3847\u001B[0m       0.2778        1.6337        3.1505\n",
      "     10        1.3951       \u001B[32m0.3542\u001B[0m        \u001B[35m1.5557\u001B[0m     +  3.1483\n",
      "     11        \u001B[36m1.3439\u001B[0m       0.3333        \u001B[35m1.5058\u001B[0m     +  3.1769\n",
      "     12        \u001B[36m1.3367\u001B[0m       0.3333        \u001B[35m1.4126\u001B[0m     +  3.1709\n",
      "     13        \u001B[36m1.2913\u001B[0m       \u001B[32m0.3819\u001B[0m        1.4607        3.1750\n",
      "     14        \u001B[36m1.2838\u001B[0m       \u001B[32m0.3958\u001B[0m        1.4269        3.1590\n",
      "     15        \u001B[36m1.2374\u001B[0m       0.3958        \u001B[35m1.3344\u001B[0m     +  3.1548\n",
      "     16        1.2711       \u001B[32m0.4097\u001B[0m        1.4780        3.1699\n",
      "     17        \u001B[36m1.2181\u001B[0m       0.4097        1.5125        3.1499\n",
      "     18        1.2188       0.4028        1.3390        3.1476\n",
      "     19        \u001B[36m1.2083\u001B[0m       0.4097        1.4251        3.1504\n",
      "     20        \u001B[36m1.1832\u001B[0m       0.4028        \u001B[35m1.3139\u001B[0m     +  3.1563\n",
      "     21        \u001B[36m1.1692\u001B[0m       0.4097        \u001B[35m1.3106\u001B[0m     +  3.1638\n",
      "     22        \u001B[36m1.1603\u001B[0m       0.4097        1.3164        3.1674\n",
      "     23        \u001B[36m1.1575\u001B[0m       0.4097        1.3184        3.1489\n",
      "     24        \u001B[36m1.1566\u001B[0m       \u001B[32m0.4167\u001B[0m        1.3195        3.1491\n",
      "     25        \u001B[36m1.1478\u001B[0m       0.3889        1.3378        3.1492\n",
      "     26        \u001B[36m1.1443\u001B[0m       0.3889        1.3468        3.1487\n",
      "     27        \u001B[36m1.1405\u001B[0m       0.4028        1.3480        3.1487\n",
      "     28        \u001B[36m1.1334\u001B[0m       0.4028        1.3572        3.1499\n",
      "     29        \u001B[36m1.1303\u001B[0m       0.4028        1.3732        3.1670\n",
      "     30        1.1358       0.3889        1.3269        3.1591\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.433 total time= 1.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8484\u001B[0m       \u001B[32m0.2138\u001B[0m        \u001B[35m1.6041\u001B[0m     +  3.1650\n",
      "      2        \u001B[36m1.6421\u001B[0m       \u001B[32m0.2621\u001B[0m        \u001B[35m1.6012\u001B[0m     +  3.2027\n",
      "      3        \u001B[36m1.6269\u001B[0m       0.2276        \u001B[35m1.5977\u001B[0m     +  3.1922\n",
      "      4        \u001B[36m1.6040\u001B[0m       0.2552        \u001B[35m1.5753\u001B[0m     +  3.1926\n",
      "      5        \u001B[36m1.5677\u001B[0m       0.2414        \u001B[35m1.5463\u001B[0m     +  3.1750\n",
      "      6        \u001B[36m1.5339\u001B[0m       0.2414        1.5567        3.1820\n",
      "      7        \u001B[36m1.4827\u001B[0m       \u001B[32m0.3310\u001B[0m        \u001B[35m1.5118\u001B[0m     +  3.1630\n",
      "      8        \u001B[36m1.4152\u001B[0m       \u001B[32m0.3379\u001B[0m        \u001B[35m1.4449\u001B[0m     +  3.1765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001B[36m1.3858\u001B[0m       \u001B[32m0.3448\u001B[0m        1.5114        3.1878\n",
      "     10        \u001B[36m1.3511\u001B[0m       \u001B[32m0.3862\u001B[0m        1.4847        3.1800\n",
      "     11        \u001B[36m1.3015\u001B[0m       0.3724        \u001B[35m1.3903\u001B[0m     +  3.1824\n",
      "     12        \u001B[36m1.2828\u001B[0m       \u001B[32m0.4207\u001B[0m        1.4693        3.1863\n",
      "     13        \u001B[36m1.2625\u001B[0m       0.3655        1.3915        3.1676\n",
      "     14        \u001B[36m1.2454\u001B[0m       0.4207        1.4117        3.1649\n",
      "     15        \u001B[36m1.1967\u001B[0m       0.4000        1.4827        3.1574\n",
      "     16        \u001B[36m1.1353\u001B[0m       \u001B[32m0.4897\u001B[0m        \u001B[35m1.2600\u001B[0m     +  3.1562\n",
      "     17        1.1381       0.4621        1.2869        3.1940\n",
      "     18        \u001B[36m1.0381\u001B[0m       0.4414        1.6344        3.1644\n",
      "     19        \u001B[36m0.9700\u001B[0m       0.4138        1.7023        3.1725\n",
      "     20        \u001B[36m0.9564\u001B[0m       0.4000        2.6015        3.1783\n",
      "     21        0.9678       0.4069        3.6252        3.1720\n",
      "     22        0.9862       0.4138        2.9490        3.1643\n",
      "     23        1.0149       0.4690        1.6086        3.1810\n",
      "     24        \u001B[36m0.9372\u001B[0m       \u001B[32m0.5172\u001B[0m        1.3075        3.1566\n",
      "     25        \u001B[36m0.9149\u001B[0m       0.4897        2.3145        3.1657\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.218 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.8217\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.6010\u001B[0m     +  3.1744\n",
      "      2        \u001B[36m1.6403\u001B[0m       0.2483        \u001B[35m1.5911\u001B[0m     +  3.1918\n",
      "      3        \u001B[36m1.6287\u001B[0m       0.2483        1.5923        3.1896\n",
      "      4        \u001B[36m1.6058\u001B[0m       \u001B[32m0.2828\u001B[0m        \u001B[35m1.5600\u001B[0m     +  3.1613\n",
      "      5        \u001B[36m1.5657\u001B[0m       \u001B[32m0.3310\u001B[0m        1.5742        3.1751\n",
      "      6        \u001B[36m1.5193\u001B[0m       0.2552        1.8579        3.1589\n",
      "      7        \u001B[36m1.4930\u001B[0m       \u001B[32m0.3931\u001B[0m        \u001B[35m1.4071\u001B[0m     +  3.1589\n",
      "      8        \u001B[36m1.3810\u001B[0m       0.2483        8.1434        3.1794\n",
      "      9        \u001B[36m1.3347\u001B[0m       \u001B[32m0.4069\u001B[0m        1.6658        3.1592\n",
      "     10        \u001B[36m1.3224\u001B[0m       0.3310        1.5881        3.1591\n",
      "     11        \u001B[36m1.2787\u001B[0m       \u001B[32m0.4897\u001B[0m        \u001B[35m1.3348\u001B[0m     +  3.1564\n",
      "     12        1.2871       0.4759        1.3557        3.1848\n",
      "     13        1.3219       0.4138        1.7984        3.1630\n",
      "     14        \u001B[36m1.2603\u001B[0m       0.4414        1.4561        3.1639\n",
      "     15        \u001B[36m1.0974\u001B[0m       0.4828        1.8171        3.1624\n",
      "     16        \u001B[36m1.0534\u001B[0m       \u001B[32m0.5862\u001B[0m        \u001B[35m1.1076\u001B[0m     +  3.1625\n",
      "     17        \u001B[36m1.0014\u001B[0m       0.5448        1.3269        3.1792\n",
      "     18        \u001B[36m0.9165\u001B[0m       0.5448        1.1437        3.1626\n",
      "     19        0.9211       0.5862        \u001B[35m1.1030\u001B[0m     +  3.1608\n",
      "     20        \u001B[36m0.8747\u001B[0m       \u001B[32m0.6138\u001B[0m        \u001B[35m1.0258\u001B[0m     +  3.1690\n",
      "     21        \u001B[36m0.8589\u001B[0m       0.5931        1.1357        3.1699\n",
      "     22        0.9760       0.4690        1.3326        3.1597\n",
      "     23        1.0582       0.5310        1.1158        3.1591\n",
      "     24        0.8862       0.5310        1.6653        3.1481\n",
      "     25        \u001B[36m0.8568\u001B[0m       0.5931        1.0289        3.1590\n",
      "     26        \u001B[36m0.8033\u001B[0m       0.5931        \u001B[35m1.0082\u001B[0m     +  3.1584\n",
      "     27        0.8180       0.5310        1.1020        3.1844\n",
      "     28        0.8098       0.5862        \u001B[35m1.0054\u001B[0m     +  3.1612\n",
      "     29        \u001B[36m0.7655\u001B[0m       0.6138        1.0098        3.1808\n",
      "     30        0.7711       0.5931        \u001B[35m0.9318\u001B[0m     +  3.1636\n",
      "     31        \u001B[36m0.7370\u001B[0m       \u001B[32m0.6276\u001B[0m        \u001B[35m0.8908\u001B[0m     +  3.1768\n",
      "     32        \u001B[36m0.7201\u001B[0m       0.5586        1.0443        3.1777\n",
      "     33        \u001B[36m0.7072\u001B[0m       0.5931        0.9778        3.1651\n",
      "     34        \u001B[36m0.6885\u001B[0m       0.5862        0.9193        3.1594\n",
      "     35        0.6902       0.6000        0.9590        3.1641\n",
      "     36        \u001B[36m0.6743\u001B[0m       0.5862        0.9426        3.1583\n",
      "     37        \u001B[36m0.6731\u001B[0m       0.5931        0.9031        3.1641\n",
      "     38        0.6779       0.6000        0.9515        3.1704\n",
      "     39        \u001B[36m0.6651\u001B[0m       0.5931        0.9406        3.1580\n",
      "     40        \u001B[36m0.6572\u001B[0m       0.5655        0.9884        3.1579\n",
      "     41        \u001B[36m0.6565\u001B[0m       0.6138        \u001B[35m0.8606\u001B[0m     +  3.1566\n",
      "     42        \u001B[36m0.6406\u001B[0m       0.6138        0.8654        3.1866\n",
      "     43        \u001B[36m0.6400\u001B[0m       0.6207        \u001B[35m0.8130\u001B[0m     +  3.1579\n",
      "     44        0.6418       0.6069        0.9016        3.1897\n",
      "     45        0.7307       0.4828        8.7117        3.1811\n",
      "     46        1.2727       0.5034        1.4287        3.1586\n",
      "     47        0.8719       0.5034        1.3416        3.1592\n",
      "     48        0.7015       0.5931        0.8779        3.1594\n",
      "     49        0.6649       0.5862        0.9812        3.1577\n",
      "     50        0.6403       0.5931        0.9573        3.1594\n",
      "     51        \u001B[36m0.6275\u001B[0m       0.6000        0.9498        3.1604\n",
      "     52        \u001B[36m0.6225\u001B[0m       0.6069        0.9629        3.1640\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=128, module__num_hidden_layers=3;, score=-1.048 total time= 3.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.8419\u001B[0m       \u001B[32m0.2500\u001B[0m        \u001B[35m1.5756\u001B[0m     +  3.5841\n",
      "      2        \u001B[36m1.6119\u001B[0m       \u001B[32m0.5556\u001B[0m        \u001B[35m1.1912\u001B[0m     +  3.6159\n",
      "      3        \u001B[36m1.1776\u001B[0m       \u001B[32m0.6528\u001B[0m        \u001B[35m0.9608\u001B[0m     +  3.6105\n",
      "      4        \u001B[36m0.8270\u001B[0m       \u001B[32m0.6944\u001B[0m        \u001B[35m0.9430\u001B[0m     +  3.6137\n",
      "      5        \u001B[36m0.5869\u001B[0m       0.6250        1.5537        3.6001\n",
      "      6        \u001B[36m0.4781\u001B[0m       0.6806        1.1750        3.5929\n",
      "      7        \u001B[36m0.3096\u001B[0m       \u001B[32m0.7153\u001B[0m        \u001B[35m0.8933\u001B[0m     +  3.5834\n",
      "      8        \u001B[36m0.2338\u001B[0m       \u001B[32m0.7708\u001B[0m        1.2545        3.6055\n",
      "      9        \u001B[36m0.1627\u001B[0m       0.7153        1.4407        3.5895\n",
      "     10        \u001B[36m0.1395\u001B[0m       0.7569        1.0289        3.5994\n",
      "     11        \u001B[36m0.0531\u001B[0m       \u001B[32m0.7986\u001B[0m        \u001B[35m0.8519\u001B[0m     +  3.5904\n",
      "     12        \u001B[36m0.0205\u001B[0m       \u001B[32m0.8056\u001B[0m        0.8765        3.6099\n",
      "     13        \u001B[36m0.0113\u001B[0m       0.7986        0.9412        3.5910\n",
      "     14        \u001B[36m0.0068\u001B[0m       \u001B[32m0.8194\u001B[0m        1.0151        3.6017\n",
      "     15        \u001B[36m0.0038\u001B[0m       0.8194        1.0548        3.6162\n",
      "     16        \u001B[36m0.0037\u001B[0m       0.8194        1.1084        3.6028\n",
      "     17        \u001B[36m0.0022\u001B[0m       0.8125        1.1777        3.5941\n",
      "     18        \u001B[36m0.0020\u001B[0m       0.8056        1.1335        3.5886\n",
      "     19        \u001B[36m0.0020\u001B[0m       0.8056        1.2014        3.5928\n",
      "     20        \u001B[36m0.0007\u001B[0m       0.8056        1.2525        3.6018\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.219 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m3.3004\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.4296\u001B[0m     +  3.6037\n",
      "      2        \u001B[36m1.5446\u001B[0m       \u001B[32m0.5586\u001B[0m        \u001B[35m1.0873\u001B[0m     +  3.6273\n",
      "      3        \u001B[36m1.1239\u001B[0m       \u001B[32m0.6828\u001B[0m        \u001B[35m0.8383\u001B[0m     +  3.6244\n",
      "      4        \u001B[36m0.7910\u001B[0m       0.6690        0.9359        3.6271\n",
      "      5        \u001B[36m0.6557\u001B[0m       \u001B[32m0.7103\u001B[0m        1.2315        3.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001B[36m0.4971\u001B[0m       0.5448        3.5441        3.5925\n",
      "      7        \u001B[36m0.3391\u001B[0m       \u001B[32m0.8000\u001B[0m        \u001B[35m0.6292\u001B[0m     +  3.6075\n",
      "      8        \u001B[36m0.2024\u001B[0m       0.6483        1.8941        3.6341\n",
      "      9        \u001B[36m0.1556\u001B[0m       \u001B[32m0.8483\u001B[0m        0.7218        3.5961\n",
      "     10        \u001B[36m0.1134\u001B[0m       0.7793        0.9536        3.5947\n",
      "     11        0.1199       0.8000        1.0114        3.5963\n",
      "     12        \u001B[36m0.0804\u001B[0m       0.8000        0.9470        3.6103\n",
      "     13        0.0997       0.8138        0.6921        3.6129\n",
      "     14        0.1026       0.6828        1.8238        3.6166\n",
      "     15        0.0926       0.7931        0.9778        3.6034\n",
      "     16        \u001B[36m0.0648\u001B[0m       0.7724        1.0415        3.5978\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-0.861 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m3.0701\u001B[0m       \u001B[32m0.2000\u001B[0m        \u001B[35m1.6122\u001B[0m     +  3.6011\n",
      "      2        \u001B[36m1.6315\u001B[0m       \u001B[32m0.3241\u001B[0m        \u001B[35m1.5468\u001B[0m     +  3.6112\n",
      "      3        \u001B[36m1.5337\u001B[0m       \u001B[32m0.4000\u001B[0m        1.6890        3.6153\n",
      "      4        \u001B[36m1.3094\u001B[0m       0.3034        3.3460        3.5960\n",
      "      5        \u001B[36m1.1403\u001B[0m       \u001B[32m0.6000\u001B[0m        \u001B[35m1.2020\u001B[0m     +  3.6003\n",
      "      6        \u001B[36m0.9428\u001B[0m       0.5241        1.3460        3.6162\n",
      "      7        \u001B[36m0.8081\u001B[0m       0.5034        1.9360        3.5958\n",
      "      8        \u001B[36m0.6354\u001B[0m       \u001B[32m0.6828\u001B[0m        \u001B[35m1.0223\u001B[0m     +  3.5972\n",
      "      9        \u001B[36m0.4026\u001B[0m       \u001B[32m0.7655\u001B[0m        \u001B[35m0.8120\u001B[0m     +  3.6114\n",
      "     10        \u001B[36m0.2023\u001B[0m       \u001B[32m0.8483\u001B[0m        0.8277        3.6121\n",
      "     11        \u001B[36m0.1403\u001B[0m       0.7931        1.0925        3.5943\n",
      "     12        \u001B[36m0.1067\u001B[0m       0.6828        2.0754        3.6022\n",
      "     13        \u001B[36m0.0892\u001B[0m       0.8069        1.1670        3.6012\n",
      "     14        0.1567       0.7586        1.4491        3.6043\n",
      "     15        0.2362       0.7586        1.6213        3.6008\n",
      "     16        0.1467       0.7448        1.8983        3.5995\n",
      "     17        0.1525       0.7310        1.9947        3.5996\n",
      "     18        \u001B[36m0.0439\u001B[0m       0.7586        1.6754        3.6024\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=1;, score=-1.303 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.1580\u001B[0m       \u001B[32m0.2569\u001B[0m        \u001B[35m1.6036\u001B[0m     +  3.5985\n",
      "      2        \u001B[36m1.6950\u001B[0m       0.2431        \u001B[35m1.5712\u001B[0m     +  3.6258\n",
      "      3        \u001B[36m1.6459\u001B[0m       0.2431        1.5897        3.6252\n",
      "      4        \u001B[36m1.6181\u001B[0m       \u001B[32m0.3056\u001B[0m        \u001B[35m1.5238\u001B[0m     +  3.5958\n",
      "      5        \u001B[36m1.5451\u001B[0m       \u001B[32m0.4306\u001B[0m        \u001B[35m1.4905\u001B[0m     +  3.6335\n",
      "      6        \u001B[36m1.4834\u001B[0m       0.3819        \u001B[35m1.4308\u001B[0m     +  3.6041\n",
      "      7        \u001B[36m1.3499\u001B[0m       \u001B[32m0.5833\u001B[0m        \u001B[35m1.0802\u001B[0m     +  3.6174\n",
      "      8        \u001B[36m1.1367\u001B[0m       0.5694        \u001B[35m1.0348\u001B[0m     +  3.6102\n",
      "      9        \u001B[36m0.8241\u001B[0m       \u001B[32m0.5972\u001B[0m        1.1105        3.6126\n",
      "     10        \u001B[36m0.5717\u001B[0m       0.5972        1.5333        3.5892\n",
      "     11        \u001B[36m0.4803\u001B[0m       \u001B[32m0.6111\u001B[0m        1.5254        3.5872\n",
      "     12        \u001B[36m0.3904\u001B[0m       \u001B[32m0.6389\u001B[0m        1.0602        3.5883\n",
      "     13        \u001B[36m0.2340\u001B[0m       \u001B[32m0.6667\u001B[0m        1.2930        3.5875\n",
      "     14        0.2365       \u001B[32m0.7778\u001B[0m        1.2969        3.5897\n",
      "     15        \u001B[36m0.2245\u001B[0m       0.7292        \u001B[35m0.8437\u001B[0m     +  3.5937\n",
      "     16        \u001B[36m0.0709\u001B[0m       0.7431        1.3703        3.6303\n",
      "     17        \u001B[36m0.0412\u001B[0m       0.7569        1.1910        3.6190\n",
      "     18        \u001B[36m0.0353\u001B[0m       \u001B[32m0.7847\u001B[0m        1.2220        3.6018\n",
      "     19        0.0548       0.7153        1.6381        3.5949\n",
      "     20        \u001B[36m0.0337\u001B[0m       0.7083        1.4585        3.5936\n",
      "     21        \u001B[36m0.0113\u001B[0m       0.7500        1.5574        3.5984\n",
      "     22        \u001B[36m0.0043\u001B[0m       0.7500        1.4727        3.5996\n",
      "     23        0.0723       0.6667        2.4087        3.5953\n",
      "     24        0.1114       0.6597        1.7957        3.5954\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.694 total time= 1.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.1707\u001B[0m       \u001B[32m0.2759\u001B[0m        \u001B[35m1.6025\u001B[0m     +  3.6020\n",
      "      2        \u001B[36m1.6951\u001B[0m       0.2483        \u001B[35m1.5660\u001B[0m     +  3.6327\n",
      "      3        \u001B[36m1.6510\u001B[0m       \u001B[32m0.3448\u001B[0m        \u001B[35m1.5283\u001B[0m     +  3.6383\n",
      "      4        \u001B[36m1.5140\u001B[0m       \u001B[32m0.4069\u001B[0m        \u001B[35m1.2945\u001B[0m     +  3.6315\n",
      "      5        \u001B[36m1.3101\u001B[0m       0.3586        2.2186        3.6303\n",
      "      6        \u001B[36m1.1909\u001B[0m       \u001B[32m0.4966\u001B[0m        \u001B[35m1.1344\u001B[0m     +  3.5972\n",
      "      7        \u001B[36m0.9771\u001B[0m       \u001B[32m0.5655\u001B[0m        1.6110        3.6149\n",
      "      8        \u001B[36m0.8761\u001B[0m       \u001B[32m0.6759\u001B[0m        1.1370        3.5958\n",
      "      9        \u001B[36m0.5762\u001B[0m       \u001B[32m0.7103\u001B[0m        \u001B[35m1.0785\u001B[0m     +  3.5989\n",
      "     10        \u001B[36m0.3938\u001B[0m       \u001B[32m0.7379\u001B[0m        \u001B[35m0.8299\u001B[0m     +  3.6203\n",
      "     11        \u001B[36m0.3570\u001B[0m       0.6138        1.2780        3.6177\n",
      "     12        \u001B[36m0.3274\u001B[0m       0.7034        1.0255        3.5990\n",
      "     13        \u001B[36m0.2326\u001B[0m       \u001B[32m0.7448\u001B[0m        0.9066        3.6041\n",
      "     14        \u001B[36m0.1386\u001B[0m       0.6966        0.9909        3.6013\n",
      "     15        0.2865       0.7034        1.3635        3.6010\n",
      "     16        0.4238       0.6690        1.1996        3.6019\n",
      "     17        0.1723       \u001B[32m0.7724\u001B[0m        0.9247        3.6011\n",
      "     18        \u001B[36m0.0580\u001B[0m       \u001B[32m0.7862\u001B[0m        1.0561        3.6010\n",
      "     19        \u001B[36m0.0218\u001B[0m       0.7862        0.9591        3.6031\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-0.910 total time= 1.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.2967\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.5923\u001B[0m     +  3.5997\n",
      "      2        \u001B[36m1.6808\u001B[0m       \u001B[32m0.2552\u001B[0m        \u001B[35m1.5911\u001B[0m     +  3.6327\n",
      "      3        \u001B[36m1.6147\u001B[0m       0.2552        \u001B[35m1.5428\u001B[0m     +  3.6297\n",
      "      4        \u001B[36m1.5550\u001B[0m       0.2552        1.9672        3.6384\n",
      "      5        \u001B[36m1.3462\u001B[0m       \u001B[32m0.3310\u001B[0m        3.0118        3.6056\n",
      "      6        \u001B[36m1.0462\u001B[0m       \u001B[32m0.3793\u001B[0m        3.4603        3.6009\n",
      "      7        \u001B[36m0.9257\u001B[0m       \u001B[32m0.6138\u001B[0m        \u001B[35m1.0206\u001B[0m     +  3.6008\n",
      "      8        \u001B[36m0.7301\u001B[0m       \u001B[32m0.7793\u001B[0m        \u001B[35m0.8257\u001B[0m     +  3.6308\n",
      "      9        \u001B[36m0.4930\u001B[0m       \u001B[32m0.7931\u001B[0m        0.8266        3.6255\n",
      "     10        \u001B[36m0.3458\u001B[0m       0.4897        4.6459        3.6114\n",
      "     11        \u001B[36m0.2141\u001B[0m       \u001B[32m0.8000\u001B[0m        1.4641        3.6014\n",
      "     12        0.2834       0.7448        1.2141        3.6043\n",
      "     13        0.2674       \u001B[32m0.8138\u001B[0m        \u001B[35m0.7882\u001B[0m     +  3.6028\n",
      "     14        \u001B[36m0.1646\u001B[0m       0.7655        1.8410        3.6284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15        \u001B[36m0.0875\u001B[0m       \u001B[32m0.8483\u001B[0m        1.1433        3.6010\n",
      "     16        0.0955       0.7931        1.3956        3.6012\n",
      "     17        0.1450       0.6966        1.6022        3.6080\n",
      "     18        0.1486       0.8000        1.1302        3.6058\n",
      "     19        0.1260       0.6552        2.6058        3.6017\n",
      "     20        0.1891       0.7517        1.8925        3.6151\n",
      "     21        \u001B[36m0.0571\u001B[0m       0.7793        1.6193        3.6253\n",
      "     22        \u001B[36m0.0278\u001B[0m       0.8207        1.2396        3.6097\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=2;, score=-1.250 total time= 1.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.0167\u001B[0m       \u001B[32m0.2292\u001B[0m        \u001B[35m1.6165\u001B[0m     +  3.6062\n",
      "      2        \u001B[36m1.6646\u001B[0m       \u001B[32m0.2708\u001B[0m        \u001B[35m1.5875\u001B[0m     +  3.6221\n",
      "      3        \u001B[36m1.6282\u001B[0m       0.2569        \u001B[35m1.5824\u001B[0m     +  3.6290\n",
      "      4        \u001B[36m1.6079\u001B[0m       0.2431        \u001B[35m1.5739\u001B[0m     +  3.6270\n",
      "      5        \u001B[36m1.5930\u001B[0m       0.2500        1.5987        3.6271\n",
      "      6        \u001B[36m1.5536\u001B[0m       0.2431        3.5941        3.6015\n",
      "      7        \u001B[36m1.5163\u001B[0m       0.2500        2.4920        3.5989\n",
      "      8        \u001B[36m1.4589\u001B[0m       \u001B[32m0.3056\u001B[0m        \u001B[35m1.4678\u001B[0m     +  3.5979\n",
      "      9        \u001B[36m1.4572\u001B[0m       \u001B[32m0.3125\u001B[0m        1.8617        3.6160\n",
      "     10        \u001B[36m1.3817\u001B[0m       0.2639        2.5385        3.5956\n",
      "     11        \u001B[36m1.3178\u001B[0m       \u001B[32m0.3472\u001B[0m        \u001B[35m1.4451\u001B[0m     +  3.5947\n",
      "     12        1.3327       0.2986        3.1216        3.6104\n",
      "     13        \u001B[36m1.2965\u001B[0m       0.3403        \u001B[35m1.4308\u001B[0m     +  3.6004\n",
      "     14        \u001B[36m1.2704\u001B[0m       \u001B[32m0.3681\u001B[0m        1.5153        3.6289\n",
      "     15        \u001B[36m1.1629\u001B[0m       0.3333        1.4886        3.6215\n",
      "     16        1.2167       \u001B[32m0.4167\u001B[0m        \u001B[35m1.3375\u001B[0m     +  3.6239\n",
      "     17        \u001B[36m1.0977\u001B[0m       \u001B[32m0.4375\u001B[0m        1.4873        3.6337\n",
      "     18        \u001B[36m1.0788\u001B[0m       \u001B[32m0.4861\u001B[0m        2.1758        3.6224\n",
      "     19        \u001B[36m1.0321\u001B[0m       \u001B[32m0.5208\u001B[0m        \u001B[35m1.1612\u001B[0m     +  3.6039\n",
      "     20        \u001B[36m0.9805\u001B[0m       0.4861        1.3766        3.6199\n",
      "     21        0.9824       0.5208        1.3300        3.5986\n",
      "     22        \u001B[36m0.9261\u001B[0m       0.5000        1.6339        3.5963\n",
      "     23        \u001B[36m0.8255\u001B[0m       \u001B[32m0.5417\u001B[0m        1.4857        3.5959\n",
      "     24        \u001B[36m0.7776\u001B[0m       0.5278        1.3770        3.6014\n",
      "     25        0.8174       0.5139        1.1700        3.6070\n",
      "     26        0.7792       \u001B[32m0.5903\u001B[0m        \u001B[35m1.1256\u001B[0m     +  3.5999\n",
      "     27        \u001B[36m0.6396\u001B[0m       \u001B[32m0.6111\u001B[0m        1.4299        3.6247\n",
      "     28        \u001B[36m0.5994\u001B[0m       \u001B[32m0.6458\u001B[0m        1.2360        3.5994\n",
      "     29        \u001B[36m0.5221\u001B[0m       \u001B[32m0.6667\u001B[0m        1.2033        3.6113\n",
      "     30        0.5413       0.5208        2.6282        3.5990\n",
      "     31        0.8030       0.5208        1.2561        3.5974\n",
      "     32        0.5299       0.6597        1.6907        3.6011\n",
      "     33        \u001B[36m0.4770\u001B[0m       0.6389        1.4606        3.6245\n",
      "     34        \u001B[36m0.3460\u001B[0m       0.6458        1.6174        3.6136\n",
      "     35        \u001B[36m0.2342\u001B[0m       \u001B[32m0.7222\u001B[0m        1.6728        3.6049\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-2.197 total time= 2.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9423\u001B[0m       \u001B[32m0.1172\u001B[0m        \u001B[35m1.6081\u001B[0m     +  3.6206\n",
      "      2        \u001B[36m1.6530\u001B[0m       \u001B[32m0.2552\u001B[0m        \u001B[35m1.5965\u001B[0m     +  3.6428\n",
      "      3        \u001B[36m1.6133\u001B[0m       0.2069        1.6018        3.6314\n",
      "      4        1.6297       0.2000        1.6035        3.6068\n",
      "      5        \u001B[36m1.6017\u001B[0m       0.2069        \u001B[35m1.5825\u001B[0m     +  3.6074\n",
      "      6        \u001B[36m1.5824\u001B[0m       0.2483        \u001B[35m1.5443\u001B[0m     +  3.6359\n",
      "      7        \u001B[36m1.5612\u001B[0m       \u001B[32m0.2828\u001B[0m        \u001B[35m1.5036\u001B[0m     +  3.6401\n",
      "      8        \u001B[36m1.4756\u001B[0m       0.2552        1.6088        3.6361\n",
      "      9        \u001B[36m1.4606\u001B[0m       0.2828        1.5517        3.6142\n",
      "     10        \u001B[36m1.4075\u001B[0m       \u001B[32m0.2966\u001B[0m        \u001B[35m1.4738\u001B[0m     +  3.6075\n",
      "     11        \u001B[36m1.3437\u001B[0m       \u001B[32m0.3379\u001B[0m        \u001B[35m1.3891\u001B[0m     +  3.6262\n",
      "     12        \u001B[36m1.3019\u001B[0m       0.3310        1.3919        3.6373\n",
      "     13        \u001B[36m1.2359\u001B[0m       \u001B[32m0.3448\u001B[0m        1.6471        3.6143\n",
      "     14        \u001B[36m1.2192\u001B[0m       \u001B[32m0.3517\u001B[0m        \u001B[35m1.3643\u001B[0m     +  3.6060\n",
      "     15        \u001B[36m1.1837\u001B[0m       0.3517        1.8900        3.6260\n",
      "     16        1.1877       \u001B[32m0.3586\u001B[0m        1.5103        3.6139\n",
      "     17        1.2040       0.3241        3.0918        3.6139\n",
      "     18        \u001B[36m1.0800\u001B[0m       \u001B[32m0.4207\u001B[0m        1.8194        3.6087\n",
      "     19        \u001B[36m1.0171\u001B[0m       \u001B[32m0.4414\u001B[0m        \u001B[35m1.3591\u001B[0m     +  3.6083\n",
      "     20        \u001B[36m0.8768\u001B[0m       \u001B[32m0.4690\u001B[0m        1.6236        3.6406\n",
      "     21        \u001B[36m0.7592\u001B[0m       0.3448        3.1675        3.5954\n",
      "     22        0.8594       0.4483        5.1069        3.6112\n",
      "     23        0.8396       \u001B[32m0.5241\u001B[0m        \u001B[35m1.2076\u001B[0m     +  3.6087\n",
      "     24        \u001B[36m0.7334\u001B[0m       \u001B[32m0.5862\u001B[0m        2.3611        3.6398\n",
      "     25        \u001B[36m0.5979\u001B[0m       \u001B[32m0.6483\u001B[0m        1.4805        3.6087\n",
      "     26        \u001B[36m0.5461\u001B[0m       \u001B[32m0.6621\u001B[0m        1.3603        3.6133\n",
      "     27        0.5621       0.6138        1.9180        3.6059\n",
      "     28        \u001B[36m0.5433\u001B[0m       0.6552        \u001B[35m1.1493\u001B[0m     +  3.6056\n",
      "     29        \u001B[36m0.5062\u001B[0m       0.6552        1.3535        3.6281\n",
      "     30        \u001B[36m0.4695\u001B[0m       0.6414        1.2383        3.6051\n",
      "     31        \u001B[36m0.3582\u001B[0m       \u001B[32m0.7448\u001B[0m        1.4918        3.6090\n",
      "     32        0.3954       0.4897        4.2942        3.6058\n",
      "     33        0.4865       0.6483        2.1937        3.6104\n",
      "     34        0.5499       0.6345        1.3470        3.6142\n",
      "     35        \u001B[36m0.3152\u001B[0m       0.6621        1.6227        3.6106\n",
      "     36        \u001B[36m0.2444\u001B[0m       0.6759        1.5737        3.6105\n",
      "     37        \u001B[36m0.2172\u001B[0m       0.3517       13.6741        3.6332\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.886 total time= 2.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m1.9441\u001B[0m       \u001B[32m0.2483\u001B[0m        \u001B[35m1.6017\u001B[0m     +  3.6098\n",
      "      2        \u001B[36m1.6587\u001B[0m       0.2483        \u001B[35m1.5877\u001B[0m     +  3.6230\n",
      "      3        \u001B[36m1.6041\u001B[0m       0.2483        1.5906        3.6304\n",
      "      4        1.6102       0.2483        \u001B[35m1.5764\u001B[0m     +  3.6144\n",
      "      5        \u001B[36m1.5799\u001B[0m       0.2483        \u001B[35m1.4841\u001B[0m     +  3.6382\n",
      "      6        \u001B[36m1.5480\u001B[0m       0.2483        1.5669        3.6499\n",
      "      7        \u001B[36m1.4587\u001B[0m       \u001B[32m0.2552\u001B[0m        1.6046        3.6115\n",
      "      8        \u001B[36m1.4443\u001B[0m       \u001B[32m0.2897\u001B[0m        1.5652        3.6135\n",
      "      9        \u001B[36m1.3807\u001B[0m       \u001B[32m0.3448\u001B[0m        1.6541        3.6078\n",
      "     10        \u001B[36m1.3578\u001B[0m       0.2483        4.6086        3.6100\n",
      "     11        \u001B[36m1.3342\u001B[0m       \u001B[32m0.4138\u001B[0m        \u001B[35m1.3334\u001B[0m     +  3.6137\n",
      "     12        \u001B[36m1.2500\u001B[0m       \u001B[32m0.4690\u001B[0m        \u001B[35m1.3004\u001B[0m     +  3.6354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        \u001B[36m1.1919\u001B[0m       0.4000        1.4400        3.6401\n",
      "     14        \u001B[36m1.0939\u001B[0m       0.4000        1.3568        3.6201\n",
      "     15        1.1972       0.4621        1.3050        3.6150\n",
      "     16        1.0985       0.4690        1.4331        3.6083\n",
      "     17        \u001B[36m0.9506\u001B[0m       0.4483        2.7215        3.6088\n",
      "     18        \u001B[36m0.8793\u001B[0m       \u001B[32m0.5379\u001B[0m        \u001B[35m1.0790\u001B[0m     +  3.6110\n",
      "     19        \u001B[36m0.8147\u001B[0m       0.4483        2.5034        3.6384\n",
      "     20        0.8275       0.4207        5.7843        3.6319\n",
      "     21        \u001B[36m0.8067\u001B[0m       0.3103        4.7912        3.6179\n",
      "     22        \u001B[36m0.7411\u001B[0m       \u001B[32m0.6345\u001B[0m        \u001B[35m0.9497\u001B[0m     +  3.6077\n",
      "     23        \u001B[36m0.6494\u001B[0m       0.6345        1.2474        3.6392\n",
      "     24        \u001B[36m0.5374\u001B[0m       \u001B[32m0.7379\u001B[0m        1.0903        3.6115\n",
      "     25        \u001B[36m0.4570\u001B[0m       0.7034        1.2343        3.6132\n",
      "     26        0.4824       0.7172        1.0446        3.6129\n",
      "     27        0.5304       0.5103        2.6638        3.6161\n",
      "     28        0.6050       0.5862        1.2022        3.6085\n",
      "     29        0.5179       0.7034        1.5495        3.6083\n",
      "     30        \u001B[36m0.3786\u001B[0m       0.6138        3.4585        3.6322\n",
      "     31        0.4349       0.7172        \u001B[35m0.9451\u001B[0m     +  3.6240\n",
      "     32        \u001B[36m0.3386\u001B[0m       0.6552        1.1097        3.6323\n",
      "     33        \u001B[36m0.2978\u001B[0m       0.7241        1.1318        3.6149\n",
      "     34        \u001B[36m0.2630\u001B[0m       0.7379        1.2203        3.6255\n",
      "     35        \u001B[36m0.2591\u001B[0m       0.7241        1.5070        3.6250\n",
      "     36        \u001B[36m0.2006\u001B[0m       \u001B[32m0.8000\u001B[0m        1.3244        3.6204\n",
      "     37        \u001B[36m0.1563\u001B[0m       0.7241        1.2926        3.6071\n",
      "     38        0.2029       0.7517        1.2077        3.6100\n",
      "     39        \u001B[36m0.1463\u001B[0m       0.7517        1.2203        3.6129\n",
      "     40        \u001B[36m0.1224\u001B[0m       0.7586        1.2463        3.6084\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__freeze_layers=False, module__hidden_features=256, module__num_hidden_layers=3;, score=-1.404 total time= 2.9min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001B[36m2.2202\u001B[0m       \u001B[32m0.3226\u001B[0m        \u001B[35m1.5031\u001B[0m     +  4.7373\n",
      "      2        \u001B[36m1.4779\u001B[0m       \u001B[32m0.4977\u001B[0m        \u001B[35m1.1920\u001B[0m     +  4.7499\n",
      "      3        \u001B[36m1.0948\u001B[0m       \u001B[32m0.7558\u001B[0m        \u001B[35m0.8390\u001B[0m     +  4.7386\n",
      "      4        \u001B[36m0.7280\u001B[0m       0.5115        2.0115        4.7630\n",
      "      5        \u001B[36m0.5766\u001B[0m       \u001B[32m0.8295\u001B[0m        \u001B[35m0.5601\u001B[0m     +  4.7344\n",
      "      6        \u001B[36m0.3307\u001B[0m       \u001B[32m0.8387\u001B[0m        0.5997        4.7490\n",
      "      7        \u001B[36m0.2120\u001B[0m       0.6959        1.5492        4.7215\n",
      "      8        \u001B[36m0.1762\u001B[0m       0.7143        1.2578        4.7176\n",
      "      9        \u001B[36m0.1541\u001B[0m       0.7097        1.2582        4.7193\n",
      "     10        0.1885       0.7512        0.9439        4.7170\n",
      "     11        \u001B[36m0.1466\u001B[0m       \u001B[32m0.8618\u001B[0m        0.6616        4.7175\n",
      "     12        \u001B[36m0.1010\u001B[0m       0.8618        0.7760        4.7171\n",
      "     13        \u001B[36m0.0870\u001B[0m       0.8203        0.8525        4.7189\n",
      "     14        \u001B[36m0.0751\u001B[0m       0.7880        1.0219        4.7232\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<function generate_model at 0x7f8e8eceb8b0>,\n",
       "  module__dropout=0.25,\n",
       "  module__hidden_features=128,\n",
       "  module__opt={},\n",
       "),\n",
       "             param_grid={'module__dropout': [0.5],\n",
       "                         'module__freeze_layers': [True, False],\n",
       "                         'module__hidden_features': [64, 128, 256],\n",
       "                         'module__num_hidden_layers': [1, 2, 3]},\n",
       "             scoring='neg_log_loss', verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Uncomment this for tuning hyperparameter and saving the best model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Hyper parameter tuning with cross validation\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=15,\n",
    "    lr=0.001,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    "    optimizer=torch.optim.Adam\n",
    ")\n",
    "\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False,\n",
    "               # verbose=0\n",
    "               )\n",
    "# net.fit(inp_imgs[:20], torch.as_tensor(target[:20]))  # Make sure the model works on a small set of data\n",
    "params = {\n",
    "    'lr': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# params = {\n",
    "#     'optimizer': [torch.optim.SGD, torch.optim.Adam]\n",
    "# }\n",
    "gs = GridSearchCV(net, params,\n",
    "                  # refit=False,\n",
    "                  cv=3,\n",
    "                  scoring='neg_log_loss',\n",
    "                  verbose=3, error_score='raise')\n",
    "\n",
    "gs.fit(inp_imgs, torch.as_tensor(target))\n",
    "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "# Save best model to pickel file\n",
    "with open('yolox.pkl', 'wb') as f:\n",
    "    pickle.dump(gs, f)\n",
    "    \n",
    "    \n",
    "'''\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "params={'module__hidden_features': [64, 128, 256],\n",
    "        'module__dropout': [0.5],\n",
    "        'module__num_hidden_layers': [1, 2, 3],\n",
    "        'module__freeze_layers': [True, False]}\n",
    "# params={'module__hidden_features': [256],\n",
    "#         'module__dropout': [0.5],\n",
    "#         'module__num_hidden_layers': [3],\n",
    "#         'module__freeze_layers': [False]}\n",
    "net = NeuralNetClassifier(\n",
    "    # model,\n",
    "    module=generate_model,\n",
    "    module__opt=opt,\n",
    "    module__hidden_features=128,\n",
    "    module__dropout=0.25,\n",
    "    max_epochs=100,\n",
    "    lr=3e-4,\n",
    "    device=opt.device,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=16,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[('checkpoint', Checkpoint()),\n",
    "               ('early_stopping', EarlyStopping(patience=10))]\n",
    ")\n",
    "# net.fit(inp_imgs, torch.as_tensor(target))\n",
    "\n",
    "gs = GridSearchCV(net, params,\n",
    "                  # refit=False,\n",
    "                  cv=3,\n",
    "                  scoring='neg_log_loss',\n",
    "                  verbose=3, error_score='raise')\n",
    "\n",
    "gs.fit(inp_imgs, torch.as_tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yolox.pkl', 'wb') as f:\n",
    "    pickle.dump(gs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from pickel file\n",
    "with open('yolox.pkl', 'rb') as f:\n",
    "    gs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module__dropout': 0.5,\n",
       " 'module__freeze_layers': False,\n",
       " 'module__hidden_features': 128,\n",
       " 'module__num_hidden_layers': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 4, 0, 0, 0, 0, 0, 3, 3, 2, 2, 0, 0, 4, 0, 3, 2, 2, 2]),\n",
       " array([0, 2, 4, 2, 0, 0, 4, 0, 4, 3, 2, 2, 4, 0, 4, 0, 3, 2, 4, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(inp_imgs[:20]), target[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate both models on a withheld test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 images that the baseline model could not classify\n"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "imgs = []\n",
    "for subdir, dirs, files in os.walk(test_path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image        \n",
    "        results = pose.process(imgRGB)\n",
    "        \n",
    "        imgs.append(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(baseline_model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])\n",
    "        else:\n",
    "            y_pred.append(-1)\n",
    "            y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])\n",
    "print(f'There are {len([1 for y in y_pred if y == -1])} images that the baseline model could not classify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8 s Â± 585 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y_pred = []\n",
    "for img in imgs:\n",
    "    temp=[]\n",
    "    blackie = np.zeros(img.shape) # Blank image\n",
    "    results = pose.process(img)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "            mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            for i,j in zip(points,landmarks):\n",
    "                    temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "            y_pred.append(baseline_model.predict([temp]))\n",
    "    else:\n",
    "        y_pred.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['downdog', 'goddess', 'plank', 'tree', 'warrior2'], dtype='<U8')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['downdog', 'goddess', 'plank', 'tree', 'warrior2', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "# We add an \"unknown\" class to mark those that the baseline couldn't make a prediction period.\n",
    "classes = labelencoder.classes_.tolist()+['Unknown']\n",
    "print(classes)\n",
    "baseline_report = classification_report(y_test, [int(y) if y != -1 else 5 for y in y_pred], target_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "test_path = \"dataset/test/\"\n",
    "\n",
    "# Creating Dataset\n",
    "y_test = []\n",
    "images_arrays = []\n",
    "        \n",
    "\n",
    "for subdir, dirs, files in os.walk(test_path):\n",
    "    for img in files:\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        images_arrays.append(img)\n",
    "        y_test.append(subdir.replace(path, ''))\n",
    "\n",
    "\n",
    "test_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images_arrays):\n",
    "    test_imgs[b_i] = image\n",
    "\n",
    "test_imgs = test_imgs\n",
    "y_test = labelencoder.fit_transform(y_test)\n",
    "# Predictions\n",
    "y_pred = gs.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721 ms Â± 629 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Predictions\n",
    "y_pred = gs.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the YOLOX model\n",
    "yolox_report = classification_report(y_test, y_pred, target_names=labelencoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Display results on the test set for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.92      0.88      0.90        97\n",
      "     goddess       0.79      0.57      0.67        80\n",
      "       plank       0.95      0.76      0.84       115\n",
      "        tree       0.88      0.83      0.85        69\n",
      "    warrior2       0.77      0.88      0.82       109\n",
      "     Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79       470\n",
      "   macro avg       0.72      0.65      0.68       470\n",
      "weighted avg       0.87      0.79      0.82       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.80      1.00      0.89        97\n",
      "     goddess       0.78      0.81      0.80        80\n",
      "       plank       0.82      0.86      0.84       115\n",
      "        tree       0.86      0.96      0.90        69\n",
      "    warrior2       0.90      0.56      0.69       109\n",
      "\n",
      "    accuracy                           0.83       470\n",
      "   macro avg       0.83      0.84      0.82       470\n",
      "weighted avg       0.83      0.83      0.82       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yolox_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}