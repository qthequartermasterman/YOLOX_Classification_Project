{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from yolox.data_augment import preproc\n",
    "from yolox.yolox import YOLOX, get_model, IdentityModule\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"dataset/train/\"\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataset\n",
    "target = []\n",
    "images_arrays = []\n",
    "count = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                data.loc[count] = temp\n",
    "                target.append(subdir.replace(path, ''))\n",
    "                count +=1\n",
    "\n",
    "\n",
    "data['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "data['target'] = labelencoder.fit_transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOSE_x</th>\n",
       "      <th>NOSE_y</th>\n",
       "      <th>NOSE_z</th>\n",
       "      <th>NOSE_vis</th>\n",
       "      <th>LEFT_EYE_INNER_x</th>\n",
       "      <th>LEFT_EYE_INNER_y</th>\n",
       "      <th>LEFT_EYE_INNER_z</th>\n",
       "      <th>LEFT_EYE_INNER_vis</th>\n",
       "      <th>LEFT_EYE_x</th>\n",
       "      <th>LEFT_EYE_y</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_HEEL_vis</th>\n",
       "      <th>LEFT_FOOT_INDEX_x</th>\n",
       "      <th>LEFT_FOOT_INDEX_y</th>\n",
       "      <th>LEFT_FOOT_INDEX_z</th>\n",
       "      <th>LEFT_FOOT_INDEX_vis</th>\n",
       "      <th>RIGHT_FOOT_INDEX_x</th>\n",
       "      <th>RIGHT_FOOT_INDEX_y</th>\n",
       "      <th>RIGHT_FOOT_INDEX_z</th>\n",
       "      <th>RIGHT_FOOT_INDEX_vis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385088</td>\n",
       "      <td>0.702528</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.364045</td>\n",
       "      <td>0.705285</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.361666</td>\n",
       "      <td>0.700772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525770</td>\n",
       "      <td>0.781881</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>-0.215838</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>0.165073</td>\n",
       "      <td>0.610637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470336</td>\n",
       "      <td>0.691998</td>\n",
       "      <td>-0.604218</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.705398</td>\n",
       "      <td>-0.624718</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.437711</td>\n",
       "      <td>0.699783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526152</td>\n",
       "      <td>0.778034</td>\n",
       "      <td>0.569747</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.764064</td>\n",
       "      <td>0.639336</td>\n",
       "      <td>0.467184</td>\n",
       "      <td>0.604327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.615995</td>\n",
       "      <td>-0.057232</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.440873</td>\n",
       "      <td>0.630020</td>\n",
       "      <td>-0.067001</td>\n",
       "      <td>0.988656</td>\n",
       "      <td>0.440118</td>\n",
       "      <td>0.630212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552748</td>\n",
       "      <td>0.769751</td>\n",
       "      <td>0.797690</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.937697</td>\n",
       "      <td>0.743715</td>\n",
       "      <td>0.761387</td>\n",
       "      <td>0.285678</td>\n",
       "      <td>0.625030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.401504</td>\n",
       "      <td>0.383240</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>0.378602</td>\n",
       "      <td>-0.318324</td>\n",
       "      <td>0.989338</td>\n",
       "      <td>0.404689</td>\n",
       "      <td>0.379723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>0.622063</td>\n",
       "      <td>0.713210</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>0.916048</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.763766</td>\n",
       "      <td>-0.045453</td>\n",
       "      <td>0.652138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.683425</td>\n",
       "      <td>-0.067524</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>0.446467</td>\n",
       "      <td>0.690085</td>\n",
       "      <td>-0.090036</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.426712</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598638</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.911403</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.867453</td>\n",
       "      <td>0.307724</td>\n",
       "      <td>0.658130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOSE_x    NOSE_y    NOSE_z  NOSE_vis  LEFT_EYE_INNER_x  LEFT_EYE_INNER_y  \\\n",
       "0  0.385088  0.702528 -0.004816  0.999651          0.364045          0.705285   \n",
       "1  0.470336  0.691998 -0.604218  0.982091          0.445842          0.705398   \n",
       "2  0.453251  0.615995 -0.057232  0.983838          0.440873          0.630020   \n",
       "3  0.401504  0.383240 -0.309374  0.984927          0.436412          0.378602   \n",
       "4  0.450490  0.683425 -0.067524  0.986200          0.446467          0.690085   \n",
       "\n",
       "   LEFT_EYE_INNER_z  LEFT_EYE_INNER_vis  LEFT_EYE_x  LEFT_EYE_y  ...  \\\n",
       "0         -0.031445            0.999706    0.361666    0.700772  ...   \n",
       "1         -0.624718            0.987435    0.437711    0.699783  ...   \n",
       "2         -0.067001            0.988656    0.440118    0.630212  ...   \n",
       "3         -0.318324            0.989338    0.404689    0.379723  ...   \n",
       "4         -0.090036            0.990196    0.426712    0.690337  ...   \n",
       "\n",
       "   RIGHT_HEEL_vis  LEFT_FOOT_INDEX_x  LEFT_FOOT_INDEX_y  LEFT_FOOT_INDEX_z  \\\n",
       "0        0.525770           0.781881           0.930616          -0.215838   \n",
       "1        0.526152           0.778034           0.569747           0.541724   \n",
       "2        0.552748           0.769751           0.797690           0.004431   \n",
       "3        0.591222           0.622063           0.713210          -0.035523   \n",
       "4        0.598638           0.656271           0.870245           0.077734   \n",
       "\n",
       "   LEFT_FOOT_INDEX_vis  RIGHT_FOOT_INDEX_x  RIGHT_FOOT_INDEX_y  \\\n",
       "0             0.980343            0.763475            0.904605   \n",
       "1             0.935673            0.764064            0.639336   \n",
       "2             0.937697            0.743715            0.761387   \n",
       "3             0.916048            0.569005            0.763766   \n",
       "4             0.911403            0.668724            0.867453   \n",
       "\n",
       "   RIGHT_FOOT_INDEX_z  RIGHT_FOOT_INDEX_vis  target  \n",
       "0            0.165073              0.610637       0  \n",
       "1            0.467184              0.604327       0  \n",
       "2            0.285678              0.625030       0  \n",
       "3           -0.045453              0.652138       0  \n",
       "4            0.307724              0.658130       0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying Dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-e4c8a02b64634499b85dd947e9c1f479\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e4c8a02b64634499b85dd947e9c1f479\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e4c8a02b64634499b85dd947e9c1f479\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-45b0f0e8af1628084e89ee1ec4750d29\"}, \"mark\": {\"type\": \"bar\", \"size\": 50}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"label\"}, \"tooltip\": [{\"type\": \"quantitative\", \"aggregate\": \"count\", \"title\": \"Count\"}, {\"type\": \"nominal\", \"field\": \"label\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"title\": \"Pose\"}, \"field\": \"label\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\", \"axis\": {\"title\": \"Count\"}}}, \"height\": 300, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Number of data in each pose\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-45b0f0e8af1628084e89ee1ec4750d29\": [{\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"downdog\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"tree\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"warrior2\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"goddess\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}, {\"label\": \"plank\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.DataFrame()\n",
    "label_df['label'] = list(map(lambda x: labelencoder.inverse_transform([x])[0], data['target']))\n",
    "\n",
    "bars = alt.Chart(label_df).mark_bar(size=50).encode(\n",
    "    x=alt.X('label', axis=alt.Axis(title='Pose')),\n",
    "    y=alt.Y(\"count()\", axis=alt.Axis(title='Count')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Count'), 'label'],\n",
    "    color='label'\n",
    ")\n",
    "\n",
    "(bars).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"Number of data in each pose\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Yolox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOX Configuration\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dotdict is just a dictionary whose elements can be referenced with a dot operation.\n",
    "    I.e. dotdict['x'] == dotdict.x\n",
    "\n",
    "    This is useful because the original YOLOX used a custom class to hold a lot of extra configuration that\n",
    "    we do not need.\n",
    "    \"\"\"\n",
    "    # def __getattr__(self, x):\n",
    "    #     return self[x]\n",
    "\n",
    "\n",
    "opt = dotdict()\n",
    "# All images should be scaled to this input size before passing through YOLOX.\n",
    "# Any image (of any size) can be scaled using the function `yolox.data_augment.preproc`\n",
    "# I don't recommend changing this. This is just fine and loads pretty quickly, even on CPU.\n",
    "opt.input_size = (640, 640)\n",
    "opt.random_size = (10, 20)  # None; multi-size train: from 448(14*32) to 832(26*32), set None to disable it\n",
    "opt.test_size = (640, 640)\n",
    "opt.rgb_means = [0.485, 0.456, 0.406]\n",
    "opt.std = [0.229, 0.224, 0.225]\n",
    "opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "opt.backbone = \"CSPDarknet-nano\"\n",
    "opt.depth_wise = True\n",
    "opt.use_amp = False  # True, Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack\n"
     ]
    }
   ],
   "source": [
    "from yolox.data_augment import random_perspective\n",
    "path = \"dataset/train/\"\n",
    "# Creating Dataset\n",
    "target = []\n",
    "count = 0\n",
    "num_transformations = 0\n",
    "\n",
    "images_arrays = []\n",
    "target = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "#         print(img)\n",
    "        img = cv2.imread(img)\n",
    "        \n",
    "#         print('transformations')\n",
    "        transformations = [img] + [random_perspective(img, scale=[0.5,1.2])[0] for _ in range(num_transformations)]\n",
    "#         print('imgs')\n",
    "        imgs = [preproc(imgx, opt.test_size, opt.rgb_means, opt.std)[0] for imgx in transformations]\n",
    "#         img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        label = subdir.replace(path, '')\n",
    "        \n",
    "        \n",
    "        images_arrays.extend(imgs)\n",
    "        target.extend([label]*(len(imgs)))\n",
    "\n",
    "print('stack')\n",
    "# inp_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "# for b_i, image in enumerate(images_arrays):\n",
    "#     inp_imgs[b_i] = image\n",
    "inp_imgs = np.stack(images_arrays).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 3, 640, 640)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for target\n",
    "labelencoder = LabelEncoder()\n",
    "target = labelencoder.fit_transform(target)\n",
    "\n",
    "# Shuffle them, otherwise our training gets all screwy\n",
    "inp_imgs, target = shuffle(inp_imgs, target)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAB4p0lEQVR4nOy9d5wcx3nn/a0Ok/PmvAtgkQMBgjlTlERRgZJs2ZaTbMvWnc/hfOfXZ/nuvfd8Wfb5zuGsk6yzZVOyrGAFUyIpURRzJkESOS0WwOa8O7OTZ7q73j96GtNYgiRAALvUon+fz+x0V1d31/T286snVZWQUuLBg4crF8pKN8CDBw8rC48EPHi4wuGRgAcPVzg8EvDg4QqHRwIePFzh8EjAg4crHJeFBIQQdwshjgkhTgghPn057uHBg4dLA3Gp8wSEECpwHHg3MAq8DHxcSnn4kt7IgwcPlwSXQxO4FjghpTwppawAXwPuvQz38eDBwyWAdhmu2QGMuPZHgeve7AQhNAm+y9AUDx481FGclVI2LS29HCRwXhBCfAr4lL2nA/0r1RQPHq4Q7B86V+nlMAfGgC7Xfmet7CxIKb8gpdwtpdy9glzkwcMVj8tBAi8D/UKIPiGED/gZ4LuX4T4ePHi4BLjkXbCU0hBC/CbwMKACX5RSHrrU9/HgwcOlwWXRw6WUDwEPXY5re/Dg4dLCyxj04OEKh0cCHjxc4fBIwIOHKxweCXjwcIXDIwEPHq5weCTgwcMVDo8EPHi4wuGRgAcPVzg8EvDg4QqHRwIePFzh8EjAg4crHB4JePBwhcMjAQ8ernB4JODBwxUOjwQ8eLjC4ZGABw9XODwS8ODhCodHAh48XOG4cqb5FQIUBSQgJUhrpVvkwcM7AlcMCYi770H/Nz+DOgHqzCzK+BGsShljEayRBcTwYZAG0gJjtoiVmcVmDMCS9W0PHlYZrhgS0NY3Er1tOz5hr3Xk4040CVjgM02EUUUHAhZUR7PI2TEUaSFNWNg7gXXqGGBSNS3SA4uYM1PI7CymaVDJS6xCEVnIgDTtG8ozfzx4eEfjiiEBnwYR7B+s1r59AgIqaKqK9Kn4gCCgbgyg04SC7TTpvR0U3o8AFCmxqhaKYaJIk6opKcyDmkmjLExStkzyi7A4OIN56gAVy6BiwvyRWSojx0BKLAmFuRxWPruklR5peFh+XDEkoAnwC/BjL3qm1j7h2r7m+ihLPhogsEVUFQLdpyJ86pnjTTFQCeGjHVG7nwAU7qm5ICRGyURUy6hAxZLMD05jzE2iAgUDRvZPUZ0Yg8ICecNkbsygPDOJnBvDsEyqVajmishCFo8sPFxKXDkkgMAPxIBArczRCALUBV3FFmxBnQTcS6Xq1LQFbFF0zsF1rqSucQCYQqAENfSghuNdaL46gmDNmevuvsf+loApwaxYyGoVYVQoSsjnJPmJecyZUQrSolCFmVdHqAwdwURSNGD64CjGzCgAhoRiugiVUs0J6hGHh3PjCiEBgS/cQAhbE/BjC6/P9e0WZEeYA0uOSerEoXI2UTiCL1zXceKvboJhyXFHw3CuLwEpAL+C8PuR+IkCJIGuGJLeM9eQH6mfY1mSSr6MVS0DkKtKpo5PYWXmkUaZhbLF1LEMlakx5MI4ecNicU5SHB/HWhjHlBaWCdVsESrFC3/EHn5s8ZYkIIT4IvABYFpKubVWlgK+DvQCp4GfklIuCCEE8OfAPUAB+CUp5auXp+kXACHwd/YTp24OCOrqv7Otcrap4KcurM5HUCcOq1bmCLdSOx/X/pkmLCl377tJxo2lMQkFMJdcu+bbxFIE/mgASQCJrfG0tCTOuobzbWFrG5WKpFQsYVZLFCWUipL0wDjG/DhVoFCUTD53GGNuFAnki5L0wVPIfBqwqJpgFipgVMGq4uHHE+ejCfwd8JfAl1xlnwYelVJ+Rgjx6dr+7wPvw15jvB+4Dvhc7XtlIQSh1jB+bOHVqQuv4wPQascC1IVUXfLtkIfbTwB1gRRLypcedzQJd313toLuKluqHbjPdxODpK6JsOQc559rucqdb0uA3y8I+YNYBM+ca/WkqLDVboME4xffA9jkUzKhmslhGVUqSObzkszJWYzFOWRpkcWcJDNcwRgdwFwYo1yVlNMSY3QEmZtDIrEMkPmiRxrvILwlCUgpnxJC9C4pvhe4vbZ9H/AENgncC3xJSimBF4QQCSFEm5Ry4pK1+O1AQDBUt/0dQfVjOwbdmoBzzBF6qJsNav1yZ7QFeH3Pfq59R3gdAnHgCKzg9ULrruOuu/Tcs4T7HMdZctxybetL6pq1dpiAKcCHoFqrp2tgNkQBqAIhoG1NI2XnelJi1q5rAlULSmUw8zkq1SJlCaW0pHR4ADM/RxlIz5iUX9iDVcwgAWPWRA6Nwew4WAYeLj/erk+gxSXYk0BLbbsDGHHVG62VrSwJAKFQPQrgwxbGIHX/gEMOcLYNr1InATjbJFhq8+Pad/fOCq/XBpbC7ZR8Iy3gjUiAc3yfa9sR8sobHDepE5ZZKzNcbbewhd+i/mws7GcosR2gDiFUAVUFPQQyFMUkSgUwO8DY0oIEikBESsp8BKO2Xy5KikdmKd/zz2F68BxPysOlxkU7BqWUUghxwa5nIcSngE/Ze/qb1r1YCGyB16k7Bt2hQncUwDEVoC4Qbi3ALdDaOcqXRgwsziYYizqJOE5B5xy3D0Gh7nNw8GYCv9T/f65951vlbI3BqVt1tUdzfbuvVXHVc9rvwyYLif1cLWyHkHPcoE6MDqmYtfNiNeIo1K6vhQRWXKNslPGwPHi7JDDlqPlCiDZgulY+BnS56nXWyl4HKeUXgC8ACBG6vPErRSOk+ohh/2Dd9e30aO48AXi9X+BceQNu777bPHDOd+A+3x0NWCr0cHbv7hAF1Htnd903U/2Xlp+JIlAXbPf5BmeTkkMKbs3EISWHIBRXXb12DbP2CdXKzNp1DNe5qut+zrN0/zajlGWxmDnHr/FwOfB2SeC7wCeAz9S+73eV/6YQ4mvYDsHMivsDAIIxAuEkQeovoaPmuwnALdhu4Xfq65wdEZCu+u7z3Viq+rudfw7c93UEcamAu0nETSbniig4Ljd3NMHpzZeq+859nN/sNhvc14S64DpaipvQqq7n4BCN87zcRFDg7GfqaBVOZKUMVIwSmCU8LA/OJ0T4VWwnYKMQYhT4D9jC/w0hxCeBIeCnatUfwg4PnsD+f//yZWjzBUMLRwhFY2c0ACf857b93dvuMKDbVtddx5b24u6IgFuglmoIUFfH3b0/rvpLcwfO/I4l+26NAc4WaHdb3FEFd6/uqOru46JWZtR+r3NOxVUP6g5E6Sr3Ywt1mfozcojG6f0dUqi4no2bbHXA0wGWF+cTHfj4Gxx61znqSuA3LrZRlxqKruD3KejUe38/9RdvqYPPx+tJAF6vASzt9R2SkK5rwdnCLTg7Y9ERkqVmwFI1/o0cg9Sup2I71izX9S1swau69p3zHAefu43O97kiGI4QO9epuM5xNAQHzrYj7G4zwok8lGof53pOHQkohSU/1MNlxRWRMegLQShaTxF2BNjdw7u/l6YLu1VxONtZ6O7N3T4DnbMFzC0k/iXXdeqcy1fgwB3WW0oITjscYXIiHlBXt93Xcavy7mMmZwu02zRwIirOvtvp59YkoK5BONqEI/RuU8t9fafMIaugRwLLiiuCBAIKxJV6RMAt6G4T4VxmgfvFddu7jqA6ROEWYCcK4XbsOcfcZW4hdvaXkoKTr+AIn+NgWxo1kK56S6+tcbYqfi4yWeoHwHU/97NxX8PRKBzhdaf/OBEBp9xPPQTpdhLq2NqCST1CIBZNjwSWEVcECShCIYA4I7COU9AhALf669YQHOHXXcfc4UOB7QV32/Fu4oDXaw/welXbfe5S7WHpdd2miRtLZcaJ6TshPLd97z7HIQQnycddlnXdZykhLNVcnDa6ow5LtRVHM3CE3v0bnGMWYI2eBmspJXm4XLgiSEAE4+hCnNECHBtap64ZuJ19Tq/n9OiOjb3Ukbc0XOgOp7lNBafuUqGG12sLzvXdgr/02BvVPes3u9qzVDtwhN4pWyqQbmJwiKTsqrPUh+Go/qrreo7z0zE9HJvf/e1AxSaGM0lb8yN4078tH64IElBiSTShnMkWdCID7kQhR2iWzjfgaAtuYXUL9NKwoYOlQunWDtxq/7lwPsL/RvdZiqX5AM613BEId/KQu36YetzfR50cDDiTGegWcIcY3L16mbq9vzRZSLrKHJiA8AhgWXFFkIAaDKMLQYCzJxBxOwjdEQG3iu8WeHcYzW0iOA9xKUnA68N4S0cMLk0gWlr/jQjhXD380m33+UvruMnDUfWX1nf7Dvyuc93e/Sp1TcKp72Bp7+/kEpwra9J5nku1Eg+XH1cECaR6UwRU2yewdPiwAiSov4RO+VIb3p0QBOcW0qXHnJ7O7Tl3E8q56ru/HSwd2LS0rpsQ3AOE4GzV3el93cInXb/XfV8ntHeuSIFDlnB29qE7P8A6x7a/dl0n9didWu08D08HWH5cESTQGlMIKuKs+QIU6j2+kzPgdvotdQyeyxyQS+q7tYalvfu5cgrO9fDdwrk0R+GNHHssKVv67QiXSV2Nd7A09OjAUf8ldXvdbUY42pObZBytoArkXG02l5zvPEfHbBDUtQRTguGMcPKwLLgiSCBEXQNwhNrH2ROPwutDgg5ZLE0RdjsC3XB74Zc62s7l9V9qGjjaBq79t8IbkcIbqeZuNX9p6vLSazrfZ0YJcnYa8tJcBMdkcHp6hxCcZ+n292vYhOROQFIAzbTIDC6+yS/2cKlxRZCAIlQ07Nl23JrA0klF3cLpqOBwtgZArdzJA3Db22LJZykRuFOIl/aKbxfiDbaXwnHMuYlgqQbgCLrj6XdrCRZnE5/bvMBVz8R+xgnq6cdOuLJE/Te7NQjdvW+aGCcPXsgj8HCRuAJIQKCr+pm5AxzhdUcG3NtuYoB6eNA5x+kB3QThqLpL5xVYOmbg7FYtL9yDtR3BdTsEnXKLuppf5mzb3iEy6SpfOmuR2wHqHltQXXKeYwo4mZl1cpCIqjd4aDlxBZAABFSdBK9P93UIwJ00tDRnwO1IdOD2/p/LWXihcPsPlpafC/I86iytd67rLnUIOuE/d+jOnRvgJBQ513Db+e5IgNtXAnWfQJR69qLjIHSu5RCP4xvwsHxY/SSgKMTWrj9L8N29vztkGOBsvwC83ou/VFN4OzjXS34hL/751j2XnX+uOudyeDq9vjt/wO0PKFHXItzhQfeEI+7hy050wBlLUODs2YscDc2bUGz5sepJQFEEfesTrxN8J2nIrR24R/qdcRZKiZBgCdAkaMqF9VPvlF7tfLQKONuEcff+bgepI7hOz+/ODHQ0CNN13K0xOPd0MjHd8we5tQ4Py4dVTwICaFbOnjzEEX63f8Cp67zITi9oTuYJjsxQWtsM82WK65LoQrxheO/HDW+mLSzVeJzhxxI74iKXlDnmhDtl2B1edN9TYDsPs9QHDllAOS/JFzwaWE5cESTgTgF2phV3ogTw+hwBt1MrG9Mxhw8RnD2I2dCBmtUgFjvr+u8UnCtb8EJwrvrua+pLypyIA9S1Bh/1ECHU5xx0yMBtVgjs/4U78lAdm6QwMXOBLfdwMVj1JAD1MQHuKceXThSCq9xtJ/tDPqwbdyM//69RqhbK/D1U7v4F7HVW3nkPUMpa28XrPfZvB0szFN1lzgAsJx9g6XluDcEZL7A0D8hZ0s0ZkqyVc1DOX0SLPVworog0badnP9dswu5pyM+VmqsIgd7WgvXrf4xZEWTv/xv0fPZMqus7BRJASoqHx5G5KmRLkDNByjd0Cl4IzpXT4JQ5JpajXTnrNTqfAPWELfd8DudyzvrwsNxY9SSghuNEIokzPY7z8rl9BI7Hf+mL7sSwVSFQWzuo/uv/gRXqxBgZf8eZARIozy8ijxyhOjqMfPwZ0s8epZy+tOPy3yjBye1wPVcI1vk4Gpl7FSjnOWvgWgvJw3LhnabNXnLE2hpobG983dBhhwTc4cA3gxCCQHsH5dvvxRw7iH/jBqQQ7xgyKJdMph57ilRlgeL+YXw33Eko3o4Svzz/4jcKcy4dEejOKnQ7Ex0tyr3taQErg1WvCcT9gpT/7BmE3FOKXZAQC4HYdiOVBXvV3jdK8llOSMC0JI+9MEWgrQ2fniZ+113Q2YUvpp811+FywSFa96QtKmdrYDp1jcA9f8OCN7PYsmPVk8DSMQDuF/JChUMC0fZmIoaEbPbSNfIiMTY4Q+joCzS0qJibroFUJyjKioQulo6fcEYbOuq+8/yddSBjnL368/hgFsvyaGA5cUWQgNPTuF/EtxM+0wDVp2Ot2UL1wDHbFf8OwGx6gd23bkMTJoGuTbbGwjsjfOkmXuf/4NbCHCeiU5abnn7HPNcrBaueBNyDgtwTiL5tCIG2po/cCy9j5vIrLmgjkzlGT08RSRSx2ragRKMr3KJzwyEDJ0LjnijFhz2uwA+IYsEjgWXGqicBNRhDr80veKmG7iqJODJcofDiCyv6whbLJs89f5qbdvZBQyNqyI9Q1RUnpjeC20RwT/NO7TsIKAVvLoHlxluSgBCiSwjxuBDisBDikBDiX9bKU0KIR4QQA7XvZK1cCCH+QghxQgixXwix63L/iDdDasMWNFW9eA3ADU0hsWEb5Ue/j5Vb/mGvErAsyfNPTXLn+lYa1naCrw35jhX/OhwicIcTHUehDviMEp5rcHlxPpqAAfyulHIzcD3wG0KIzcCngUellP3Ao7V9gPcB/bXPp4DPXfJWXwDaeoKoyqW2kQVi21VErBFKLz6PXG5tQEoOHZ1HTEzStCFlZy86nx8TOOaBWyNQsMcSeFhevCUJSCknpJSv1razwBGgA7gXuK9W7T7gw7Xte4EvSRsvAIna8uUrgpACqrjEdo8AUin0627D/P5fI0eGL9gskOf4nO9501mDp7/1Ertv7Uao4kyTflwowD1Qy9EABCAkzHjziSw7Lkg2hBC9wE7gRaDFtez4JNBS2+4ARlynjdbKll7rU0KIPUKIPZdzFHmUS+/4EIBQBOpNHyUYLWA9/E2k+daZeY6w54Bx0+LhwQzPzxbJWvJ1awa+0fmVqsmPvr2HD12dItKR/PGR/HPAHUpUACxJZtYzBZYb5y0fQogI8C3gd6SUZ3lvaqsRX9B/T0r5BSnlbinl7suZuBjh8siJAERTC+qO91B+4TtYJ0+clzaQr1j8n2/v47/98Tf5iRtv5CM33Mx//Z/fZ8+pPGYtz/9cV3HGBjzx/DCNC9N03LkNsQryPd25BLolqWQ9x+By47xIQAihYxPAV6SU364VTzlqfu17ulY+BnS5Tu+sla0IVEW/fJ2lomDe8XH0VCM8/HdgvV6jcav7eUvy37/5JJ/5Zx/jhc/+JzbKMoH5DN/72//No3/zIwZPZzDOQSQSkFIyOpHj8c8+xK71CURxCqj+2PvQHC1AAYRlkp9esVflisX5RAcE8DfAESnl/3Id+i7widr2J4D7XeW/WIsSXA9kXGbD8kJRCbT1XNZbqPEE+e13IB/9J6ypN/6ZpoTvvTTKX//Bf+b6mMmD/98f8+SXHuWrf/DfEHMnWN8S4od/8SUGxjPnHPmXKVvc9+++zgc7MzTeug3MIojAj5Uz8I3gaAOKtJAZby6B5cb5aAI3Ab8A3CmE2Fv73AN8Bni3EGIAuKu2D/AQcBI4Afxf4F9c+mafHxRFoaGn8/LeRAj0tVuQZg5x8txZhCYwWpH83y9+jczYC/zsPe9DX7eW8JYU19xxIzsiUU699AiHD77AK48fpeS6hGMGHH7iCLcMPcBV8y/A/BTE+/ixdggsgTMOw/TWIVx2vKVVKaV8hjd+2951jvoS+I2LbNclgRAQjl3+fCileS0imkCeeA1uehfO43IemiVhz8kJnvvm56n6evn1oRB9f/ZpvvixX+ObT+/nCVNh+NUn2RJpJP3UD5l571a6GiNnLjCdKbP37+7nQw1FlJk58q+9SqRn02X/XcuNAvYEph6WF6s6Y1AISAXeut7FItDSgAjHsAYOQz7z+nZIyT/+2T9QXhgioa5F+u8h9767mT4wwBdjI0xe1cyrxw+wsJBh8Nnv8/T3D5I9s3a45PlvPkfrxGH8oSD+P/hPhO75icv/o1YAJfn6mYc8XH6sahJQgMbLfA8BWHqAxWgf6vhJGDxmz1DsqmNJi+nhV4k0Jrmr/TiPbh7i0Yzgzs3b+Q/X3sua9/4XFDPJcHqcLbfeSnZ2FrXmFcjkqhy7/wfs7vYRS8UQ/W0ovmVgthVAtQzVpfOUebjsWNUkAOeeFfhSQ/Xr6GvbwZqHoSHA9ubLM2QgaGjsRzMksVCRq9e8TGtknkCrya9lj/PMyNd5T6LMvtnT7D20h/LkcfZn8kgpObjnKBsXh2lqiuNrbUGEmpbhF60MsrNZCguFt67o4ZJidZOALwjL0GsKIVC6tiAtCXPjyFKRV//qqxw/bkcLfIrC7be9l4TeRFyNUCpXgRZkrpPS0QlU/xEmy3l+eW0P8VATE+NZHnvsEMWKwZGvfJOrepoIpnwobSkIrF4SKM9PU1mcX+lmXHFY1STgi0TxhSLLcq/g+l0IEYPJSawnnyF5cD/f/dxXKZXt3IHQhkZmzRJ/f3iY+587RWCrH7EjSeTDN/Ptp44zVixzy8ZuRiqTZErTNM6Mc+jZV9g49grNPUFERIf2RlAubnpTCRiWZN+xYX7vv3+F3/yt/8t3nzzF8HR+xUfwStMA69LOiejhrbGqSUD3BdCWy37u6YfeNQhrDGVwH51dUdQXHuDP/+SbVAyLxg2d7LrhHqaKef7xG49SeP7vQXyD3P3/H//72WMMS8lX9x/jjvU9HDp4kOv62jj813/Fzr44gVQEghr0bwRxcSRQtSz++svf4s5b7uBP/u0vcP9X/4jRl1/gy/fdz6GR3PIPhnLBE/+VwaomgXDMRziqv3XFS4FAgOpt9yAZQzCFnlR5d38HB778AF/82lGapUr3jlvQAinmpIqopGC2ytRsmfGaM+zRyUWOjKSZK0/xhb/+Mu3p04TWdiASflA0iDVcVGqAlJIvP/AKv/tbv4teTfOe3ddw7+7rePeN28m8+ip/8Ev/iYPHp1csCbHIj30C5I8lVjUJNPgh4V+ehBohBPpVd0G0E3xZiMZo2r6Tf3XXGp77sz9nbvA0H/3gdpKRFvqDAXy9t0KmhbnAesq1/Jj2hnU8tecka2WcD4hpbrl7MyJUBa0IxTwoKd4uC0hgsWDwF3/832gIGNz/1a/y/e99h//9tf/Duhs2c/Xt29i752E+99nvY5grI4pejsDKYFWTgIZgmfQAG4lGWH8tBCzwp2jqXceONQFuiczwcx+4jZ9//x0szA7QZ2qYo2DtK7Cu3MM1vgQAgxMHGS/O8j//4D/yvj/89/g370JYFchOQCwA4YsLeD67Z5CjrzzNr3/wwyQmYW5gCtQK2cPHee8H7yaWrPDEt77L6dGVGcQzNFVdcb/ElYhVMA7tTaAs788Tqgq77oWHTyBiWdRoBLkomY4aZBamztT7X4U8rU8dIDQ2z1CpwGsVe9ktiWS2vEi+uxXRsw5kGrJ5mBiAa+8B9e37A6SUPPijJ7GqWfbP5PmHP/88H2sP8FO/9D7+53/9PLf8808SUg2mJw9x4sQY/T3xi30cF4yRfQeQlpc2vNxY3STgX4H1bBo7YdsdUDwOwRhme5THRk7Yx4QfIRQyluCf/fCzCBJIiqBItGAKs2pg6AGeO7yHq65bD8WgvZBfTx/0b7+owUIFCfteeJSw38dH3vdBOqeKNB4bxOzYxdyNHyHRu4lNG3cQnHyCmT17kXduXvaxSWZ2Hs8rsPxY1SQgfP7lH2WnKLDxdtg3CrkMxWgXMxUF4etn/Yf/LfmpY6hqgslXv0q13Exi3Xpa+2Jc/1O/wuFH/4nhgRnyigGqBqICuRzc+sug+d92kySQQWJYi+QKOWKJMf77xgiivxGlP8bX/t8Po8cbePKLOkm/wdzIANKSZ2YtWj54BLASWNUkkGprP7N68LLCH4fUbuTUi/xw/whHTwxA/KN0XvcBOhpu4cQPvk7o5k/SuOEqSkoHllAJJrtpWH8TwS4filpCFhZh4STsuhPR2HrRZBZC4PeFsYD5yVFGB0YI+m8g9uDLpLNDNL57M4WZeVpjARqmj1EqG4RCy+pR8bBCWNWOwc0bm1CUlViGR0Cii4IM8mdfegjDqCDnH2DP3/0Zh56fBSVLSxscevwFDjz0dQZeG+D06Srhjqto3rSewXwUqylJZec9sHH3RROAAEICejddhwT+9E//nlRrI6lyFlkq0vSum3ll3wBffuFpJkQe3/gw2czyRu0veGoqD5cMq5oEmlRWRhMAZKKJfcMRXtpbmz9X5sgc+CP2//2vcPjZfWSsNRQqKUqHv8Liq/fxxDef5NDL4/hkEEMoSAT+pgaEfvHKmsQeobdQsoNwQ1Mz5IZH0D+wkdDH70LrDLPwzMMUSkVKRYtGUeXFxwcu+r4XAlNKJrzBQyuCVW0OrOgMvJrO4zOtVKru6LdBNX8Ys2k9e7/8L5CmYkcA5M0Ujj3I0aMpCqWf487NqUviy3Cy/ySwb2SOJx76RxAaQglSrsYgeAp8XVBMoEcaUIDHFir8ZrxCszJ30fe/EFQrJkcPDC/rPT3YWNUksJKQQtCzcSv+QIXykoFxuYnjyEp9QmZVnCKc6KBQ6SV/9GlmWrdStFqJXoRjTkrJ0akKX/mHCcrzWU4O78GsNtL+W19j6yvfYGRWo7d8FWIxgtSSbNlyM++7rciYZfH57Dj/rjWKZPlIVJoG+bHTy3Q3D254JHCZIIAt7XmkOfW6Y+GudeROjOAstRGIVAiGfYhUC9HgAU6ONDE2VWBj59sb/CSlZGimyuf+9yiRkkFTTNC+5Rrabnwvxs+0UfyXg3z+yYdo2z5DT7REUR1hYizIb/z+/0v31Zvo1LIEw+GL+PVvq9V4XoGVweolAaGgR1Mr2oTOnla6+9o5cdQ9eaagOLEP91o7VvoIyd5OZn0hFsZOo4c3Yy5OAuve1n3zJYvP/a8jxI0k27Z1koilmZqfwteWIB6HkW3vp1XrZ+bINB03rCXY28DW99+ElgghVAUhkiBr5sQqmMjUw5tj1ZKApmus3bV5RdugBlWCoSWPWAljls+eRMs0JPl8gFhLhLnDB4k2f5ICsbd1T1NKHnh4krkDM9x6YyfX9PlQTz1N8Og0g93dtBHipl/cSDK7AW3oCOqpcXw3XQ2aimnU1f/K+AS5o2Mkb92J0C9u5KKHdzZWLQmoAtYmVvbnVRHk5JKe1MqBlT+rSIv2EG/fQHb2IIV0Hn9jMwdOC665QA6TUjI2lCU8Ncuv/+oWQoth2mWRx775NdZFNOSWT7BLQLhJRTRB5vQCvvIYmBKpSKyqhaqpYEmMH30LdaJMWQviv2Uz4jKHWitcznWoPLwZVnWIcKUZrlQ0KRbPjns19t6AojfgdrkVpvfS2BlB+jWUcAy1tEg5O37BFvLUeImw5uMDn9rOrntbWddTobTvSV585lGme64lGg8RgjNhEzm+QH5m0p7MQ4AWsF8HKS1ee/IQhUAAbe8o1mj6Ip7C+WEiYzBf8GYUWAmsahJYaZw+cpy5U4NnlbXt/CnWf+zv2Pzu3yHgq6UCW1X2v3KKucMPEE+GiAZMrrr6wqYUt6Tk+W89SenIAFgWQoDeY2Ic/io7W9rx3/xxgpagXGOWQq5E+tgg+qadoCoIxJmcCqkoLDS3YpwcBFkm/4NnkdXLK6DzU7PkFrwlyFYCK91ZrlpYluQ7//R9quXcmbKoAv8h8QC3/MePofi3sucHP8upo8ewjo/xzNABvjW9h1T/bjJjJaLVAgLfed+vWqwy/fjTjJ+O0qTl0a/djuiIkPqd3+buXywj21rYa8Aro7BRk+z95iC9Rw9BR4CwYWLpNQVB2OHN6z5yL9Hv/y1qTDL/7X/AilWI/+SHENrleWWsagXL9AyClcCqJQGh6ss+lNiNdK7CQw9+96yypliA99zbQzQ+D4sWd991O1zdiXzuR9z698/y/eIiajBF1SiAmQES530/YZns9i2w1siT/bv/SezRDvTeHkRLN/51m5H+aXaJKP7Hj3L/4yNMDe5j53ZoLgyxcOw4Vsc6GprtsKACpK7ehrLlP8LkIi0NFuq69fbgqMsEWamA4ZHASuAtpUQIEQCeAvy1+t+UUv4HIUQf8DWgAXgF+AUpZUUI4Qe+BFwNzAE/LaU8fZna/4aItbYRbrjcqw6cGxI4cHiAUwMHzpR1hoP84a/fS3jHrchnn0MEu6EhDtUOROd16I0LKCwyduAJSuJaYMcF3VNTBVvaQvjV12CnBVRgehCOKvBMBBHrwIqvoXNB51fW50l8oI9w61Uo4yfRZ46hbt4AR49gzOYxLcteJjgcRItGCFx1NSRilzdcaFbPuaCrh8uP8+kqy8CdUspcbXXiZ4QQ3wf+NfCnUsqvCSE+D3wS+Fzte0FKuU4I8TPAHwE/fZna/4ZobgyRSqzMIh1SSh58+EmMminQnYrztX/3c1z/rq3IQQPz0Qm0j70LWvsh2wxaEmIHQRyjWtaolC0QFza5hgj48f/nP0SUZiFzHGbGYXoGqCBRwR/F7wvhb1qLaGqBRBNoYaQFWrmCKOWRR55AnBiC2SmoZhE+P0I1QVGQoThi67vghtuhscHLH1hFOJ+1CCXgGLZ67SOBO4GfrZXfB/whNgncW9sG+Cbwl0IIIZd5GtuQAoEVek8z+QoPfe87gERoYQKt7+eJwnam9xToC1g03fJeook4/nADItKCWDBJ3nwHzd89RqbUR9QXRmihC7qnEAKiEfvT2ANr5ZnFUc0ap6iasIVXCPuQYf8jRUQDgogPfhJ1fhJ1+iQMn4byOIgc6AKhGDDxLHx7AH75d8F3/v6K88Gs9fqVmD0sD87LaBZCqNgq/zrgs8AgkJZSOvrbKNBR2+4ARgCklIYQIoNtMswuueangE/Ze5d+3LrCygweksDDz77KsYMvokS34ev7OUbLGf7wH6YImFPoIT/JRAMNkSOkmqdQSiFiCR9iMojWsYNUKYyImIhg8u03QtRigLUHUDYgoHJmLICUIM3avuo6R/dBSzc0d8GWW+1KZh6wQChgWJAr2xOeXGIMjXvzC64Uzuu/KaU0gauEEAngO8DGi72xlPILwBcAhAitmn9/sWzw5S/+PbFNnyDSfy/zp2cp506it6QIaTqFvM5kVmc+0ElwskA2U8I/Cx+683aSSpah/Qukgs3IS+gk0wU4uT4Se5VkS4Kq2LL9OrasaQuggJpwXQi4TDO25bM5vLEDK4MLonQpZVoI8ThwA5AQQmg1baATGKtVGwO6gFEhhAbEsR2Eywux/CkQEvjBnklGW3+CeLIVY2GKQGsKQ+aJJgL4fSFU4wRmsYiZNqnoOkYFPvXJ9/Kff7GfF15J8qGf+DVyixvtSUsvEXSNM2qAJW3NQFFBU0CKldGYliI9PelxwArhLSVFCNFU0wAQQgSBdwNHgMeBn6xV+wRwf237u7V9ascfW25/AECopWvZJhSRUlI0JI8Nmvzp9xs4eLib06+OMHVqkkq5iilUpBogbwWYL/goqnFESKGUz3H19Rvo3thOUcI1O7q48/33ElLH0LVLm69frT0KwwRLB79ip+k6/5iVlr/K3NQ7oBVXJs5HE2gD7qv5BRTgG1LKB4QQh4GvCSH+C/Aa8De1+n8DfFkIcQKYB37mMrT7LdG1vu+y5rvbarXk9JzJj54v8NVnJC8+MUhpIYcaKqLEBEbJwJyfwlpMkyln0NpaMNQERrZCoSiIxpKIWCPFnI+ZNKxLKdx09Tqe+c6fQOlXge5L0k4T+x+XsezvEPYkxlVAlWAJCMCyzh/wOljetEIrhfOJDuwHdp6j/CRw7TnKS8DHLknrLgKpRo3L8UpLCSVT8vSRIvc9WOKRH2aYPXYaGdQhWgFRxszPwNwMqq9IqVoi2NKKWclRHT2NlmrEH4+jqD76rtrEtuv6uWGThh6w9fKJYy9y84030NLUfMnabAKFChRNiPlsIihi+wrK2C/BShKAxCYlDyuDVZsxmBSXNpQtgVxZ8oPXqnz2H/O88NIi5dEhECW7Gy0u2H4IKwfRKPgMVD2MFtRR0qcRehmyk2j0kjU1NF+Q/FyE+ekmksFmYj4FC8kHfuKn2bG+lVjs4nMcJGBKyEsoCTD8UKmZAY52EMYWQAtYqQHDpoQFzxJYMaxaEriUTux8RfLdFy3+7CtZXv3BcQxfGHwSZAFiCbtSMgq5WZAlCDeiF7Jofgu9mCEYsMhnpkhEfSzOncIqFbEamhl4doF1O64i6FfxKZKHjxQ5pK7h9tSli8HngVnDFnhVQEaCKSAnwWdCTLX3I8BKTcEipSRfXqGbe1i9JAAXr95KCUcmy/zx/VmGDi7is7LcdYfF+PwsE1kfM2oINBPkIpTK+BoUlHIM3V8gkAgQ1TUqk2Mo5VksNUt2agzD8CHURsypKURrgmopjy4kh04X+fd/O8g117TANS2X5Pc7EJrd09csDspAj4CoWp+MdSUnZTUMi6mRzArd3cPqJAGhoASjF3UJS8KBoRwPjVrc9u4k236hgfagbWYMF2FsusxH3v9VSopCuKOZakUjmQyh60nU0hTpoQMsFtNUZl+jkp8AoVApZNFC/fh9ZSqKD4w8wijx6IE8n//SQSqE6Wi4tHP7iZrTTwWKVVhUoEuzNaV5C7ISQirEsB2GK0EEZrXK7PDJFbizB1ilJCBUlVhL29s+37AkT09LHp3QKQoVNSs4KgSzIeiPWLx2IsORV4+gyTGieo4EBczcCPnxHAZZyqUcCxMDyHIGzBFAIaT6wKxglgcxzX5MXxsiWyQzV+BP/uIZpvM6/du62ZjwXzJBlEBV2vb/rAlZC9ZqnJm4LCQgKGxCWNEJxKSFVcq9dT0PlwWrkgQURSGWfPsjCF8ezPHP/8dJrOIiQlTwqRbXvm8HDcEA3xo6zuP/9CTZ0eMUpk4grQyZow2YVhZZmgTRDMYgBK4Da5KOlq1saGlhfetamhIxDu55jofGZ7GsPhQ9xOkDh5lfzJNas52WjhjtKeWSOTRNYBxAgm7AFr8t+MIuQhVQwo4URHi9FuD46t4JyUQeLh9WKQkI4om3Px7hub2zjDz895Ryc+DTUSyN8f2vEYqEmB/8IdWFQ8QTHdzygV/hzruv43vfm+bFZ55H+NJEYs1kjn4LfdPHMSbDfPIjm/jd2+5GTfUR8Csc+sZaHvvC/6acG0QEdpAeHMDft4mmljhdrSE2tVzCTEdZW4IMsBTbKRgU9QyxMrbwq/XqZwu8hLkq+HWIvEMyCz1ceqxKEtAEdLy9KfsB2N4fYUOfxdyCwtz0OHq4ETLHmJ0W9Fx9E1uu/m1aoiHC/iBjmTx6aJK1a2FmfIQt69dyaFbSvS7H8ck5epXjRKINiO5OyMcZqaYxg41Q0lDUMlpqLT5/lObuJjY2mySDl04xX6x90kBBsfO3DeyJIZqwCcCBgS3kCrZD1HEUqkgWCqAHBX7l8hCBhZcruJJYlSQAFzcucX1rmJ1buzh5qkRXUxuSALMZFS3WgC/QzMEnn+L5ySEsX4ANHW3s6u/g1rvW8U/f+gGR4lF6WtopDNxPd9Rgd1MRZeqH0JBA6rfz6uQh8mYEEfChxdrwBwIkm5PEdB83rtEvqZAFpKRDwqIUhCW0KJDEni9EwRa+POAM89QtyfBomlfSkt1bk5hF+Nsh2JuHf7sZrgnZJsSlJoLZokWu4tHASmF1koBQL2oAUVdzkN/+Fz/NV776XcZOjNCeaqBsZTFMAxGawZdQiF23nbWbriGo+YkGIBwKsfuazxKJBmhqCGOWZ8kXQqw5cD88+VegnaIabuDRVw6iiAas6gKmtZZCcZHmoMq2NWHWpcQlTXD64dMneOVAho3v2sY1HRoNqoIqoaQIsgKi2JEDo2Rx+MQo33lpnuJikY/81C4W0vDdl6pUWzUGhkwetiSJLTr9YXHGn3CpDJfxoQnS816IcKWwKklA+PygvX1dQFEEOza1sun/+1VmJ+cxhUQIP7mZLLHGOMGQPRjfUoLEIzqqKhCKWNJDttpf1/42KCWYyTJ5aoCjsxNYVgQRasZnZokmuohEgly73k/40s7TgdYY46mhYWafOE3P7a385+cgEYvQ26mS1AzSJya4aUeUw/kC33rwIMmNnaiW4B++PcIvf7gDa2iS730hzYYb1vKJu0K8cqLEZ/ZWSLwryr2dCtuAxkugGZQzCxjF/FtX9HBZsCpJQPX5US+CBMCeqcevq3R0NdULO+JvozEqctP18MBXKRXCFPVuMHT0oI4/5MOvwbYNbVzdoV7yGbv6WpP87s9sZ/vaBH/1ncN8YG0HHe1V/u7BcfY8N0K5qpPzG4zte5npcgXrcJmPX7OOUlXl3/zu9+ne1c+77wjxmz/j58/+7xF+eMjkV39lIyiCv0nD78agUbX9CSoXQwaeKbCSWJUkEAiF0P3+lW7GGcieLTAdRPhVrKqF0CKoviZkJUhH33puvbaBpuCld7l1RDU+/+wx/v0fjZKTIR6ef5n5pu2s/1AvXNfPL96qsr4lRIu5g/knHiLUFWNANPPRXSEeeTLILbuTVKom/+e+ozw/YvCr/347L+wRnDxVZMPtQYYViR5VWCds88DDjydWJQm0tcVJpS5sjr7LCdHSAF2dcHIBoSdAjWCJGE3dfQSDGrs7fZfF6x5SBJVihWJrK7/zvn5mtSg3bQtzeFYhdItke0oyP1Zk+9Vt/NyHfpuFTJEnDmcZPJmme2c3H7ymmYcOp/n5Dyewnh7ir/7VXzIykOS6X76J0cM97FmvsTlW8w2s6DhkDxeDVbkCUUyzBeAdA02Bzet5duwUxWIFRVWIRENEk01cu2MTfbFLlyDkhk8V/N6v3EhPqpnHn5nh9MEB/uLBOQ4dyPP+RoVYBfYOlrG0EFG/SndzhJ+9pZX33NjBx27r5MhonmPPHCLsgxOHJ2jtCNO9pZPwyCjbj0/yk3H4wXNFfjArMZBve47AgmcNrChWpSbwDhL/MygXg3x/eApT11E1lY6+LmItTdx6bQOBy6RLCwEdCT+RyjQ7r+5gnd+ke0uMjkYfhycMGkICGQmxqbneF2iqIKYKruuLkysbHN/QTV4JsmtjJ9dfdRWPv1ZiW28EMxLkCw/PkN63n8fNBF/v7OM3f6WBG2IXHuF47ei0N8noCmJVagLvNORLFt89OMgwYXwBk4hvke41bfRsa2N7j3ZZp/BXFfjEe3uZL0j2DAX57j6Dz35tlJePZPjH/SUaNJOg//UNEEDUr/FrH1iL3ydZjAX47PeGGcwLvvSVx2mNl/nwtSEWx+exFjLkoxZ7rbfn4hs/fBDPObhyWJUkUASGgRnsjLkS9Yk0ViI77cipBf7y8CSVzm5iMZVU9zqEKtjYEaUpdHn1Fr8q2LSzG1/c4o47k1SbJXfdGqc/Jbh+ix9dMc8594IEFismDw2V+ci/OMX+fVC6diPFxhBy4xr+9tuL5IsqkaYQoTUxGsws6cMGR97OOCBv5aEVxao0B06pPn4Pe7RcEjtdNunaTmAnyiSwU2gDtW+d+noFjmheChF9cf80RwZHUYwFFhcWSW1speSPsaHFh7IMxkugZHDktJ99j7+InCmysKaNWz+6GXwqWze9PuxZrVq8ejzL//jOMD/1c/1094Z58bEZPnpjjBfHprhh3WYqC0P8/bf2U/UpbIkV+Inb15BsUmm4xLkOHi4/ViUJqGs2kxeCPLY2oFAfJCNq2z5soQ9gk0EEmxji2OTh7Adq26HaOVrtPI16bPwtSUOqZOcXqZbz+AOtTE1OEVuosLs/siyreXWkdG7uD7Pp5s28Z3s7g2mTzzw1i7AkoZ4g6+cN/D5bPxobzbDnGy+hjE6S2HwjQwdPctcdEZ5+7DWOfekU193SxumD+xk9NMDv/NZN/Py1u1FNk3DI/8a/38M7GquSBFo3NtEioIKt/kewBThIfZJNE1uI9VqdhdrHpG42COwZeR2y0GvXCdSuFa59R2rbkdrHWaPDV/uISABTWliVClUKaPlpdu/spDGyPCKjqfAL1yb46jP7efnpfezbXyEQauC2u/o4/vUfkjt4krmACVqOq0IKt6WqtOxOsqX5FL/91WOMRfr58L19RESe33rPZo6XigzsbOeGzhBRn4YQ2tsWfm+S0ZXHqiSBpB8StS5W4+xJNZ2hs6VaXUdLMKhNwVUry3H2hJwCm1QKtY9VO+bUka56jtahYWsZY1YVTSthmrNogR6qSpxgKsioKjCxtYwAb6xlwMX1sEIImpIa+eFR2mJw8y/tpqhGSc/nYOEo67p1ErMLMPca123aQHxDE6JHY8u6Ar/fcBP3vSa4tSPKPe/pxzQkH0vGYavdqovVZKqmZCTj+QRWEquSBMAWSI26UDtTZxm1cieVSFD3Bzi9koXdk1vUScLEJgFH2E1sIjFd5wnX+dVauS4l6fEZjIIBYpFKoYBMxRmoSh7CXqDRybPx1doVqt0/6vo4ZY5Wo1EnC3fK7huRxvRsgYcfeZpy52b8+0/z2X91HbItxANfnqM97qc5Dl19vQQiOUi2QM/1iFQ/1+/QufnaECGfhhCXdoATQLlUZuCwN7XYSmLVkoCjtjuC4udsrcARVPcDcMbTuyfZcPar1AlDcx13ZulxCMM5x6id4zMkB39wAKOSh2oRKTNoWKztDdJQu066VteZ5We6dh3HnKF2TQWbrJzf45gdUWxyCFJ3ejoOz1BtW434ef+H7qK9pZ/O/hYOHhujM5EgJHK0KZK0lcXXEkPbvRtxzXsgmEIIQeJtDJe4EEjLolosXN6beHhTrEoSCAiFBPXe3XEEKtgC4ZCAM5GGI8iOkPmok4RDAgKbQHTqZOGQgok9S4+z7zjIDWBieJHBF78P5TRIP5gxFH+ctoggALRgOyOdGbed+5quj9Nex0SxsE0St//CbVs7y644zk8fEKuYDI4X6b+tjVhfiJF4gP/xpSdpfvElKteup63NonLrL7F49Q1oinJGw3DIx41LqgxICVVv9aGVxHmTQG0Zsj3AmJTyA0KIPuBr2PNRvAL8gpSyIoTwA18CrsZeiPSnpZSnL3nL3wiqTrCp44zTT8X2BYSo9+iOsJRrZRZ2L+ysxiNq3z7q6rfTMzsC4RCIozU4guoIjYl90p7Hj1FaXACpAVGUUJLEpjUkG0Nn8hWC1NdJcITaMS8cs0NiRy1M1091SKBa+0DdV+H8BgcZ3Ue5dzOlbJEnXzA4eGoB89Un+Fh/N+FuDWPne/jGjt1MK8oZh2ag9uz81J2ezr6fui9DdX3OucgxbwbpLUG2wrgQTeBfYi9E6kxW+0fAn0opvyaE+DzwSeBzte8FKeU6IcTP1Or99CVs85tC6BqF3m6OY6vGCWyhyFP33Lu9/M6L65DDmSm2qL/s4Vp5gbqwuV92NzE4WocFmJbk0GunwQwBaVADKA3dNHS10ptQKFBfAswRfHdkwunhNWwCKlOPcqjUzRHHQeloNE75WWaNJjk9t8DDPzxJqZTl8NQYn3zsMe56VwdcexNP3PPTLOg+DGyTxFms1FfbLrja45gjaq1tDomFqBNuGPtFcaIkjobkRFkcsiwDlpctuKI4LxIQQnQC7wf+K/Cvhb3c753Az9aq3Af8ITYJ3FvbBvgm8JdCCLFcKxOrqqC5USFOvUcuY2cOOj2542BzFvrSObtXc3pfx1QIcbbXHs5WkR1TwiEJR6CltFgYHEYoBaRiggjjDya56YYeIj6FCvUeVKNuflSoax+O+u9EMBTXt7PYqDsL0kddu3GTQaVQJD53jMkjL9LV1Ulydpb3bG1Cv2U9e977caZCflqEOKMVOQ5PWWtXhTrZlGofzVWnAAxSJ0mHxJxQLLXvAHXHph/IFCXzVY8EVhLnqwn8GfBvsH1OYJsAaSmlE9sZBTpq2x3ACICU0hBCOFPYzbovKIT4FPApe+/iJgBxQwE2CejHfjEdtbjTuS/2y1nBfgklds/n1gRkrUXZWqMna3WdSTp1bGJxfAxOL+iYEv7atQwpUauLSCFB2sq72tzE1euilJV629w9uY4tYFXshy1dH3co03liVWyhcsjArYmUa+VBIBwOE+t/F9c2dNGxto3NT75I2+wRCq2bmY8mQIiz/ApOdMXRiBwxdQiuSD1C4RBGmNebKI5W4RBrvvbsHWIZn1ikkPXWIFtJvCUJCCE+AExLKV8RQtx+qW4spfwC8AX7HqFL1hUo2ASwDjv5x/HqOzF4Rx3NUn9xndDfUmehowo7L61DFI624JgZRu1D7TuLLXhmVVJYOAhq0l4DLNDO9R+4nl3rw2dMAZ/r28QmLkckgtSFrFhru7t3dUyBIPXEJkf7cXruMxELDT7ygVYOnAyxJx/kZD7De2/5aSrXfYhJxFn5EM41nPs4GomjVTjmlFtbkNhp2f4l93ecnA5xOGUVan6YcglML09gJXE+msBNwIeEEPdgv4cx4M+BhBBCq2kDncBYrf4Y0AWMCiE0bOf33CVv+ZtgHsEw9ksXApqp95DOC9tBnRicXraE/XJCvYcVrnKnd3QEpFK7h3uS8Bx1LWFqpsjsTADyz4J/O8HebWxc18i4z05pLmILh2Pnh6hHBwQwz9naikNazj/N8UE4ddz+DEe7qGKHIKMCfvTsEFmjlYkcqDt3IT5yPVqzOEN+vtr1qf0GJ3Cnuz7O77Zc9ylR960Y1CMojlnimAiOhuKYAkEgVi6C4TkGVxJvSQJSyj8A/gCgpgn8P1LKnxNC/CPwk9gRgk8A99dO+W5t//na8ceWyx8AIANh0rrvTE/pCJqCzWDOqMIuoJG6Cu5kBJrYarij2lrYrFes1YtTFwRHbXd6zCC2FlCq3W92cYrqwiCIIKgjtK3v5s4bm2kStoAZrrpOIpOjcZSoq/2OxuFkPTqmgPOtuD4OIUVq7XZILSgE7Tf0YOQMdpdhpsFHsgFOUVf7HQ3AiXQ4ghygrhm5w5aOX8RJrHI0JtN1TVE734lwOMed32uW8mA4rfSwEriYPIHfB74mhPgvwGvA39TK/wb4shDiBHZn9jMX18QLg97QghqOnXnZnO8A9Zc3gC2sULflC9gvcYV6dh7Ue0fHk71AvUdzog0FbJJwvqvYL/je02DQAgGFYHMT1/crrGvwn/Gm+6mHJqFueuSpD392tJFF1/EitsYhatcpuO65NIToJA0VgcGiyczBGdJlnWuua2HaVJg2oaTXtRxHW3IGTjm+kFLtPo7gO+aTo71UqEcxHL+CY4Y55oTzGx0nqw74pYk3o8jK4oJIQEr5BPBEbfskcO056pSAj12Ctr0tBOIhCPrOpO2msHtEx6HlRAecHtVJIgJbkPKu8hx2iNHJNchgj0p0VNowZ4fN3Lb8IjC//wTSaEAEqrS2d/HBD99MWRNEqPf0AlvzWOqDcGx/amVOvoLjuHSbHU7bHYKQtTYUa7/HEcpgg0amUmTNmhB5v+R/HYff2AwnqfsVHDvf0UCcBCVnQnBnzUJ3NmW59nGea5nXm1jucRgOiUog61kCK45VlzHoD9hLDgSxVf4p7FCFkynoDB12RgI6Kqyj6rtTbR1HYBZbA6jUtqvUByLNUrfNi9RJJWdZLM4eBOMpqIZYu+FuOre2U+Bs+9pJujBd7XDUa8cmd3pSSZ0w4tiC6QitST086Jg4BrYQZ2ttLwdUhnPz/OXv/yP+He/nX/7kJhLVAFfVUhwdp6Z7rULHF+JoUyHqmoGPugbiznUwqA+9dojMOR6sXdeJEsxNe2MIVxqrjgSSOmxR7V7bSa7RsF/aGPWElVCtLEu9N3PUYeeldwTOETQftpNRoa5hRDl7CLLjSMSSxPIFbiNG7OYP8Wv/4ZPgU/HXznHb3U50we05d+7p9m1kqWsMVWyScHwF7p7YMW8ckyVeu1ZZEajXrGO9VqWzv5drd/qpamdHRxxzwNEynJGVndR9Fo5gV6j7U5z2O0ToRDocknAIxannEIeRc8ZzelgprDoSCGO/sM3YwtmL/VIWOftldV7UKPaLnKeeGeiow1Vsp4YTAXCSjAR1jcCd3ddca0MeOH1iksPf+gbvDczyGz/Zz3RnmJKoj2zUsbUIJ5cB6sLohNKcwUFOAlEF29Pv5AC4Rx06v6tM3QmXwQ7LOO2SAtb3p7jm7x8lNrEXseOfkRd+Fl3Pz+07cV6OYK3cSQpKUdcGHD9AxlXf+R1ONqETPbCoJ4tozjXSzjhKDyuFVUcCw4s+/uwF8MclDQ2CbgVSoibcGiR90OW3w/ZBCSUVQsJeNCxFvVd14uOO2uqE3iLUQ2EGNnE0UrfxnWjCwESW0cVpjq+7k+zmd1MW4kwOgFU7bwFb4B1Pe9F1XUdwEtSHQzsOSichCeri46jnmqvNTkKP265f0BQ2bd6M+ZW/Qf3oB6j09ZEVglTtOo4jz4mqODkRaVc7nIFJzv0dR2oIe0BUmnrI0PElmNRJ1gmPSkCZHn+zf6eHZcCqI4GJfIi/ftRAlvIEOwOoQR8B1cSvS8p+H6oGTZokpAmKBUlDi2CtD1pCgkgAtjbbL7JPgC4gpNdeemGvsuOrHXM0Bick6M7aM4H8TIWS0gTR7RxL+9iJODMpyQJ2z+nUd3rdjtr2vOv4AvU8AJWzZ0lyMhid1GMdW+CddOQEdZLKYAtnXghO33kbbX/1X2h+5DHkpz5JnjqxNFE3B6BOhEXq+f8OQbrzBsLYJBHFNj8qruOLwAT1DEuH+AQQLrr1EA8rgVVHAhDEeuI1yExR6GlDCyqUsgV8G/oJrk1SrRqcLEuUYp6KFkaMh3hOEUR1g1i/ipi2NQafDq0xCEhoUCR6AExNEDcE3T4IKxBSIKTCGgW0EugatAYhakmMkWEa9FYWs4LvPTKC6NG4fXMURRFnbOcctknhp24WJLG1Eick6EQrqtjzDDgxeCfEpmITRZq6P8ARNsdH4TjrVOzeuppMMtixkfJjP6Tllz+B9OnkatdwNA7H/HDGXTg9uEldY6q62uKYLE7yVZo6seSo+xqcaEWi9t8Snimw4lh1JKBUKijWLJEda8icPIlIpPCt6aazzc+mTo1SQeXgkTxWSaLGq8zPlalkyywaFnOnQ2iaBZkM0pKMNIYIhH0oI7MUcosom3tJxCXJHj/lsI9yCYzpNNbhQapWFAWThpZWwkaYkb8/QLU0hTHzNOUfLPKXezrgi5+krytEQIOgZveY7nkPF0R9TgONeu6/Y4Z0Y2sejiagU7e/HVXdiRw4cxo4drgT4/cBRaFzvNhNafIQHbNpRHsTc7VrzFFPZCpSJwL3aEeHDCrU05UdInBIwB0pcRydFjYpCOwp4asSFrzgwIpj1ZFASyrIB+7t5/23tfDEHoWjuSh3bw3z0V0RGhMK+apg7o4YE/kYpxYkYxXBfU9aVI0yasykWFVovq6J6VOS4qlp5NgEca2CiUYgn6FNKRMc8TOhNqEUqxRHqgTyMUS+QqA8w/TRIYy2rciN19N0+DvIzElmRRi57SY+/SLIfRUCSZXuHkFDFBIKhEOCFg3imsQvBBULAiqEpKBBQtGCYgV8fnsMgDOIyMA2Udprv71EXS13Bj85w4DPyuLTBbmmFjpu3YmvMclGYID6lGjjnD2S0Mk5cJx9jtYSoj4fozO2AeqmgluTcMybZmwiyANpS5LxViRfcaw6Evj9T2ziN/55P5oq+OBVMQxToqsCUZscL6hCY0CwoRFu7xFMFy2eeaXEcNlHrCWAHlEIITEKFWZMk/jaFsKKSXo4T3MixLG9M0gJSmUILElx9BBZRcW0wujhCM2NBro1TGhNnoENVyHmDqKFwwg5jvrsPihDOKwwF/KhhRUqIkKkrYFAsEoiVkKJRFgkRCIiCHX40Eq28I/nIRYD3YTiNFCExoSkKQhtUUFD3DZPugWELSgZkBPg90GDArqsp+1GBbT+7i+wLmKBruLDVs8z2MLpCLvTozth1GDtfMdBGcAWdMdccJyqztgHR6NwTAuHGBwCqxiS/II3gnClsepIYHFiBmmYWEJFUQQ+7c3ntbFM2NarEMtXEP4wJw9mmF2YJ4CFfz6NQoam9kZarQEGHhlGzQnCW65GiTSgtzYx6w9gViQlU8XfFEXrbaaxUWXN1hAdN9xKIGgRiAcZntfJIRCKJKAL4n4oKrBoCDQfNCWgtQWKBegKQLkKpSpMT4BPlRQVSXpBIITEXzUoDBeYnFOwMjNo4TB5KwiayZbGJDt0i3JxkW1dPrb0hGmvOTRHcjAyb9JaSdO7vhlZC0465kMF2+/gRDry2E7KGV6fRyFr5Y7z0j3+wTFVnHyCJmySMbDNDcc/oVUqVCaccWceVgqriwSEQiCZAKGAdFLSJVbN7lQUx6fPGc0gFhBoU2Mcfi1LY18Hm9a38dLDA0ycHKOQXuDWn7+TP/z5VjqDLRxPV7h/yORwWseKqKgaVG7eyuJojnzRxO9TiERMerYEGJ2EtVujTE5J5qYg7oNyRqKHoLETupOgSUFpGiIRaI1DqApyEVIGZKogAnCyFYaKEDAEgTCoPoH065Rbo5RNqMgAqqKQkIJ8XnIyDK+OChqTMSbbBT9Iww4T7m2AdRGIFixe++vH2NlUJr6jH+2qLSTiERK1J9NVe5Tu+QMck8JxMkKdMIrABmoTl2CTBtRDjGXsrE1nkpZF6hpC1ahiZqYv2b/fw9vDqiIBoQhaOlrRfXbiq5QSy4TRkzO09jTg86lk0nkKhRJt7Q0gIaQJfv+nNpOe38fBUy8QabuK8uwBIq2N9F1/A3phAnMxgS8aoyfq455OiwPPH+fYuKRzQyOjxydQpcXd1zRxTX+SmbESrVWTQqvGl75xhLmGFsKJOKIsWRzPkx8eZvDbJv5rm/irX+4k2CBQVdAEKAJI1ZOHJFCxYKIqGAOenYC5IFSLsFhRmV+AWIPglZdy6JYfM5dnav9xtP6NqG0hxoRGfjzNYK7Isa0N/GKXj93NGnf+u4+QP3iahZeGSCQz+LdH7OfH6+cDdEKPzoAqKSWVKvh94owvwoETHnWGaxewTQmHEGaw/RSOw3MByYNSnkmP9rAyWF0kIAS6Tz+jAQyNzvPSoQHu3L0FVZVgWgQUlUR7IxKJtEAKyX3fepFHnj6GaRgM7f0HFqZm2Hzjzfin9/GKDPFv75OsbwoxsmDRuqmPRGOcdl2ybn2QYjpKf4OgvKjQGA1x651hsGzhNe9dxwMns4wdn2bu+CT56XG0aIxYUGdo/whapZVQ+I1nVRKAX4FeP/QA1/bCUBWeGYODBvhTMDknCZQFJ54dJNTRBP5GYpUKpRGJmYgwu2ccpM4Jf5RXUj6uigr0gEZ89zoiV621iectYFkgBGfWHNDf4K1xDy8G2wGYdO1vWlJ/AXsCyvRbN8HDZcSqIgGfrtPTZfdPM/M5/uiP/w8HDh0j/q9+g0QiyNoNfZRKFVrDflRVUDUqDBwZ54HvP4Xf30ilqrDtxjvBHyOSiqHmDhOYLBDQI5Sb2+hLVihNz1KYmUak53ltsEpz+zqmZgTq+iY+t0fS1yTJZyuUItDYapGK+ZiMCjZd5ePw41Mszk6T8yV59z1bCAXO//E7g4LW6bCmBxZN2FuCF5MK6mKVjtsCVAJ+4ok2jHQGsyFFR4cCAzNMnK6wvaOPOxO2xuFAde1ICyQSo2rh86tn3duyQFEcIrj0C5B4WFmsKhLQVIWQCqdPTrP3pYOY+TK/+PGfpDmV4vjJQXRfgMZImEqxilUxGR0d50//5BuUswv4CiPMTJlMBavcdset9Han0MNXEZJFyiR44dAUkYigu81PQ996ssdneOz51+jeEmNNZ5Jkt8qhMZPZ01PMjmeoCj/+zUEKJY2umEVxVqV74yaC8UZ8oRC/+ZPtaOrrpUnK2nSEii14dX9GvY4iIKHBbWHYFRKcuCvF0UoKy2eHDEvVMHnLrtf1wdtYJ+HuLtsv8YbyW8uC1HQFy5SYpkTT7aiKtuQtkRakZwvEUkGkJdF8yjkv+Vbw0oTeGVhVJACS4aMneGnvafzhGOs3bcYvDf7nn/8l191yPbc3x/FrGoOvHaXkg+dfO8azrz7D8MhxlMg2hF8jPXuURx5c4O6P/iRNgTIHDx1Cpno4dPAYIyJPxBfg8N4X6enspJLOYU1EUFO7mDhSpDDrY21QkOpMUFE0upqbKBtF1LiPdMLPcLFMvpynMnqAePX9iJqlba+/ISkWqvh9kMsXSM+USCTCTIwN0dTQSEt3C1LC4nyBRIs9CZgQEBOwU8AGBbISRkswXwFTgWQQ1nRCSrPJ4c06cCFASkE2V6FchnxR0tLqI6DzepNBQCjqQ1FAnodaIGvhSfd1MgslTpyaJpMtXti/2MMlx6oiAcMwefT5l1EiMbas72N9XxPxcIhd117F8cFJnjl8ioVihaQIkUtP8vLTL5Bo7sL0t3Druz/I6MwEa/p72LKhh1ylSl93E51dd1GNCE5OTGDMZ1F9Gh/86K00NTSDCBCNBpiYHMYXTzB85AT62mZa+9rZ9+pRjj39GEOHn0NPNNK37XZOHt1HxCjw67/2QTAKTJ6Y4fT0GEEtxuT0AtlsjvVr1jKbmUA1dMzWTpKJRlTV9hsITRBvDr3udwsBYbU2gCfMmYkCLcOgkq+gRgPMT6SJN8UwDYk/dG4/hATCER2hGugBgV8Tb0AcEk1XkEgURTkzlNI0JZWFKoGUjlii5ZgWKC4rIxr3MzM9Qz7nZQutNFYVCWiqzq5dd/Hwkw8jdldYv60XRVEoFqp0rulFwWTg+CjtbUme/lERK5wkEIOORDuZuQmamlL0tHexprWZRHOISrZEWQiiyQC//uHrMObHefrZVwibIfLD4xzc9yK6ZlGWOtmyxaYd21AXppnOHCSaz5NMKTRu6eTU8AgDj/wfdF3ljmu3Y84e5qWXhlnfsYaOzg6C/jDbdm8+Y2sL0Y97WkZBvRuXEsyqhWWYaAHtTKhTSjArJkIVKKrArJggIJteYLESo2LpWPMmiaSGtCST4zOcHpwiEAuyZk0nxXyJltY4smoR0lWEJlCXaPlSSoySiepXKWbLaH6VQLBuYygqBBpfTzBCgKZIKhUTTbPzN4QQWNUK0jJfV9/D8kIs4xygb9wIEZL2ROEXh0gkwoMP/F8amuJ0dDYghJ9quchCukh7RwJVQD6XR9NVqlKjbCmkswX+8ZuP0trWSv+G9cwMDTK7ADuv30xrc4C5hSKnRuYozOe5cWsrmakpivkCxbkJgtEELeu68QWjCC1Iem6ePa8eJtzdxMb1awknoiiGwdhMFWFUURYLXLurGVNYKKqCrimob+Cel9L2B6iqsy+Rsqa2WyBNC0VXziIBx8qWQD5bJuDXycxn0MMhpAWhkIauaSAk42Oz6KrO488eolQyue3GLXR2JBGq7fxTzumvODvH4q3gvFlG1eDkiXEsS6erO0XIby8I9+DDT/OhD/3eef53PVw89r8ipdy9tHRVaQLBSJiFmXkolZDlWShXGNh/jPbeHqaPmxSLJUolA9QA3d0ddPV1EI7o/POfuwNdCzI5sUAlIHjihafY2hUgnNxIa3uEXWujpPMGatUkkbSnFYkGr0L1CybH0xhoTE5nUIwC/Rua8cdiTI4vsiUWIhlRaIsHQAYQRoBStWoPWMotEgqFSEWDaD4dJaS9TrgUBQxT8syLp5idn6OjIcbWLT2EAj4UXT3LS29vC6S08wAjUT+WJdGCAfw+Hb9fQRqScsnCFxC0dzRiGJIPf3A3uq7X7m0nU0lLYlQkqn52e4QQSAmVqomq2KaCYZjoPr2+BFqNqEzTZG5skkouw9hkhvRcnu5N/VQrBpaqovqcGQc8rDRWFQlYpSxHnn6SjvZ2fnD8BFKBUycHuP29dxNtbCKTzjA7PUUq3szY2BxPPf0yoUiU9VvXEwkHmZtbYHZsik1rUoyfPEEq7ie8rhsDQXUmzWvPvUysrZVIMIxsTWGpPuLJCJZQWSwaHDwwxOauFK3JMBu6GkinTU5P5Eg2apQLBof27UMVEp8M0trRRKlg0NqZopQvo1vqOaIFkrn5Ek888hJSlojfuJvxsTSdrXG0sA9LKBiGSSG7yPDpUwwPTTAzPEpPax+bN3WTS89ydGKW9ZvWMz40wuFXD7Bp40a2XX0V8ZYGvnX/g4SjEe64/RYquUUqVZPMyDESyQDJ1h4sLYKq+dBCIXyqPJNNtLiQxzAlPkUhV8hjGhXi0Rj+UADNr+PzKXaOQzRKOB6juW8thWKFQNDHYrbAvqOnePDBBxkc9LIF3wlYVSQQ9vnILWQYFWBFdEaHJ5ipGDz38h4ak01E4yla27toaoiyZk0vQtGwUJF6gFhDlMa2Rtb3r8WnqlTyaWYnZ5g4cZpYJEDZMrn+vbcSScSxSiUq5QrSNJClIv5khLakYGB+DjXkZ4F5ps15wsEE89OzPP/Eabr7GkmmEoS0CNmFPE1tzSRSYTAhELbnCZKWxDBMqhWLhZk05UKJkakCG7qaMSslNq7vIJGy04Q1RZDOZNnz9Es89dATFCsLrN+4nrHBMWZOnuTlpyStTU109G1i+ugYCzOTbOhbT6VQ5IFvfJuWpg6GT5xiejHN3ideIpibYC49QVUL88H3fJBg4zwNzY0EZYhAUiHR1kO5Ys+1IIVA13WKRZPJqQKhoI6mwszCLEOjw4QCCVLJBFg5RodHmZkeZfDINDNzo7zn9ns5PDxEZk5SkY0r+8J4AFYZCUghCYdAL00zNzFFXBekWnQSgRJ+Y5ZIJMLzT/8T16xfQ0uwhKr7yC5m2bBjIyGfwK/60ZIq0rJIRkO0tPaSz+bBNBGLZYzFRaZm0mRyeVq6OhkaneTxp55AyDK33X4n7/vpD2CZFjK7SKlqMj06SSIY4OptG4gEylCpMDY9RKFkkJkLMz5UoL2phXgsysjpEQwhiTakePnZ/Tx5/wNI8iyWysQbWlm3ZTuvvHyQ9Rv7aE0FCVQDzM5kmJnOs3HnFirDe0iGArz/3/w2De1NFBcLWFISikaoSklxao5qLsejTzzOfX/3t5SqFeIN7azbvIOpqXn6kwE61u9gx+5dVLMWx1/ZR0NM5/TQNN2dQTR/B8n1WxkZnmB6bIib7vkwwWiExUyagYFZWjo6KOWzpBfmuWpHOz5dI+CPsevaXRTmO9nWP8cjj79Ka1+SoYVR7th0A//jz7+00q+MB1YZCYxOj/GZv/1zbl/TRTE9z8lskbwluWXDJtpaWnnu1dd4/NAejrY1szg9Sjgep1IyoFjk+MlTNPf00t/TiSoMYqEA81NThKWP7k39BBN+svMTqFVBSyxONKrQEvOxq6eDwuw00WyazNFXUS1JpCnFyw8/yqmBYXbfdisdvc3omXHyU1McemYfZamRGWinobUDX88OUruTdK3vRvNrPPWjpzm2/yWef+kJNvR0kyssMja4n70HXuRnf/GT9HSlWDRzKA0N+KwqH/uF90PFZPTVHnzhJLqlcOjFg5gywOTENCEd8sUi48On2Pv007x09DX8WoD+Nf1cfetdbLr6al5+/gVSVo6EMkR67yKVaBdHjhyke1MfT73yMrtLPRQrczTlqkzPpdnQ3YPIzZNsCDHv8/Pdb36PxuYA9977ISZHprn/5Pc5eOgA2zZtJpmKUpIGqqKTbO9gPicwpe0zoOqtPPROwKoigY5EgmZ9kVeGx7ipNci2cJgbrr+B3p4eTEtl06Iff0AjIvL0dbWxbve1DB05BmaBG3esQ/H76OlpJt7Ti1Eq09DVRXHoBPNz44RkkoXJSUZPnqBkmWy9djeaNAlWZylbUxx8dYhwsIlirsDmq7ZTyiywODvFyddeZOqYzuTEOE1NKbCgkMkwrwbw+QOMa8eoVOeYq+YZGx/nkX/6HieHRzkyPUDZKjC1OEsq6OPaneto9RdR85O89IOnqfhVRk+PcdPtd9HW1stjT79IX3MnmYUMDzzyIJYIkEgkqWQzZBcXaO9rYWF+mK1tLWzdtJlUqoHBw3tYMMooZo4v3/8looEgvQ3NaOxn05bNjAzNEW1pJxhuZ/dtNxBu7mbTzn40n4IvHCCfL3P6lSP0bljL+PBpvv6NbzA7vsDew/uQ+HjqiUeYy4zj98fo7u5HVwMEggaJYIp4Y4TDx46s9CvjgfMkASHEaewBYSZgSCl3CyFSwNexZ/U+DfyUlHJB2G7mPwfuwR5I9ktSylcvfdNfj80b2vmTn7+FV189xuOvHcCXSdMiJxg9MElLWxc7121mXev1NHS3Uq5YdEZ9tO7cRLVYwZ9KEWtM4o/4EFYeXVdQRAUl6UcNhSlXFRJdnZw+egSh6UycHGE6nWN8fBx/yE8wnECXClVD8qWvfZOnnn2WZDLFdCVPtViikMthmAZ9vV2oAjLTQ4TCgiOvPMf1u6+jpSPJ1NARtjUHKU8ZTGvgq0zSo1UYzxWYGT/C9x8sM33fFPdsXU/31u3sfM+t+DWIqUUiskRpcYYffP/7pNNzqJUscyOSo5NjrO3uYeLgMKJYpSmaoLG5mXgyRWAujRFQ+Po3vsqpoVFu6Okj0Blkw5YdzI8epWqF2HHDrezatov+revQAhoIg9MnxmhpbmV6bpHXnnmSZKqZplQneiDI3NwhDEtQKmaIhgLoaphIdA1G0eDE+CtUK4skw820NPZQLDkTlHlYSVyIJnCHlHLWtf9p4FEp5WeEEJ+u7f8+8D7soH8/cB3wudr3ZYegyL7Dg1iLs+xe18e3XjzA914d4a4bd6E1N2D6c8TMHNFQN/6gRXFxjrZtO1GCSfxNSZBQXZwH3QeWidLURjDZiBJLEBAaViHLjturaJQJRKJUq1UWRwahOkcwEsUfDFBZ1GmWQVqyrfiDIW6+YzvJjiaoaIR6tzAzeorSqQEUoZDJpHn31h0Yms7BEyeozM+RGZ/g2p4O7r3xOg4fH2Ihk+G9fZ3osSjdTZ2Uq5u4ak0L7TuuoXnLFoxcFn8wyL19nRjpBUpWhrm5ecaP7aVSNunt7OD4yDBNqRR33f1uFrIVmnp7CcejXN3ZRiDYSDzaxg27u+lvaeddd9xK2K8yFe/myNgML+8ZIpnsI9k+w749+7AyWbILGe6+50O0N6T4f371U6h+hdMTsyjBCN/5dpWh0/s4PTLK7FwZQ1pY+VPEQ42sb24jlthMtKGZjtY+rFeOc2RgaDleDQ9vgosxB+4Fbq9t34e9RuHv18q/VFuJ+AUhREII0SalnLiYhp4P1scUNjf7eXq4yleeOIhWKiHigs9+/UcsGhIhJU1+DdSvcffmBrriQfp3b6O5twelauKLhBkdGqe9I0nLui5kVTLw4j7USAOVsqA4N8nJo4Mk1m+hbef1JGSRXKFIb7OKb/4UQ0PjhEI+Th86gMhnSc9N8if/6zC37ejgrvfeTC43wrEXXmH0xDDveve72LRjK3k9Qml2nqopCUXi3HH7XTTFIjR2dbH7xjlefvYZtPw8oY4UifZmjhwdYD4bpQ0FLdGAlkgxdeI4owPHCKkqB0eGOXb4OIV0Dt0XoKJpZNUAa5rWkmruZMvVHfSvb8HIzZJav4lvf/khOhoauPWmO2jSVSoVi7LiI9nWTEshTTTsI+LLcurQINs2bSEWjFLJZggEfCzm5gkGQkzPTJKdOknAB9v6FFo/+EGefrYJo5QjoPjIZmZoiGsU9AgL1SqHD+6hUpwlV/S0gHcCzitjUAhxCnv4twT+Skr5BSFEWkqZqB0XwIKUMiGEeAD4jJTymdqxR4Hfl1LuWXLNTwGfsvf0q18/2vzCsd0/ztUNaUI+HQ2VWBASkSBTiwVC/jDD2SpzxRKjmQqKZaII1Z5PP6ATliYhXdAlDbq7I1Q1PzNpg4mZHLGWJNlimbF0kbimoZTKtEUEJVOlMa5yVU8Cw6iQrWiUTcFrkwUaQn6sSJh1UY2rbrmZLdftQk9ECPg1FtJZkr4gwXU7UROdUMhjoiCERmF6mnBzK9nsIqqic3L/PmKyRD4Q5LXHHyXu07n6g/fStvN6hKJiFHMce/5Zjj37KG3d3Tz58jFe23uYA8MjKNUy29Z0s27dJo4cH6C0OMe7br6RdX1ryFdKDKUXmBvKcNMtt3By8CRf/t43GZuZorezl83dvXQkm9h50w4i8Xb8qRQtHa1omRyLp/Zy7MjL9KxZS9fmHQQaWnnyH+5D1cNU/SGKhsHE4iKzw3NIKZhcnGRifJSyKVCBNT2tFDNpHj5UYTFnvNW/1cMlw8VlDN4spRwTQjQDjwghjroPSimlEOKC8o+llF8AvgBO2vDFY6xsUpwwKGOwRYFGXWAoed631UcoaNDts8gHgozO+7FUhY1d7bSvX8+xUxPEAjrz4+OIwgLhcIjjk3lULBoSYULlLLvXdzI9t0hZDZIplZkYX6C3tZGqWmbPlEljIoLm8+HXBbc1p8iXDaIhwcZ1a+jf2E2orZ34jpsRZoWoVcaSEYTig2oVgiEEAsUfIBTQKMzO4G9sxVicZ+tt1yCFguILsr6vk9njx4hGQ4hihnKhxOCPvs1Lr54gZCxy30OPcnB4nFAgwNzEOD3JRg4d2ItcXKBSKtLR2swjjz3Mg5ZC1pJEmxq4uncDz774FD98/hkGRk6TCgRoDSk0hSXB8mmUyRBDJ47y/g/dhZKZYt8Lz/HwUy9QzOfZMXUaXyRCpFykqaeNw0dPceDoS9x2y03cffuNDB05TDGb48Xnj9G+uYeWSIjGUIiwUBiYCPLI0Qnqi5Z7WCmcFwlIKcdq39NCiO9gL0k+5aj5Qog27CnnAMaoT1UH9tKAyzKbZLtf0KSAPxSi3w8lVefZqUVOHzHYEatgZPKEmwIczFu0N8TITsxTOfA44YBKT2eCRDJExu+jYJS4+13XMD0zS6qjm9npeSIBHzuvipBKxljIzPLCgQH2H5ujI+Yj1RShs6+P+bLC0MlhzEqReFOKzevaWXvV1cQaGjFHTmEk4qhKFFmdR2npR/iTyPw4FLNMnRwkGg4R6d1EuCHK9MAL7PvREyjSYsd124m2daKrYZL969BCPqSuIhWVps5Wqg8+wf7RYSaHxynns4zPzbMzGeL0wiSpYICbN3fi0yTHJxdoa4hhFsuEAxHaeltYTJ/i5OA4LbLCpvWdoAYJBnUOnzjET966i/6eFD2aydDRvRw/tIeXXt3HniPTNCaizKQXGV0ssHbdOixFEDctlHKWowcP0NaYorezHdOo0tceJ5edY2R4gMWZBRayFd73Ex/iL579J8h4k4utNN6SBIQQYUCRUmZr2+8B/hPwXeATwGdq3/fXTvku8JtCiK9hOwQzy+EPAMnRcpVTAtaLCnMl2JzU2BrVWNukMjlboCgUkmhsjSgcHF1gsGjQ7FNp0QXpXI67dq1hy8ZtVEyDmew8/kiUrVdvRyZiLGYWsYZPMzE2xneeOMA/nZhF1wSF0wa3tgRZ093G5s5OokYBv6rR05XELC5SmpmlqbEFrTFJ/sRBRvYfIBJvIBR6Gn8wyvzsNIVKlYb+zYyNFemITDPzxLc58szLjAycQFYL9Ebmiei7URo6qczMsDA2QSZX5PCewwTVMlpmmFalyvrtKUS4n2RfIwN7jzIwoLJ5wxo69AInxiaJaDHSi9P0N+kcHRzlvTd1k+pp4wkzw9ZNO8hn5jkyZ7H3xBiGmeeF/a/xnedfYXo+TV9zI4V0joVMjt7mBLs2JilpQWKiRGNljmQiRU7xcefVWyiVKmQGXmPju24ilytx/8OP0ygK+CcHkAtFtm9uxj/9GBS8JcjeCTgfTaAF+E5tcIsG/IOU8gdCiJeBbwghPgkMAT9Vq/8QdnjwBHaI8JcveavPCYsqJaoS9hcMTOBgftFeGHOuemaaayObtefMV+zgVLpqkS1BNWug7x9h16LBvrFZzHSavpYw4wf3I9QAo1PzFC2Bz68zPFckJi2img/DDxtakgweOMTxZ16mvSfIjyYlza9CW9zPjYrFcHqEWCTI0NAkaiBAIBAknc5SKpYZGx4n4NdIBH009Pbhi0Dbth0EkcTDKrGWBnILOUrHB5i3jjOw/yCVxTQ+XWV03wCncpKyoSENg53be2ne2EhY1Vm3poOFbIFCGKqNzWzq30xO8TP33Qd4+Ngk6/p6eWzfcbZ3N5NbzPHkqwcJJuNULZUGWWR4Ls3R2TS3rGlk7U0baA1VyCxqNPdfh5krEguH8TV3ovlDFOemSSQTRKJxlKqJaRXxBQNohUly+Ry3rG+F0jwL1TD+chFrapLRbA6j0oA3iGjlsYqGEpvAcaB6ZkSboL48l4W9lJdVGwRTtOz59lOaQiysUyybmFWDpA550x6uG1bsmXJnDchJCPsUZisWBhALatzV18JiuczM9AITWZOwKljUBItliy6gTwGpC7Y2qmxYE2fDHXeSircRKE/R2BLAyBfZf2CI6ak80eZWxqpVglae9a0JFMUg3pnixGvHiLdvpq2tmYA/gBnwUyxXUIvzWFOjHB5dIFcIMr2YI1M2WNPfQ2ZyEiUQJZJKsmvrOkyqTKTzDL56gLb2RsqLWRp7+4i1hDAy0yiZIl3vvYPc5CCzAwsMv7if+bFRIuEQ0ZY29ICPxWKOoKzQ0d2DpehkFxbo6O1Gqjpta3sINzdSmZpiNp0hPZuhOaaRr/o4fuAQi4sFTkwsEFRNquUsekilWtT5b3sbKFirKl/tHY5zOwZXJQmc1z154znulh5zTwF+vvADjcJeFj2mCcarki1+wbUpja4mHb05QEXE2HNoElPzsWjAK5NZEork5u4gCU0hHpCEwzoiHCQZThEJ6IyNT3DgZJ72sElPV4KyKUmlWpnKZwnGk0SsKtnFDBUtRCwgaW9rRA+E+OGBKXxGgaguaOpIkYqFqZSrZGYzvHJ8htOTWTSjQotPYTFXpqEjRdgnOTm+SFgICPu549q1NMZVxucKyHSGtrVddOy+htSadcxOLhIzxikXhxjYP8vQoUkWpjPokQiLZZP5XAkr1UDMyqBVTGRTO//lCYGB+pbP0sOlgkcCywa3JuIDopqgSRG0+xSyQhLwSUYzFsKw5+J3VuoNAutUWBuElITTBZjTBb6AQrUiUYVAMU12xvysbbXoaAwSjoQwqlUisTj5xSzZ+RxrejpQ9TjxjiQNa7pZODXM0cFJ1EiI9EKWYEDFsgymxtL4/UHG5kuUzDJhypQXK8znTMYqko72EOFcGSNvcqQqaPQpvHtLiPauJro7ujhwdID0YhbdH+LAWI51EZUpKdh7usB7dvUyMjSFrsCRvOD4ZJqqCb1JjVu2tXIynePvDnRiWp45sHy4IkmgGdsTkEaoca7evoGb1mxgejrDy0N7ODF8mrrIhoEc/nCcro42wrqOKFuk01nyuTRZq4gp32AJXSkxDOOcmoKzHJdGfTUfOLd24QNSCug6NBj2tOKNwIioTflt2suChxUYk/aagmdartiTiYaBba0hmpMR9ICGritES0XifsFsJsPwjEUirJIMqozOVmiIhdE1nZywuLpL4eRshXgyRX52gaBucXw4T1XVyUhBLKIS9KuE/AHmLT8jY7Ns6YlwVX8r82PTDKeL5EWAudlFbt7WyOHJHLGgn4aedr7zwH66wxZXbe3knjt6eXk4z8//tYFpeQlDy4dVP7OQs+6NG7PYYqgjzTyHjh5j29V9RBIx/DPxc5wPSImUkr61vWzq6iOzUKSlPUVDdyPhSBJfIER7ZwfxSNiejceA3Fyal196gYXqG4S7yhZTR0Y4OTHEVHkeLEklk2MuM4+qaGQLOSrSOrOMOCZMSoklLIakxCehbNotHAUUq7YcmLT9HRq1qchq20dHCjBSwMTWLtoV8AthzzSkwO1xncm5EsW05OX0IrGYzq3bmhmbyCFyZY4cH6bqh50bEjS1+nn5VIloSCWlKGTSJifnZ5mtSjRF4YXBPNOVERZLFRoFxPU8z05XePVHY2CBaoH+/DQJwLIEwXQW05Q8fCCPafkv7l/u4ZJgFWkCFeyAxLmST97MA/DWUBQFVVUQQkELRNiy4zY+8x//Fa0tKSLxOPFomFAwiBAqqqq6Jga1AAVpGUjTwpQKlrTAlJTTWWbmp9E0H5lclgqWPY9gxUAgKEzOcXpshOMTw1iuthfn8rzw4nPkqgUsbMF31gjUcWkbEpSSwcL8ArlChmqNZCwgoQiS0iJrwYKUKFLSKmCtZutOaQsGgRY/mAaEApDSwTBtraZoCCYKknVBlTImB4pQVmGhCiFhr4CsADFs0goA64CCDwrSnhV5shpjht63/T/x8HZwJZgDatqWCHMGZAVbRCwu/TIXAp9PR1EU4g2NNCZjRMIxNC3OTTddTW/vBm655Tq6e9oI+AOMj43R0tqCz+fHMAzSmTSNDU21BVLtCTxLlSoDA4MMT2a4585rUV1T/UoJlWIBXzCEaVQoFQuoeuDNm2iBWagyOz1NJjtHVVpnGUqVxQIyV2Hk5EkGp0coWNUzfgznacl8laG9hzm9MEVWls86pkgQEoxShUrVQAiFatWgVClTKRUxLBMLkyrSXnOA+mKm9kViWLL7Yv4JHi4Yq54EquAPoq27DbMyh5x8DfIZsKrYrrc89cWyLyd0hNpJIBBl7domIqEEA8cO0LcmRjAUplIpMTk9z7q123n3e9+DFgzz3HN7OD06z/GDe1FQ+MQvvI/+/k46+zZRwmTq5CDZuVkCeiNaNEG4qZWP3H0NjfHgec/8Cxc2W7CsrRhSLZcwpcSSEkezEUIgDQshBEa2RKVURgiVY4cO8eAjP6KrqZndt16HIUx7FKH7whZYxSrPPH+Qf/+fv3hBT9bDxeJKIAHFRMS6Eb5W1FQDVcOAgoEoV5EyC+lDoBXAyIA0QCpAjktLDo6Aadj9n6hd390Pq9hZCxUQFZDOOY7CXruC7kciMKumXS4CoGxECUbYecsu/uD3foX33biBoE8DITANg2ypxNR0joNHh4gl/axva0D3+2luamAuncGvQCKZrM1OLJFSIKW9ekg2u0i+UKFaqTI7dIx4RzcHjw0yMHicgROD+FVJc3OKZLyJUtFE9WlMTC9w5NgwhdkTlEslXn3tGI2NDfyzX/9lWlrauHr3LlLJhtqaagaarhOLhvnOt57gE5/4t5fwuXt4a6xyEtjU38GnPvmThOLtTM5neeKZV3j5eIFStYQS9mMJH8boIFQqEGuGzAJUCqAUQBZBN6AyBPJ8UlkVzu2IBNsyB9tPvxZ78OU4UFpSz4kXvEHE4Q2RADpB7cSfaGTXdWv56Ls7SEZjPP/SXp59eQ9jE1EKmRyq6sevLOCPBNmyqZsTo2X6uhq5dmeSzRs6gConjp1g38HDVC3BxESW2dlZCrkyZnUSRdMolcoYVWfyD5W6z8Wt4J8rsyIEokgoGET1x0BLgWHhD6foaPFjVDQOHZi8wN/u4eKwykmgs6uNj//s++npaCaWCFMx4aVXTvH3X3uEQq6A8IchEEWWArarvVIGawoo2z1s6xb8LU34KpPk5waxFkfAyIGZBqtAvSfX7I/eDMZcbRXPIEIkkLIFKmkQftAUkI1AAoxTwDC2SVLE1gwcLQEuPLdBBeLgb4fARhAW5PdCdRxbMP1AFDs2MIXtNI0BvRDZBvnvgzyXALrNhMv1XjjrG6SA9st0Dw/nxiongddfVEHTVEzTQlryTBlqACwFLIndq2mg+CEYQwv4CTQ0UyqZmKiokQQYZYyR/cAiFE/bAocJShJEChHtJ9HTR1trH3PlKgsTg0g9hImCVShBuQL5HFTmoTIG1QVswSxhC7PTu1rYQy0uVDMQoHSB1gyVU0DGbt9ZmQhBbPOjHSI7IajAwkNgTPJ6YffVPk57zgUfNnFdzLvTiU0EHpYPqz5PYAmkhVG1XleGkXt9XSsL+VmMPOTmjgICVBVTqS3JW61NuE8SZBb8TSAaUWSJu25o5Td+7hY27t7I1GyOz37R5IU9Q+SKE1jSJFsoYZgqhJOoa7chKxWs7DhkMmDpoJShMgrBbqhMQ2nAvgcVoFxr4LnUbae3N8Eq2SRDCltAM3Z9kQARBstnt50AVDIoPZuRXb+HPP5NyL2GLdCOuu8DekA0gBzFzlxwppcU2BpFj30txQTLqeOYSI5/5a0IwssUfKdg9WoCywShaiRjYXSf7QvwBYP09nTg06pUK3BqOM+2Hb2Eg0GEohBu6aYp0UN6ZoFjJ48xP3GaYwOv0Lv9bjbu3AWVEiN7D3Lk0D5KxdOo6gLhMAihky/7MBDg0wn7mon6E1SNBRZmRrAsFZsYqti9uAlEQURrZVHbRDEtwtEQvsYORPN6rIVxSvOjlEsjyGoJqotgKmhaP8FwEyoZpJxEMImUGvlCHsPIE483k8kUsbWMHKgRkA1gLdr7VLH9I1lsEyhYKytgE0Qn9hAuD8uHK80ceIdC8/mIRyP4fKq98KcvRLlksW1zJ93dzSiBAH5/EL8WZHxkgKg/xFU7uli/vpvJTIJMuYCiC7au7WD9mmaMqsmPfvgy33t4D/nFPEcOTTCXXaRs+lBTcXzRVrRSmv6GEh0dTbz3Pbu57votNMaDKIEQpiU4eGSCg6cXOXb4FOm5Ea67ZjsbuyNs6O8iEo9iGAYKJoZpcuDACWamT9HS2MRMeoFSXvDSsyeYmJ/i1f2nmZmdBcNxetqp2DBHKBREDa8jOzdaC9vW1jP3sIzwSODHDG4BMUnEw0RjcUASTya5eucGVKOMovtoagjS19dKa1sb2aJGOmuQzRZpbUmyYX038YROc1OCZDJSX8XYNFG0ujW49D14s1wCd76Bsy2l5OjxCR5/7BUe+eHj5HITbNm4k/Wb1uPzV9m1azuKVPnbL/+Ib37nh0wMH8HWXDwsHzwSWPVwBNcf8BMOBey5CBUVXzhEYyrO5vXd+P06oBBPxrhmVz/JVAKAaDzK2t52VM0e2qtpGj6fVkuBdt6RcxCDlCBNUOwEZlkbewGSXDbL+MgwG7duB6BazKHoAfbuHeDDH/0tRkdmLuvz8LAUHgl4WAIhxBniCIaCtDQnURQ7I7Cto40d29aiaQoIQbIxwcZ1vbS1plA0FX8wQCwaoq05dYY4FFXBp2sIURszIe1EZCEEpXwOXyCIkCZz6Tw7dn6c8dFlmHXOgwtXWnTAw1ui3mtDPpfnZC5/5tjAwBBPPfGCq7ZAURzSEOgBH+FggIZUDFVVQdNpak5wzY5+NN12knZ0tbF71wY0TaVSKtHU2kxrTKdUgWq5jId3BjxNwMNlgxCiNhDK1jZiiShNiQiKrnH8+LC9KKmHZYSnCXhYZkgpMYy6oM/PLjA/u7CCLfJwLngZGx48XOHwSMCDhyscHgl48HCFwyMBDx6ucHgk4MHDFY7zIgEhREII8U0hxFEhxBEhxA1CiJQQ4hEhxEDtO1mrK4QQfyGEOCGE2C+E2HV5f4IHDx4uBuerCfw58AMp5UZgB3AE+DTwqJSyH3i0tg/wPuygfz/wKeBzl7TFHjx4uKR4SxIQQsSBW4G/AZBSVqSUaeBe4L5atfuAD9e27wW+JG28ACRqS5d78ODhHYjz0QT6sFfL+lshxGtCiL+uLVHe4lpyfBJ79WKADmDEdf5orewsCCE+JYTYI4TYc+61Ajx48LAcOB8S0IBdwOeklDuxJ8r7tLuCdIaNXQCklF+QUu620xi9xEUPHlYK50MCo8ColPLF2v43sUlhylHza9/TteNjQJfr/M5amQcPHt6BeEsSkFJOAiNCiA21oncBh4HvAp+olX0CuL+2/V3gF2tRguuBjMts8ODBwzsM56uH/xbwFSGEDzgJ/DI2gXxDCPFJYAj4qVrdh4B7sBcGLNTqevDg4R0KbyixBw9XDM49lNjLGPTg4QqHRwIePFzh8EjAg4crHB4JePBwhcMjAQ8ernB4JODBwxUOjwQ8eLjC4ZGABw9XON4hyUIiCxxb6XYAjdjrbK80vHacDa8dZ+PttqNHStm0tPCdMnzv2LkymZYbQog9Xju8dlxp7fDMAQ8ernB4JODBwxWOdwoJfGGlG1CD146z4bXjbKzKdrwjHIMePHhYObxTNAEPHjysEFacBIQQdwshjtXWKfj0W59xUff6ohBiWghx0FW27OsnCCG6hBCPCyEOCyEOCSH+5Uq0RQgREEK8JITYV2vHf6yV9wkhXqzd7+u1yWQQQvhr+ydqx3svRTtq11ZrE9k+sIJtOC2EOCCE2GtPgLti78fyrvMhpVyxD6ACg8AawAfsAzZfxvvdij0/4kFX2R8Dn65tfxr4o9r2PcD3AQFcD7x4CdvRBuyqbUeB48Dm5W5L7XqR2rYOvFi7/jeAn6mVfx749dr2vwA+X9v+GeDrl/CZ/GvgH4AHavsr0YbTQOOSspV4P+4DfrW27QMSl7Mdl0XYLuDH3gA87Nr/A+APLvM9e5eQwDGgrbbdhp2zAPBXwMfPVe8ytOl+4N0r2RYgBLwKXIediKIt/R8BDwM31La1Wj1xCe7dib2AzZ3AA7UXelnbULveuUhgWf8nQBw4tfQ3Xc52rLQ5cF5rFFxmXNT6CReLmjq7E7sXXva21NTwvdizRT+CrZmlpZTOYhDue51pR+14Bmi4BM34M+DfAFZtv2EF2gD2tPk/FEK8IoT4VK1suf8nl2WdjzfDSpPAOwrSptJlC5cIISLAt4DfkVIurkRbpJSmlPIq7N74WmDj5b6nG0KIDwDTUspXlvO+b4CbpZS7sJfS+w0hxK3ug8v0P7ks63y8GVaaBN4JaxSsyPoJQggdmwC+IqX89kq2BUDaS8s9jq16J4QQTkq5+15n2lE7HgfmLvLWNwEfEkKcBr6GbRL8+TK3AQAp5Vjte5r/v72zR4kgCKLwV4k/iKCCmYEsmIqBoYGRwcabGXoKETyCmWcwMFCMdQ+ggn8rG6ixhzAog6rByER3u4N+HzT09Az0g2reTFcPFFwQplg6JsXrfNQ2gTtgIzPBM0Si56qwhuL1E8zMiNqOY3c/qaXFzFbNbCn780ReYkyYweAXHZ2+ATDMt9KfcfdDd19z93Ui/kN33y+pAcDMFsxssesDe8CIwjHxGnU+JpFQ+WcipE9kxz+AoynPdQZ8Al+E4x4Q+8kb4A24BlbyWQNOU9cLsD1BHTvE59wz8JitX1oLsAk8pI4RcJzjPeCWqB1xDszm+Fxev+f93oTjs8vP6UBRDTnfU7bXbi1WWh9bwH3G5RJYnqYO/TEoROPU3g4IISojExCicWQCQjSOTECIxpEJCNE4MgEhGkcmIETjyASEaJxvxlq+KS5cQzgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(images_arrays[1])\n",
    "print(images_arrays[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a baseline model and your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining baseline model\n",
    "X,Y = data.iloc[:,:132],data['target']\n",
    "baseline_model = SVC(kernel = 'poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOX (Including weights pretrained on COCO)\n",
    "\n",
    "# The head (i.e. the connection between the YOLOX backbone and neck to the rest of the model) is by default just an IdentityModule.\n",
    "# This head should be exchanged with some torch module that performs the rest of the function (in this case classification)\n",
    "# The head module should be a torch module expecting an input that is a list of 3 tensors of sizes:\n",
    "#        [torch.Size([BATCH_SIZE, 64, 80, 80]), torch.Size([BATCH_SIZE, 128, 40, 40]), torch.Size([BATCH_SIZE, 256, 20, 20])]\n",
    "# Note: These sizes may change if the `opt.input_size` or `opt.test_size` are changed.\n",
    "# Each of these inputs is a different output of the YOLOX neck and represents the features learned at various scales.\n",
    "\n",
    "# The YOLOX model expects a single tensor input of size: [BATCH_SIZE, 3, opt.test_size[0], opt.test_size[1]]\n",
    "# BATCHSIZE is the Batch size\n",
    "# 3 is the number of color channels (the YOLOX is pretrained on 3 channels. Even if the image is grayscale, convert it to RGB\n",
    "# opt.test_size[0] is the number of horizontal pixels in the input\n",
    "# opt.test_size[1] is the number of vertical pixels in the input\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_sizes:List[int], input_channels:List[int], num_classes:int, hidden_features:int = 128, dropout=.5):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc0a = nn.Linear(input_channels[0]*input_sizes[0]**2,hidden_features)\n",
    "        self.fc0b = nn.Linear(input_channels[1]*input_sizes[1]**2,hidden_features)\n",
    "        self.fc0c = nn.Linear(input_channels[2]*input_sizes[2]**2,hidden_features)\n",
    "        # Concatenate the three outputs into one linear layer\n",
    "        self.fc1 = nn.Linear(len(input_sizes) * hidden_features, hidden_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        a = F.relu(self.fc0a(torch.flatten(x[0],1)))\n",
    "        b = F.relu(self.fc0b(torch.flatten(x[1],1)))\n",
    "        c = F.relu(self.fc0c(torch.flatten(x[2],1)))\n",
    "        x = torch.cat([a,b,c], dim=1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = 1e-3\n",
    "                m.momentum = 0.03\n",
    "\n",
    "def generate_model(opt, hidden_features=128, dropout=0.5, freeze_layers=True):\n",
    "    return get_model(opt,\n",
    "                     head=ClassificationHead([80,40,20], [64,128,256], 5, hidden_features=hidden_features, dropout=dropout),\n",
    "                     freeze_layers=freeze_layers)\n",
    "\n",
    "\n",
    "model = generate_model(opt)\n",
    "model.to(opt.device)\n",
    "\n",
    "# Check if frozen\n",
    "assert not any(p.requires_grad for p in model.backbone.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Run a training loop on a training set with both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# Run inference as a test to make sure network runs (i.e. all tensors are the right shape)\n",
    "# Use only the first 10 images, for speed's sake\n",
    "model = model.to(opt.device)\n",
    "with torch.no_grad():\n",
    "    yolo_outputs = model(torch.as_tensor(inp_imgs[:10]).to(opt.device))\n",
    "    # print(yolo_outputs)\n",
    "    print(yolo_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Take this pre-generated model off the GPU so we have space\n",
    "try:\n",
    "    del yolo_outputs\n",
    "except: pass\n",
    "try:\n",
    "    del model\n",
    "except: pass\n",
    "try:\n",
    "    del gs\n",
    "except: pass\n",
    "try:\n",
    "    del net\n",
    "except: pass\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            print('cuda', obj.is_cuda)\n",
    "    except:\n",
    "        pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.1727\u001b[0m       \u001b[32m0.3056\u001b[0m        \u001b[35m1.5339\u001b[0m     +  1.2163\n",
      "      2        \u001b[36m1.6946\u001b[0m       \u001b[32m0.3681\u001b[0m        \u001b[35m1.4758\u001b[0m     +  1.1879\n",
      "      3        \u001b[36m1.5372\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.4065\u001b[0m     +  1.1808\n",
      "      4        \u001b[36m1.4094\u001b[0m       \u001b[32m0.4167\u001b[0m        \u001b[35m1.3545\u001b[0m     +  1.1912\n",
      "      5        \u001b[36m1.3520\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.2979\u001b[0m     +  1.1819\n",
      "      6        \u001b[36m1.2063\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.2620\u001b[0m     +  1.1791\n",
      "      7        \u001b[36m1.1157\u001b[0m       \u001b[32m0.5972\u001b[0m        \u001b[35m1.1241\u001b[0m     +  1.1801\n",
      "      8        \u001b[36m1.0258\u001b[0m       0.5347        \u001b[35m1.1148\u001b[0m     +  1.1797\n",
      "      9        \u001b[36m0.9142\u001b[0m       \u001b[32m0.6389\u001b[0m        \u001b[35m1.0050\u001b[0m     +  1.1795\n",
      "     10        \u001b[36m0.7633\u001b[0m       \u001b[32m0.6528\u001b[0m        \u001b[35m0.9820\u001b[0m     +  1.1802\n",
      "     11        \u001b[36m0.6698\u001b[0m       \u001b[32m0.6944\u001b[0m        \u001b[35m0.9160\u001b[0m     +  1.1804\n",
      "     12        \u001b[36m0.5584\u001b[0m       0.6389        0.9204        1.1804\n",
      "     13        \u001b[36m0.4995\u001b[0m       0.6806        \u001b[35m0.8571\u001b[0m     +  1.1817\n",
      "     14        \u001b[36m0.4311\u001b[0m       \u001b[32m0.7083\u001b[0m        \u001b[35m0.8385\u001b[0m     +  1.1816\n",
      "     15        \u001b[36m0.3484\u001b[0m       0.6736        0.8805        1.1807\n",
      "     16        \u001b[36m0.3363\u001b[0m       0.6389        0.9479        1.1822\n",
      "     17        \u001b[36m0.2850\u001b[0m       \u001b[32m0.7708\u001b[0m        \u001b[35m0.7794\u001b[0m     +  1.1786\n",
      "     18        \u001b[36m0.1925\u001b[0m       0.7569        0.7897        1.1803\n",
      "     19        \u001b[36m0.1830\u001b[0m       \u001b[32m0.7917\u001b[0m        \u001b[35m0.7707\u001b[0m     +  1.1808\n",
      "     20        \u001b[36m0.1429\u001b[0m       0.7569        0.8626        1.1799\n",
      "     21        \u001b[36m0.1395\u001b[0m       0.7431        0.8029        1.1807\n",
      "     22        \u001b[36m0.1135\u001b[0m       0.7153        0.9016        1.1811\n",
      "     23        \u001b[36m0.0864\u001b[0m       0.7292        0.8802        1.1804\n",
      "     24        \u001b[36m0.0855\u001b[0m       0.7083        0.9204        1.1819\n",
      "     25        0.0857       0.7847        0.8098        1.1809\n",
      "     26        \u001b[36m0.0642\u001b[0m       0.7153        0.9041        1.1802\n",
      "     27        0.0675       0.7708        0.8842        1.1809\n",
      "     28        \u001b[36m0.0549\u001b[0m       0.7083        0.9690        1.1784\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.25, module__hidden_features=64;, score=-1.057 total time=  46.6s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4303\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.4861\u001b[0m     +  1.1816\n",
      "      2        \u001b[36m1.7726\u001b[0m       0.3448        1.5724        1.1818\n",
      "      3        \u001b[36m1.5549\u001b[0m       0.2897        \u001b[35m1.4681\u001b[0m     +  1.1931\n",
      "      4        \u001b[36m1.4412\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.3435\u001b[0m     +  1.1876\n",
      "      5        \u001b[36m1.3422\u001b[0m       0.4759        \u001b[35m1.2965\u001b[0m     +  1.1823\n",
      "      6        \u001b[36m1.2501\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.2219\u001b[0m     +  1.1822\n",
      "      7        \u001b[36m1.1524\u001b[0m       0.4897        \u001b[35m1.1815\u001b[0m     +  1.1813\n",
      "      8        \u001b[36m1.0785\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m1.0868\u001b[0m     +  1.1827\n",
      "      9        \u001b[36m0.9770\u001b[0m       0.5034        1.1155        1.1824\n",
      "     10        \u001b[36m0.9051\u001b[0m       0.6345        \u001b[35m0.9923\u001b[0m     +  1.1822\n",
      "     11        \u001b[36m0.7878\u001b[0m       0.6138        \u001b[35m0.9315\u001b[0m     +  1.1845\n",
      "     12        \u001b[36m0.7407\u001b[0m       \u001b[32m0.6621\u001b[0m        0.9541        1.1854\n",
      "     13        \u001b[36m0.6265\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m0.8855\u001b[0m     +  1.1834\n",
      "     14        \u001b[36m0.5507\u001b[0m       \u001b[32m0.7310\u001b[0m        \u001b[35m0.8770\u001b[0m     +  1.1832\n",
      "     15        \u001b[36m0.5089\u001b[0m       0.6207        \u001b[35m0.8489\u001b[0m     +  1.1831\n",
      "     16        \u001b[36m0.4498\u001b[0m       0.7241        \u001b[35m0.7934\u001b[0m     +  1.1826\n",
      "     17        \u001b[36m0.3457\u001b[0m       0.7034        0.8559        1.1843\n",
      "     18        \u001b[36m0.3131\u001b[0m       0.6966        0.8239        1.1849\n",
      "     19        \u001b[36m0.2629\u001b[0m       0.6828        \u001b[35m0.7911\u001b[0m     +  1.1835\n",
      "     20        \u001b[36m0.2434\u001b[0m       0.6759        0.8919        1.1836\n",
      "     21        \u001b[36m0.1870\u001b[0m       0.6414        0.9403        1.1823\n",
      "     22        0.2088       0.6759        0.8900        1.1821\n",
      "     23        \u001b[36m0.1461\u001b[0m       0.7310        \u001b[35m0.7691\u001b[0m     +  1.1815\n",
      "     24        \u001b[36m0.1103\u001b[0m       0.7034        0.8114        1.1833\n",
      "     25        0.1106       0.7241        0.8883        1.1817\n",
      "     26        \u001b[36m0.0908\u001b[0m       \u001b[32m0.7448\u001b[0m        0.8269        1.1817\n",
      "     27        \u001b[36m0.0827\u001b[0m       0.7379        0.8464        1.1824\n",
      "     28        \u001b[36m0.0770\u001b[0m       0.7241        0.8893        1.1840\n",
      "     29        \u001b[36m0.0618\u001b[0m       0.7172        0.8379        1.1830\n",
      "     30        \u001b[36m0.0561\u001b[0m       0.7310        0.9136        1.1824\n",
      "     31        0.0607       0.6828        0.9953        1.1819\n",
      "     32        0.0785       0.7103        0.9402        1.1842\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.25, module__hidden_features=64;, score=-0.885 total time=  51.5s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4614\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.7340\u001b[0m     +  1.1897\n",
      "      2        \u001b[36m2.0504\u001b[0m       \u001b[32m0.3931\u001b[0m        \u001b[35m1.4937\u001b[0m     +  1.1951\n",
      "      3        \u001b[36m1.5408\u001b[0m       \u001b[32m0.4207\u001b[0m        \u001b[35m1.4335\u001b[0m     +  1.1844\n",
      "      4        \u001b[36m1.4614\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.3453\u001b[0m     +  1.1838\n",
      "      5        \u001b[36m1.3850\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.2901\u001b[0m     +  1.1833\n",
      "      6        \u001b[36m1.3055\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.2351\u001b[0m     +  1.1843\n",
      "      7        \u001b[36m1.1644\u001b[0m       0.6000        \u001b[35m1.1593\u001b[0m     +  1.1860\n",
      "      8        \u001b[36m1.0862\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m1.0934\u001b[0m     +  1.1842\n",
      "      9        \u001b[36m0.9490\u001b[0m       \u001b[32m0.7034\u001b[0m        \u001b[35m1.0176\u001b[0m     +  1.1832\n",
      "     10        \u001b[36m0.8843\u001b[0m       0.6690        \u001b[35m0.9748\u001b[0m     +  1.1844\n",
      "     11        \u001b[36m0.8039\u001b[0m       0.6828        \u001b[35m0.9232\u001b[0m     +  1.1832\n",
      "     12        \u001b[36m0.7410\u001b[0m       0.6759        \u001b[35m0.8706\u001b[0m     +  1.1839\n",
      "     13        \u001b[36m0.6542\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m0.8163\u001b[0m     +  1.1838\n",
      "     14        \u001b[36m0.5644\u001b[0m       \u001b[32m0.7448\u001b[0m        \u001b[35m0.7511\u001b[0m     +  1.1836\n",
      "     15        \u001b[36m0.4959\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.7289\u001b[0m     +  1.1843\n",
      "     16        \u001b[36m0.4355\u001b[0m       0.6828        0.8340        1.1847\n",
      "     17        \u001b[36m0.3997\u001b[0m       0.7517        \u001b[35m0.7221\u001b[0m     +  1.1841\n",
      "     18        \u001b[36m0.3342\u001b[0m       \u001b[32m0.7655\u001b[0m        \u001b[35m0.6662\u001b[0m     +  1.1837\n",
      "     19        \u001b[36m0.2541\u001b[0m       0.7655        \u001b[35m0.6647\u001b[0m     +  1.1840\n",
      "     20        \u001b[36m0.2453\u001b[0m       0.7172        0.7270        1.1836\n",
      "     21        \u001b[36m0.2143\u001b[0m       \u001b[32m0.7724\u001b[0m        \u001b[35m0.6091\u001b[0m     +  1.1842\n",
      "     22        \u001b[36m0.1697\u001b[0m       0.7655        0.6362        1.1833\n",
      "     23        0.1708       0.7103        0.8552        1.1839\n",
      "     24        0.1721       0.7379        0.7377        1.1847\n",
      "     25        \u001b[36m0.1509\u001b[0m       0.6897        0.9594        1.1836\n",
      "     26        \u001b[36m0.1344\u001b[0m       0.7724        0.7420        1.1830\n",
      "     27        \u001b[36m0.1189\u001b[0m       0.7310        0.9152        1.1838\n",
      "     28        \u001b[36m0.1008\u001b[0m       \u001b[32m0.7862\u001b[0m        0.6496        1.1838\n",
      "     29        \u001b[36m0.0810\u001b[0m       0.7724        0.7496        1.1880\n",
      "     30        0.0836       \u001b[32m0.8138\u001b[0m        0.6121        1.1842\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.25, module__hidden_features=64;, score=-0.834 total time=  52.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4882\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.7926\u001b[0m     +  1.2107\n",
      "      2        \u001b[36m1.7598\u001b[0m       \u001b[32m0.3403\u001b[0m        \u001b[35m1.4344\u001b[0m     +  1.2178\n",
      "      3        \u001b[36m1.5204\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.2580\u001b[0m     +  1.2147\n",
      "      4        \u001b[36m1.2676\u001b[0m       0.4514        \u001b[35m1.2300\u001b[0m     +  1.2159\n",
      "      5        \u001b[36m1.1408\u001b[0m       0.4931        \u001b[35m1.1762\u001b[0m     +  1.2180\n",
      "      6        \u001b[36m0.9304\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.0531\u001b[0m     +  1.2165\n",
      "      7        \u001b[36m0.7818\u001b[0m       0.5347        \u001b[35m1.0127\u001b[0m     +  1.2157\n",
      "      8        \u001b[36m0.6813\u001b[0m       \u001b[32m0.7083\u001b[0m        \u001b[35m0.8767\u001b[0m     +  1.2240\n",
      "      9        \u001b[36m0.5172\u001b[0m       0.7014        \u001b[35m0.8559\u001b[0m     +  1.2269\n",
      "     10        \u001b[36m0.3759\u001b[0m       0.6528        0.8974        1.2164\n",
      "     11        \u001b[36m0.3139\u001b[0m       0.7083        \u001b[35m0.7523\u001b[0m     +  1.2145\n",
      "     12        \u001b[36m0.2041\u001b[0m       \u001b[32m0.7500\u001b[0m        0.7845        1.2150\n",
      "     13        \u001b[36m0.1712\u001b[0m       0.7500        \u001b[35m0.7239\u001b[0m     +  1.2148\n",
      "     14        \u001b[36m0.1168\u001b[0m       0.7500        0.7385        1.2143\n",
      "     15        \u001b[36m0.0789\u001b[0m       \u001b[32m0.7708\u001b[0m        \u001b[35m0.7022\u001b[0m     +  1.2151\n",
      "     16        \u001b[36m0.0587\u001b[0m       \u001b[32m0.7847\u001b[0m        0.7313        1.2175\n",
      "     17        \u001b[36m0.0523\u001b[0m       0.7778        0.7509        1.2150\n",
      "     18        \u001b[36m0.0369\u001b[0m       0.7847        0.7735        1.2147\n",
      "     19        \u001b[36m0.0299\u001b[0m       0.7500        0.8386        1.2167\n",
      "     20        0.0324       0.7708        0.8298        1.2163\n",
      "     21        \u001b[36m0.0189\u001b[0m       0.7778        0.8221        1.2167\n",
      "     22        \u001b[36m0.0167\u001b[0m       0.7569        0.9174        1.2160\n",
      "     23        0.0200       0.7708        0.8969        1.2150\n",
      "     24        0.0172       0.7292        0.8958        1.2150\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.25, module__hidden_features=128;, score=-1.048 total time=  50.1s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.6080\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.5759\u001b[0m     +  1.2162\n",
      "      2        \u001b[36m2.1140\u001b[0m       0.3034        \u001b[35m1.4844\u001b[0m     +  1.2186\n",
      "      3        \u001b[36m1.6615\u001b[0m       \u001b[32m0.4897\u001b[0m        \u001b[35m1.3588\u001b[0m     +  1.2181\n",
      "      4        \u001b[36m1.4011\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.3072\u001b[0m     +  1.2192\n",
      "      5        \u001b[36m1.2417\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.1994\u001b[0m     +  1.2178\n",
      "      6        \u001b[36m1.1174\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.1303\u001b[0m     +  1.2188\n",
      "      7        \u001b[36m1.0264\u001b[0m       0.5793        \u001b[35m1.0849\u001b[0m     +  1.2193\n",
      "      8        \u001b[36m0.8597\u001b[0m       \u001b[32m0.6552\u001b[0m        \u001b[35m0.9738\u001b[0m     +  1.2197\n",
      "      9        \u001b[36m0.7155\u001b[0m       0.6483        \u001b[35m0.9483\u001b[0m     +  1.2174\n",
      "     10        \u001b[36m0.6085\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m0.8824\u001b[0m     +  1.2195\n",
      "     11        \u001b[36m0.5134\u001b[0m       0.6966        0.8921        1.2182\n",
      "     12        \u001b[36m0.4510\u001b[0m       0.7034        \u001b[35m0.7982\u001b[0m     +  1.2174\n",
      "     13        \u001b[36m0.3405\u001b[0m       0.7034        \u001b[35m0.7956\u001b[0m     +  1.2181\n",
      "     14        \u001b[36m0.2646\u001b[0m       \u001b[32m0.7310\u001b[0m        \u001b[35m0.7628\u001b[0m     +  1.2177\n",
      "     15        \u001b[36m0.2004\u001b[0m       0.7034        0.7820        1.2179\n",
      "     16        \u001b[36m0.1728\u001b[0m       \u001b[32m0.7517\u001b[0m        0.7716        1.2186\n",
      "     17        \u001b[36m0.1381\u001b[0m       0.7310        0.7736        1.2185\n",
      "     18        \u001b[36m0.1143\u001b[0m       0.7448        0.8114        1.2180\n",
      "     19        \u001b[36m0.0941\u001b[0m       0.7241        0.7772        1.2173\n",
      "     20        \u001b[36m0.0704\u001b[0m       0.7448        0.8576        1.2181\n",
      "     21        \u001b[36m0.0618\u001b[0m       0.7241        0.8186        1.2197\n",
      "     22        \u001b[36m0.0547\u001b[0m       0.7517        0.8171        1.2182\n",
      "     23        \u001b[36m0.0520\u001b[0m       0.7379        0.8592        1.2192\n",
      "     24        \u001b[36m0.0410\u001b[0m       \u001b[32m0.7724\u001b[0m        \u001b[35m0.7577\u001b[0m     +  1.2198\n",
      "     25        \u001b[36m0.0381\u001b[0m       0.7586        0.8324        1.2184\n",
      "     26        \u001b[36m0.0307\u001b[0m       0.7586        0.8979        1.2184\n",
      "     27        \u001b[36m0.0306\u001b[0m       0.7724        0.7966        1.2169\n",
      "     28        \u001b[36m0.0243\u001b[0m       0.7586        0.8709        1.2196\n",
      "     29        0.0321       0.7724        0.8301        1.2179\n",
      "     30        0.0295       0.7655        0.8982        1.2246\n",
      "     31        0.0273       \u001b[32m0.7862\u001b[0m        0.8795        1.2164\n",
      "     32        \u001b[36m0.0159\u001b[0m       0.7793        0.9134        1.2197\n",
      "     33        0.0199       0.7448        1.1039        1.2200\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.25, module__hidden_features=128;, score=-0.897 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.4240\u001b[0m       \u001b[32m0.3172\u001b[0m        \u001b[35m1.8653\u001b[0m     +  1.2163\n",
      "      2        \u001b[36m2.0395\u001b[0m       \u001b[32m0.3862\u001b[0m        \u001b[35m1.4761\u001b[0m     +  1.2172\n",
      "      3        \u001b[36m1.5565\u001b[0m       \u001b[32m0.4897\u001b[0m        \u001b[35m1.3601\u001b[0m     +  1.2207\n",
      "      4        \u001b[36m1.3863\u001b[0m       0.4759        \u001b[35m1.2357\u001b[0m     +  1.2176\n",
      "      5        \u001b[36m1.1542\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.1631\u001b[0m     +  1.2189\n",
      "      6        \u001b[36m1.0114\u001b[0m       \u001b[32m0.6483\u001b[0m        \u001b[35m1.0571\u001b[0m     +  1.2180\n",
      "      7        \u001b[36m0.8691\u001b[0m       \u001b[32m0.7034\u001b[0m        \u001b[35m0.9312\u001b[0m     +  1.2182\n",
      "      8        \u001b[36m0.7158\u001b[0m       0.7034        \u001b[35m0.8354\u001b[0m     +  1.2216\n",
      "      9        \u001b[36m0.6028\u001b[0m       \u001b[32m0.7310\u001b[0m        \u001b[35m0.7842\u001b[0m     +  1.2186\n",
      "     10        \u001b[36m0.5057\u001b[0m       \u001b[32m0.7517\u001b[0m        \u001b[35m0.7278\u001b[0m     +  1.2303\n",
      "     11        \u001b[36m0.3768\u001b[0m       0.7448        \u001b[35m0.7242\u001b[0m     +  1.2275\n",
      "     12        \u001b[36m0.3068\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.6800\u001b[0m     +  1.2184\n",
      "     13        \u001b[36m0.2399\u001b[0m       0.7448        \u001b[35m0.6588\u001b[0m     +  1.2184\n",
      "     14        \u001b[36m0.1835\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m0.6512\u001b[0m     +  1.2203\n",
      "     15        \u001b[36m0.1513\u001b[0m       0.7655        0.7001        1.2207\n",
      "     16        \u001b[36m0.1155\u001b[0m       0.7724        0.6693        1.2188\n",
      "     17        \u001b[36m0.0844\u001b[0m       0.7724        0.7460        1.2209\n",
      "     18        \u001b[36m0.0722\u001b[0m       0.7586        0.7361        1.2179\n",
      "     19        \u001b[36m0.0529\u001b[0m       0.7724        0.6737        1.2187\n",
      "     20        \u001b[36m0.0469\u001b[0m       \u001b[32m0.8000\u001b[0m        0.6665        1.2180\n",
      "     21        \u001b[36m0.0347\u001b[0m       0.8000        0.6640        1.2172\n",
      "     22        \u001b[36m0.0250\u001b[0m       0.8000        0.7216        1.2197\n",
      "     23        0.0368       0.7862        0.7309        1.2197\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.25, module__hidden_features=128;, score=-0.912 total time=  52.0s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.3469\u001b[0m       \u001b[32m0.2361\u001b[0m        \u001b[35m1.6743\u001b[0m     +  1.2755\n",
      "      2        \u001b[36m1.9773\u001b[0m       \u001b[32m0.3194\u001b[0m        \u001b[35m1.5210\u001b[0m     +  1.2828\n",
      "      3        \u001b[36m1.7824\u001b[0m       \u001b[32m0.3958\u001b[0m        \u001b[35m1.3673\u001b[0m     +  1.2832\n",
      "      4        \u001b[36m1.3757\u001b[0m       \u001b[32m0.4028\u001b[0m        \u001b[35m1.2311\u001b[0m     +  1.2824\n",
      "      5        \u001b[36m1.1420\u001b[0m       \u001b[32m0.4792\u001b[0m        \u001b[35m1.1318\u001b[0m     +  1.2830\n",
      "      6        \u001b[36m0.9640\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m1.0382\u001b[0m     +  1.2822\n",
      "      7        \u001b[36m0.7448\u001b[0m       \u001b[32m0.5903\u001b[0m        \u001b[35m0.9570\u001b[0m     +  1.2817\n",
      "      8        \u001b[36m0.6086\u001b[0m       0.5903        \u001b[35m0.9074\u001b[0m     +  1.2825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m0.5012\u001b[0m       \u001b[32m0.7014\u001b[0m        \u001b[35m0.8340\u001b[0m     +  1.2850\n",
      "     10        \u001b[36m0.3498\u001b[0m       0.6875        \u001b[35m0.8172\u001b[0m     +  1.2826\n",
      "     11        \u001b[36m0.2214\u001b[0m       0.6875        0.8520        1.2822\n",
      "     12        \u001b[36m0.1692\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7520\u001b[0m     +  1.2791\n",
      "     13        \u001b[36m0.0891\u001b[0m       \u001b[32m0.7500\u001b[0m        0.7982        1.2820\n",
      "     14        \u001b[36m0.0638\u001b[0m       0.7500        0.7827        1.2801\n",
      "     15        \u001b[36m0.0413\u001b[0m       \u001b[32m0.7639\u001b[0m        0.8138        1.2829\n",
      "     16        \u001b[36m0.0310\u001b[0m       \u001b[32m0.7778\u001b[0m        0.7827        1.2794\n",
      "     17        \u001b[36m0.0188\u001b[0m       0.7708        0.8010        1.2789\n",
      "     18        \u001b[36m0.0152\u001b[0m       0.7500        0.8517        1.2774\n",
      "     19        \u001b[36m0.0141\u001b[0m       0.7639        0.8628        1.2778\n",
      "     20        \u001b[36m0.0072\u001b[0m       0.7639        0.9174        1.2758\n",
      "     21        0.0078       0.7569        0.9481        1.2765\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.25, module__hidden_features=256;, score=-1.073 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.3449\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m1.9608\u001b[0m     +  1.2784\n",
      "      2        \u001b[36m2.5865\u001b[0m       \u001b[32m0.3034\u001b[0m        \u001b[35m1.5556\u001b[0m     +  1.2845\n",
      "      3        \u001b[36m2.0627\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.3557\u001b[0m     +  1.2848\n",
      "      4        \u001b[36m1.4532\u001b[0m       0.4276        \u001b[35m1.3093\u001b[0m     +  1.2839\n",
      "      5        \u001b[36m1.3037\u001b[0m       \u001b[32m0.4759\u001b[0m        \u001b[35m1.2520\u001b[0m     +  1.2831\n",
      "      6        \u001b[36m1.1264\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.1526\u001b[0m     +  1.2844\n",
      "      7        \u001b[36m0.9362\u001b[0m       \u001b[32m0.6207\u001b[0m        \u001b[35m1.0829\u001b[0m     +  1.2835\n",
      "      8        \u001b[36m0.7532\u001b[0m       0.6207        \u001b[35m0.9990\u001b[0m     +  1.2845\n",
      "      9        \u001b[36m0.5936\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m0.9294\u001b[0m     +  1.2888\n",
      "     10        \u001b[36m0.4482\u001b[0m       0.6828        \u001b[35m0.8901\u001b[0m     +  1.2835\n",
      "     11        \u001b[36m0.3057\u001b[0m       \u001b[32m0.7310\u001b[0m        \u001b[35m0.8867\u001b[0m     +  1.2833\n",
      "     12        \u001b[36m0.2289\u001b[0m       0.6414        0.9524        1.2847\n",
      "     13        \u001b[36m0.1791\u001b[0m       0.7172        0.9029        1.2818\n",
      "     14        \u001b[36m0.1014\u001b[0m       0.7310        0.8927        1.2833\n",
      "     15        \u001b[36m0.0714\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.8599\u001b[0m     +  1.2810\n",
      "     16        \u001b[36m0.0482\u001b[0m       0.7172        0.9544        1.2829\n",
      "     17        \u001b[36m0.0304\u001b[0m       0.7379        0.9052        1.2808\n",
      "     18        \u001b[36m0.0295\u001b[0m       0.7517        1.0567        1.2802\n",
      "     19        \u001b[36m0.0202\u001b[0m       0.7586        0.9447        1.2795\n",
      "     20        \u001b[36m0.0196\u001b[0m       0.7448        0.9968        1.2803\n",
      "     21        \u001b[36m0.0130\u001b[0m       0.7379        1.1566        1.2795\n",
      "     22        0.0160       0.7586        0.9405        1.2800\n",
      "     23        0.0190       0.7586        1.0151        1.2805\n",
      "     24        \u001b[36m0.0097\u001b[0m       0.7379        1.1112        1.2804\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.25, module__hidden_features=256;, score=-0.928 total time= 1.1min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.1667\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m2.3852\u001b[0m     +  1.2740\n",
      "      2        \u001b[36m2.4276\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.4716\u001b[0m     +  1.2809\n",
      "      3        \u001b[36m1.6836\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.3816\u001b[0m     +  1.2817\n",
      "      4        \u001b[36m1.3912\u001b[0m       \u001b[32m0.5862\u001b[0m        \u001b[35m1.2334\u001b[0m     +  1.2823\n",
      "      5        \u001b[36m1.1280\u001b[0m       0.5586        \u001b[35m1.1979\u001b[0m     +  1.2819\n",
      "      6        \u001b[36m1.0206\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m1.0064\u001b[0m     +  1.2819\n",
      "      7        \u001b[36m0.7988\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.9586\u001b[0m     +  1.2810\n",
      "      8        \u001b[36m0.6526\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m0.8301\u001b[0m     +  1.2825\n",
      "      9        \u001b[36m0.5118\u001b[0m       0.7241        \u001b[35m0.7816\u001b[0m     +  1.2814\n",
      "     10        \u001b[36m0.3893\u001b[0m       0.7241        \u001b[35m0.7714\u001b[0m     +  1.2837\n",
      "     11        \u001b[36m0.2740\u001b[0m       \u001b[32m0.7448\u001b[0m        \u001b[35m0.6845\u001b[0m     +  1.2867\n",
      "     12        \u001b[36m0.1974\u001b[0m       \u001b[32m0.7517\u001b[0m        0.6937        1.2820\n",
      "     13        \u001b[36m0.1461\u001b[0m       0.7310        0.7475        1.2800\n",
      "     14        \u001b[36m0.0842\u001b[0m       \u001b[32m0.7655\u001b[0m        0.7142        1.2809\n",
      "     15        \u001b[36m0.0641\u001b[0m       \u001b[32m0.7724\u001b[0m        0.6968        1.2796\n",
      "     16        \u001b[36m0.0449\u001b[0m       0.7379        0.8743        1.2798\n",
      "     17        \u001b[36m0.0310\u001b[0m       \u001b[32m0.7793\u001b[0m        0.7206        1.2806\n",
      "     18        \u001b[36m0.0263\u001b[0m       0.7793        0.8212        1.2791\n",
      "     19        \u001b[36m0.0241\u001b[0m       0.7655        0.7216        1.2782\n",
      "     20        0.0256       0.7655        0.7638        1.2790\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.25, module__hidden_features=256;, score=-0.909 total time= 1.0min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.3651\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.5639\u001b[0m     +  1.1866\n",
      "      2        \u001b[36m1.7970\u001b[0m       \u001b[32m0.3958\u001b[0m        \u001b[35m1.5481\u001b[0m     +  1.1843\n",
      "      3        \u001b[36m1.6583\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.5210\u001b[0m     +  1.1844\n",
      "      4        \u001b[36m1.5482\u001b[0m       0.3750        \u001b[35m1.4608\u001b[0m     +  1.1838\n",
      "      5        \u001b[36m1.4724\u001b[0m       0.4236        \u001b[35m1.4076\u001b[0m     +  1.1850\n",
      "      6        \u001b[36m1.4292\u001b[0m       \u001b[32m0.4931\u001b[0m        \u001b[35m1.3866\u001b[0m     +  1.1848\n",
      "      7        \u001b[36m1.3649\u001b[0m       0.4653        \u001b[35m1.3553\u001b[0m     +  1.1825\n",
      "      8        \u001b[36m1.3497\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m1.3531\u001b[0m     +  1.1835\n",
      "      9        \u001b[36m1.3014\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.2902\u001b[0m     +  1.1834\n",
      "     10        \u001b[36m1.2642\u001b[0m       0.5139        \u001b[35m1.2770\u001b[0m     +  1.1861\n",
      "     11        \u001b[36m1.2392\u001b[0m       0.5278        \u001b[35m1.2396\u001b[0m     +  1.1846\n",
      "     12        \u001b[36m1.1752\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.1704\u001b[0m     +  1.1847\n",
      "     13        \u001b[36m1.0891\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m1.1086\u001b[0m     +  1.1840\n",
      "     14        \u001b[36m1.0493\u001b[0m       0.5417        \u001b[35m1.0922\u001b[0m     +  1.1841\n",
      "     15        \u001b[36m0.9944\u001b[0m       \u001b[32m0.6528\u001b[0m        \u001b[35m1.0170\u001b[0m     +  1.1852\n",
      "     16        \u001b[36m0.9926\u001b[0m       0.6181        1.0204        1.1849\n",
      "     17        \u001b[36m0.8957\u001b[0m       0.6250        \u001b[35m0.9568\u001b[0m     +  1.1839\n",
      "     18        \u001b[36m0.8211\u001b[0m       \u001b[32m0.6597\u001b[0m        \u001b[35m0.9217\u001b[0m     +  1.1845\n",
      "     19        \u001b[36m0.7728\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.8947\u001b[0m     +  1.1867\n",
      "     20        \u001b[36m0.7441\u001b[0m       0.6806        \u001b[35m0.8527\u001b[0m     +  1.1870\n",
      "     21        \u001b[36m0.6734\u001b[0m       \u001b[32m0.7222\u001b[0m        0.8757        1.1849\n",
      "     22        \u001b[36m0.6371\u001b[0m       \u001b[32m0.7292\u001b[0m        \u001b[35m0.8461\u001b[0m     +  1.1871\n",
      "     23        \u001b[36m0.5727\u001b[0m       0.6875        \u001b[35m0.8390\u001b[0m     +  1.1867\n",
      "     24        \u001b[36m0.5173\u001b[0m       0.7083        0.8547        1.1862\n",
      "     25        \u001b[36m0.5136\u001b[0m       0.7153        \u001b[35m0.8053\u001b[0m     +  1.1863\n",
      "     26        \u001b[36m0.4925\u001b[0m       0.6736        0.8159        1.1844\n",
      "     27        \u001b[36m0.3912\u001b[0m       0.7222        \u001b[35m0.7583\u001b[0m     +  1.1874\n",
      "     28        0.4492       0.7083        0.7738        1.1857\n",
      "     29        0.4078       0.7083        0.8143        1.1872\n",
      "     30        \u001b[36m0.3728\u001b[0m       0.7292        \u001b[35m0.7433\u001b[0m     +  1.1865\n",
      "     31        \u001b[36m0.3294\u001b[0m       0.7083        0.8531        1.1847\n",
      "     32        \u001b[36m0.3088\u001b[0m       0.7153        \u001b[35m0.7389\u001b[0m     +  1.1846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     33        \u001b[36m0.2872\u001b[0m       0.7222        0.8317        1.1919\n",
      "     34        \u001b[36m0.2672\u001b[0m       0.7153        0.7610        1.2070\n",
      "     35        \u001b[36m0.2633\u001b[0m       0.7292        0.7828        1.1953\n",
      "     36        \u001b[36m0.2396\u001b[0m       0.7292        0.7985        1.1873\n",
      "     37        \u001b[36m0.2124\u001b[0m       \u001b[32m0.7431\u001b[0m        0.7755        1.2016\n",
      "     38        0.2449       0.7222        0.9061        1.1942\n",
      "     39        0.2237       0.7361        0.8156        1.1845\n",
      "     40        0.2159       \u001b[32m0.7500\u001b[0m        0.7808        1.2029\n",
      "     41        \u001b[36m0.1848\u001b[0m       0.7500        0.8142        1.2039\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__hidden_features=64;, score=-0.972 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.8075\u001b[0m       \u001b[32m0.2690\u001b[0m        \u001b[35m1.5934\u001b[0m     +  1.1955\n",
      "      2        \u001b[36m2.1208\u001b[0m       \u001b[32m0.3103\u001b[0m        \u001b[35m1.5700\u001b[0m     +  1.1898\n",
      "      3        \u001b[36m1.7836\u001b[0m       \u001b[32m0.3310\u001b[0m        \u001b[35m1.5627\u001b[0m     +  1.1874\n",
      "      4        \u001b[36m1.6901\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.5426\u001b[0m     +  1.1968\n",
      "      5        \u001b[36m1.5933\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.5138\u001b[0m     +  1.2149\n",
      "      6        \u001b[36m1.5387\u001b[0m       0.3172        \u001b[35m1.4951\u001b[0m     +  1.1890\n",
      "      7        \u001b[36m1.4989\u001b[0m       0.3586        \u001b[35m1.4761\u001b[0m     +  1.1932\n",
      "      8        \u001b[36m1.4594\u001b[0m       \u001b[32m0.4414\u001b[0m        \u001b[35m1.4586\u001b[0m     +  1.1871\n",
      "      9        \u001b[36m1.4294\u001b[0m       0.4414        \u001b[35m1.4215\u001b[0m     +  1.1865\n",
      "     10        \u001b[36m1.4088\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.3882\u001b[0m     +  1.1868\n",
      "     11        \u001b[36m1.3946\u001b[0m       \u001b[32m0.4897\u001b[0m        \u001b[35m1.3529\u001b[0m     +  1.1871\n",
      "     12        \u001b[36m1.3314\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.3160\u001b[0m     +  1.1883\n",
      "     13        \u001b[36m1.2597\u001b[0m       \u001b[32m0.5586\u001b[0m        \u001b[35m1.2541\u001b[0m     +  1.1872\n",
      "     14        \u001b[36m1.2209\u001b[0m       \u001b[32m0.5793\u001b[0m        1.2578        1.1873\n",
      "     15        \u001b[36m1.1834\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.1854\u001b[0m     +  1.1873\n",
      "     16        \u001b[36m1.1346\u001b[0m       \u001b[32m0.6207\u001b[0m        1.1989        1.1885\n",
      "     17        \u001b[36m1.1090\u001b[0m       0.5862        \u001b[35m1.1626\u001b[0m     +  1.1876\n",
      "     18        1.1356       \u001b[32m0.6345\u001b[0m        \u001b[35m1.1280\u001b[0m     +  1.1879\n",
      "     19        \u001b[36m0.9870\u001b[0m       0.5448        1.1500        1.1872\n",
      "     20        1.0283       \u001b[32m0.7103\u001b[0m        \u001b[35m1.0456\u001b[0m     +  1.1867\n",
      "     21        \u001b[36m0.9145\u001b[0m       0.6828        \u001b[35m1.0026\u001b[0m     +  1.1878\n",
      "     22        \u001b[36m0.9117\u001b[0m       0.6690        1.0072        1.1886\n",
      "     23        \u001b[36m0.8441\u001b[0m       0.7103        \u001b[35m0.9521\u001b[0m     +  1.1867\n",
      "     24        \u001b[36m0.7867\u001b[0m       \u001b[32m0.7172\u001b[0m        0.9667        1.1870\n",
      "     25        0.7885       0.7034        \u001b[35m0.9027\u001b[0m     +  1.1864\n",
      "     26        \u001b[36m0.7065\u001b[0m       0.7172        \u001b[35m0.8725\u001b[0m     +  1.1874\n",
      "     27        \u001b[36m0.6966\u001b[0m       0.7103        \u001b[35m0.8691\u001b[0m     +  1.1881\n",
      "     28        \u001b[36m0.6459\u001b[0m       0.7172        \u001b[35m0.8659\u001b[0m     +  1.1869\n",
      "     29        \u001b[36m0.6392\u001b[0m       0.7172        \u001b[35m0.8197\u001b[0m     +  1.1885\n",
      "     30        \u001b[36m0.6176\u001b[0m       0.7172        \u001b[35m0.8100\u001b[0m     +  1.1880\n",
      "     31        \u001b[36m0.5476\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m0.7991\u001b[0m     +  1.1947\n",
      "     32        \u001b[36m0.4997\u001b[0m       0.6966        0.8053        1.1872\n",
      "     33        0.5017       0.7172        0.8856        1.1865\n",
      "     34        0.5077       \u001b[32m0.7379\u001b[0m        \u001b[35m0.7900\u001b[0m     +  1.1866\n",
      "     35        \u001b[36m0.4504\u001b[0m       0.7310        \u001b[35m0.7759\u001b[0m     +  1.1874\n",
      "     36        \u001b[36m0.4086\u001b[0m       0.7310        0.8190        1.1878\n",
      "     37        \u001b[36m0.3640\u001b[0m       0.7379        0.8083        1.1868\n",
      "     38        \u001b[36m0.3564\u001b[0m       \u001b[32m0.7448\u001b[0m        \u001b[35m0.7726\u001b[0m     +  1.1860\n",
      "     39        \u001b[36m0.3527\u001b[0m       \u001b[32m0.7517\u001b[0m        \u001b[35m0.7654\u001b[0m     +  1.1880\n",
      "     40        \u001b[36m0.2966\u001b[0m       0.7448        \u001b[35m0.7610\u001b[0m     +  1.1909\n",
      "     41        \u001b[36m0.2880\u001b[0m       \u001b[32m0.7655\u001b[0m        0.7918        1.2012\n",
      "     42        \u001b[36m0.2821\u001b[0m       0.7379        \u001b[35m0.7338\u001b[0m     +  1.1891\n",
      "     43        \u001b[36m0.2610\u001b[0m       0.7241        \u001b[35m0.7272\u001b[0m     +  1.1889\n",
      "     44        \u001b[36m0.2325\u001b[0m       \u001b[32m0.7724\u001b[0m        0.7681        1.1883\n",
      "     45        0.2547       0.7379        0.7671        1.1875\n",
      "     46        0.2357       0.7655        0.7649        1.1862\n",
      "     47        \u001b[36m0.2021\u001b[0m       0.7517        0.8487        1.1851\n",
      "     48        \u001b[36m0.1862\u001b[0m       0.7586        0.8302        1.1860\n",
      "     49        0.1945       0.7379        0.8317        1.1864\n",
      "     50        0.1958       0.7655        0.8203        1.1872\n",
      "     51        0.2113       0.7379        0.8657        1.1904\n",
      "     52        \u001b[36m0.1788\u001b[0m       0.7655        0.8897        1.1882\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__hidden_features=64;, score=-0.864 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.9448\u001b[0m       \u001b[32m0.2345\u001b[0m        \u001b[35m1.5896\u001b[0m     +  1.1852\n",
      "      2        \u001b[36m1.9124\u001b[0m       \u001b[32m0.3586\u001b[0m        \u001b[35m1.4987\u001b[0m     +  1.1884\n",
      "      3        \u001b[36m1.6197\u001b[0m       \u001b[32m0.3724\u001b[0m        \u001b[35m1.4966\u001b[0m     +  1.1900\n",
      "      4        \u001b[36m1.5457\u001b[0m       0.3655        \u001b[35m1.4388\u001b[0m     +  1.1891\n",
      "      5        \u001b[36m1.5285\u001b[0m       \u001b[32m0.4966\u001b[0m        \u001b[35m1.4337\u001b[0m     +  1.1872\n",
      "      6        \u001b[36m1.4966\u001b[0m       0.4000        \u001b[35m1.4086\u001b[0m     +  1.1901\n",
      "      7        \u001b[36m1.3940\u001b[0m       0.4552        \u001b[35m1.3178\u001b[0m     +  1.1873\n",
      "      8        \u001b[36m1.3572\u001b[0m       \u001b[32m0.5172\u001b[0m        \u001b[35m1.2751\u001b[0m     +  1.1896\n",
      "      9        \u001b[36m1.3171\u001b[0m       \u001b[32m0.5517\u001b[0m        \u001b[35m1.2390\u001b[0m     +  1.1883\n",
      "     10        \u001b[36m1.2354\u001b[0m       0.5517        \u001b[35m1.1957\u001b[0m     +  1.1877\n",
      "     11        \u001b[36m1.1995\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.1788\u001b[0m     +  1.1885\n",
      "     12        \u001b[36m1.1566\u001b[0m       \u001b[32m0.5931\u001b[0m        \u001b[35m1.1238\u001b[0m     +  1.1877\n",
      "     13        \u001b[36m1.1447\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m1.1035\u001b[0m     +  1.1886\n",
      "     14        \u001b[36m1.0835\u001b[0m       0.6828        \u001b[35m1.0507\u001b[0m     +  1.1902\n",
      "     15        \u001b[36m1.0313\u001b[0m       0.6552        \u001b[35m1.0198\u001b[0m     +  1.1890\n",
      "     16        \u001b[36m1.0111\u001b[0m       \u001b[32m0.7310\u001b[0m        \u001b[35m0.9937\u001b[0m     +  1.1963\n",
      "     17        \u001b[36m0.9247\u001b[0m       0.7103        \u001b[35m0.9618\u001b[0m     +  1.2048\n",
      "     18        \u001b[36m0.8739\u001b[0m       0.7172        \u001b[35m0.9088\u001b[0m     +  1.1962\n",
      "     19        \u001b[36m0.7999\u001b[0m       0.6828        \u001b[35m0.9016\u001b[0m     +  1.1876\n",
      "     20        0.8098       \u001b[32m0.7448\u001b[0m        \u001b[35m0.8286\u001b[0m     +  1.1891\n",
      "     21        \u001b[36m0.7478\u001b[0m       0.7448        \u001b[35m0.8165\u001b[0m     +  1.1888\n",
      "     22        \u001b[36m0.6876\u001b[0m       0.7034        \u001b[35m0.7916\u001b[0m     +  1.1873\n",
      "     23        \u001b[36m0.6830\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.7813\u001b[0m     +  1.1887\n",
      "     24        \u001b[36m0.6814\u001b[0m       \u001b[32m0.7655\u001b[0m        0.7861        1.1887\n",
      "     25        \u001b[36m0.5522\u001b[0m       0.7517        \u001b[35m0.7234\u001b[0m     +  1.1883\n",
      "     26        \u001b[36m0.5219\u001b[0m       0.7586        \u001b[35m0.6888\u001b[0m     +  1.1861\n",
      "     27        0.5327       \u001b[32m0.7862\u001b[0m        0.7070        1.1886\n",
      "     28        \u001b[36m0.4746\u001b[0m       0.7586        0.7295        1.1878\n",
      "     29        \u001b[36m0.4446\u001b[0m       0.7724        \u001b[35m0.6706\u001b[0m     +  1.1854\n",
      "     30        \u001b[36m0.4314\u001b[0m       \u001b[32m0.7931\u001b[0m        \u001b[35m0.6465\u001b[0m     +  1.1871\n",
      "     31        \u001b[36m0.3706\u001b[0m       \u001b[32m0.8069\u001b[0m        \u001b[35m0.6425\u001b[0m     +  1.1872\n",
      "     32        0.4012       0.7931        0.6595        1.1865\n",
      "     33        0.3759       0.7793        0.6885        1.1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     34        \u001b[36m0.3470\u001b[0m       0.7724        0.6500        1.1864\n",
      "     35        \u001b[36m0.3369\u001b[0m       0.7724        0.6785        1.1864\n",
      "     36        \u001b[36m0.2991\u001b[0m       0.7931        \u001b[35m0.6021\u001b[0m     +  1.1859\n",
      "     37        \u001b[36m0.2734\u001b[0m       0.7724        0.6641        1.1882\n",
      "     38        0.3051       0.7862        \u001b[35m0.5879\u001b[0m     +  1.1860\n",
      "     39        \u001b[36m0.2571\u001b[0m       0.7931        0.6299        1.1868\n",
      "     40        \u001b[36m0.2371\u001b[0m       0.7793        0.6531        1.1860\n",
      "     41        \u001b[36m0.2258\u001b[0m       0.7793        0.6858        1.1840\n",
      "     42        \u001b[36m0.1984\u001b[0m       0.7862        0.7036        1.1837\n",
      "     43        0.2169       0.7793        0.6954        1.1855\n",
      "     44        \u001b[36m0.1878\u001b[0m       0.7862        0.6700        1.1842\n",
      "     45        0.2364       0.7931        0.7326        1.1841\n",
      "     46        \u001b[36m0.1678\u001b[0m       0.7931        0.6790        1.1821\n",
      "     47        0.1882       0.7793        0.6766        1.1890\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__hidden_features=64;, score=-0.881 total time= 1.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.2206\u001b[0m       \u001b[32m0.2986\u001b[0m        \u001b[35m1.5685\u001b[0m     +  1.2176\n",
      "      2        \u001b[36m1.9927\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.4898\u001b[0m     +  1.2227\n",
      "      3        \u001b[36m1.6624\u001b[0m       \u001b[32m0.4444\u001b[0m        \u001b[35m1.4270\u001b[0m     +  1.2233\n",
      "      4        \u001b[36m1.5372\u001b[0m       \u001b[32m0.4583\u001b[0m        \u001b[35m1.4022\u001b[0m     +  1.2223\n",
      "      5        \u001b[36m1.4132\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m1.3466\u001b[0m     +  1.2385\n",
      "      6        \u001b[36m1.4049\u001b[0m       0.5208        \u001b[35m1.2972\u001b[0m     +  1.2238\n",
      "      7        \u001b[36m1.3293\u001b[0m       \u001b[32m0.5764\u001b[0m        \u001b[35m1.2725\u001b[0m     +  1.2321\n",
      "      8        \u001b[36m1.2212\u001b[0m       0.5347        \u001b[35m1.2123\u001b[0m     +  1.2230\n",
      "      9        \u001b[36m1.1942\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.1305\u001b[0m     +  1.2359\n",
      "     10        \u001b[36m1.1252\u001b[0m       0.5694        \u001b[35m1.0835\u001b[0m     +  1.2354\n",
      "     11        \u001b[36m1.0364\u001b[0m       \u001b[32m0.6736\u001b[0m        \u001b[35m1.0198\u001b[0m     +  1.2337\n",
      "     12        \u001b[36m0.9564\u001b[0m       \u001b[32m0.7014\u001b[0m        \u001b[35m0.9450\u001b[0m     +  1.2437\n",
      "     13        \u001b[36m0.8624\u001b[0m       0.6736        \u001b[35m0.9189\u001b[0m     +  1.2238\n",
      "     14        \u001b[36m0.7532\u001b[0m       \u001b[32m0.7153\u001b[0m        \u001b[35m0.8702\u001b[0m     +  1.2230\n",
      "     15        \u001b[36m0.6605\u001b[0m       \u001b[32m0.7361\u001b[0m        \u001b[35m0.8065\u001b[0m     +  1.2228\n",
      "     16        \u001b[36m0.5871\u001b[0m       0.6806        0.8072        1.2249\n",
      "     17        \u001b[36m0.5064\u001b[0m       0.7292        \u001b[35m0.7785\u001b[0m     +  1.2219\n",
      "     18        \u001b[36m0.4591\u001b[0m       0.6875        \u001b[35m0.7536\u001b[0m     +  1.2239\n",
      "     19        \u001b[36m0.3539\u001b[0m       \u001b[32m0.7708\u001b[0m        \u001b[35m0.6846\u001b[0m     +  1.2224\n",
      "     20        \u001b[36m0.3407\u001b[0m       0.7292        0.7190        1.2223\n",
      "     21        \u001b[36m0.3164\u001b[0m       0.7500        0.7497        1.2235\n",
      "     22        \u001b[36m0.2442\u001b[0m       0.7222        0.7065        1.2235\n",
      "     23        \u001b[36m0.2076\u001b[0m       0.7431        0.7564        1.2218\n",
      "     24        \u001b[36m0.1752\u001b[0m       0.7569        \u001b[35m0.6592\u001b[0m     +  1.2217\n",
      "     25        \u001b[36m0.1741\u001b[0m       0.7431        0.7894        1.2243\n",
      "     26        \u001b[36m0.1487\u001b[0m       0.7361        0.8238        1.2230\n",
      "     27        \u001b[36m0.1467\u001b[0m       0.7639        0.7610        1.2218\n",
      "     28        \u001b[36m0.1061\u001b[0m       0.7708        0.7490        1.2230\n",
      "     29        \u001b[36m0.0916\u001b[0m       0.7222        0.8408        1.2231\n",
      "     30        0.1002       \u001b[32m0.7778\u001b[0m        0.7687        1.2228\n",
      "     31        \u001b[36m0.0915\u001b[0m       0.7431        0.9680        1.2212\n",
      "     32        \u001b[36m0.0779\u001b[0m       0.7500        0.8352        1.2209\n",
      "     33        0.0908       0.7431        0.8924        1.2213\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__hidden_features=128;, score=-1.055 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.3364\u001b[0m       \u001b[32m0.1586\u001b[0m        \u001b[35m1.7191\u001b[0m     +  1.2219\n",
      "      2        \u001b[36m2.1588\u001b[0m       \u001b[32m0.4414\u001b[0m        \u001b[35m1.4958\u001b[0m     +  1.2260\n",
      "      3        \u001b[36m1.7586\u001b[0m       0.3310        1.4982        1.2239\n",
      "      4        \u001b[36m1.5547\u001b[0m       \u001b[32m0.4828\u001b[0m        \u001b[35m1.4856\u001b[0m     +  1.2269\n",
      "      5        \u001b[36m1.4967\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.4369\u001b[0m     +  1.2263\n",
      "      6        \u001b[36m1.4622\u001b[0m       0.5724        \u001b[35m1.3922\u001b[0m     +  1.2244\n",
      "      7        \u001b[36m1.4191\u001b[0m       0.4414        \u001b[35m1.3625\u001b[0m     +  1.2256\n",
      "      8        1.4254       0.5448        \u001b[35m1.3042\u001b[0m     +  1.2256\n",
      "      9        \u001b[36m1.2795\u001b[0m       \u001b[32m0.5793\u001b[0m        \u001b[35m1.2420\u001b[0m     +  1.2263\n",
      "     10        \u001b[36m1.1766\u001b[0m       \u001b[32m0.6069\u001b[0m        \u001b[35m1.1700\u001b[0m     +  1.2275\n",
      "     11        \u001b[36m1.0904\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m1.0699\u001b[0m     +  1.2245\n",
      "     12        \u001b[36m1.0207\u001b[0m       0.6759        1.0711        1.2244\n",
      "     13        \u001b[36m0.9528\u001b[0m       0.6690        \u001b[35m1.0290\u001b[0m     +  1.2241\n",
      "     14        \u001b[36m0.8909\u001b[0m       0.6759        \u001b[35m0.9571\u001b[0m     +  1.2251\n",
      "     15        \u001b[36m0.8043\u001b[0m       0.6759        \u001b[35m0.9048\u001b[0m     +  1.2250\n",
      "     16        \u001b[36m0.7567\u001b[0m       0.6828        \u001b[35m0.8659\u001b[0m     +  1.2245\n",
      "     17        \u001b[36m0.6488\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m0.8449\u001b[0m     +  1.2253\n",
      "     18        \u001b[36m0.6022\u001b[0m       0.6552        0.8765        1.2243\n",
      "     19        \u001b[36m0.5171\u001b[0m       0.7103        0.8650        1.2247\n",
      "     20        \u001b[36m0.4442\u001b[0m       \u001b[32m0.7379\u001b[0m        \u001b[35m0.8181\u001b[0m     +  1.2250\n",
      "     21        \u001b[36m0.3952\u001b[0m       0.7172        0.8558        1.2284\n",
      "     22        \u001b[36m0.3614\u001b[0m       0.6966        \u001b[35m0.8179\u001b[0m     +  1.2266\n",
      "     23        \u001b[36m0.3026\u001b[0m       0.7172        0.8302        1.2256\n",
      "     24        \u001b[36m0.2952\u001b[0m       0.7379        0.8546        1.2262\n",
      "     25        \u001b[36m0.2348\u001b[0m       \u001b[32m0.7448\u001b[0m        \u001b[35m0.7493\u001b[0m     +  1.2257\n",
      "     26        \u001b[36m0.2162\u001b[0m       0.7448        0.8357        1.2250\n",
      "     27        \u001b[36m0.2067\u001b[0m       \u001b[32m0.7517\u001b[0m        0.7861        1.2244\n",
      "     28        \u001b[36m0.1987\u001b[0m       0.7310        0.8682        1.2240\n",
      "     29        \u001b[36m0.1601\u001b[0m       0.7448        0.7967        1.2252\n",
      "     30        \u001b[36m0.1474\u001b[0m       0.7448        0.9217        1.2289\n",
      "     31        \u001b[36m0.1420\u001b[0m       0.7517        0.8241        1.2278\n",
      "     32        0.1531       0.7379        0.9524        1.2390\n",
      "     33        \u001b[36m0.1168\u001b[0m       0.7379        0.7895        1.2273\n",
      "     34        \u001b[36m0.1012\u001b[0m       \u001b[32m0.7724\u001b[0m        0.8360        1.2312\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__hidden_features=128;, score=-0.822 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.1918\u001b[0m       \u001b[32m0.3379\u001b[0m        \u001b[35m1.6084\u001b[0m     +  1.2226\n",
      "      2        \u001b[36m2.0853\u001b[0m       0.3379        \u001b[35m1.4882\u001b[0m     +  1.2248\n",
      "      3        \u001b[36m1.6214\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m1.4677\u001b[0m     +  1.2270\n",
      "      4        \u001b[36m1.5804\u001b[0m       \u001b[32m0.5655\u001b[0m        \u001b[35m1.4087\u001b[0m     +  1.2240\n",
      "      5        \u001b[36m1.4565\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.3575\u001b[0m     +  1.2253\n",
      "      6        \u001b[36m1.3807\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.2840\u001b[0m     +  1.2235\n",
      "      7        \u001b[36m1.3276\u001b[0m       0.6069        \u001b[35m1.2605\u001b[0m     +  1.2239\n",
      "      8        \u001b[36m1.2421\u001b[0m       \u001b[32m0.6414\u001b[0m        \u001b[35m1.1661\u001b[0m     +  1.2242\n",
      "      9        \u001b[36m1.2326\u001b[0m       \u001b[32m0.6552\u001b[0m        \u001b[35m1.0954\u001b[0m     +  1.2263\n",
      "     10        \u001b[36m1.0898\u001b[0m       0.6138        \u001b[35m1.0700\u001b[0m     +  1.2244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m1.0272\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m1.0237\u001b[0m     +  1.2260\n",
      "     12        \u001b[36m0.9348\u001b[0m       0.6552        \u001b[35m0.9280\u001b[0m     +  1.2252\n",
      "     13        \u001b[36m0.8872\u001b[0m       0.6276        0.9578        1.2246\n",
      "     14        \u001b[36m0.8032\u001b[0m       0.6759        \u001b[35m0.9165\u001b[0m     +  1.2247\n",
      "     15        \u001b[36m0.7046\u001b[0m       \u001b[32m0.7448\u001b[0m        \u001b[35m0.7896\u001b[0m     +  1.2258\n",
      "     16        \u001b[36m0.6260\u001b[0m       0.7172        0.7901        1.2260\n",
      "     17        \u001b[36m0.5518\u001b[0m       0.7310        \u001b[35m0.7446\u001b[0m     +  1.2232\n",
      "     18        \u001b[36m0.5215\u001b[0m       0.7448        0.7851        1.2281\n",
      "     19        \u001b[36m0.4441\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.7386\u001b[0m     +  1.2250\n",
      "     20        \u001b[36m0.3459\u001b[0m       \u001b[32m0.7724\u001b[0m        \u001b[35m0.6694\u001b[0m     +  1.2277\n",
      "     21        \u001b[36m0.2895\u001b[0m       0.7724        0.7045        1.2276\n",
      "     22        0.2927       0.7517        0.6749        1.2257\n",
      "     23        \u001b[36m0.2755\u001b[0m       0.7655        0.7193        1.2255\n",
      "     24        \u001b[36m0.2434\u001b[0m       \u001b[32m0.8069\u001b[0m        \u001b[35m0.6469\u001b[0m     +  1.2242\n",
      "     25        \u001b[36m0.2046\u001b[0m       0.7655        0.7326        1.2264\n",
      "     26        \u001b[36m0.1925\u001b[0m       0.7724        0.6792        1.2247\n",
      "     27        \u001b[36m0.1500\u001b[0m       0.7448        0.8468        1.2223\n",
      "     28        0.1560       0.7448        0.8169        1.2217\n",
      "     29        0.1610       0.7310        0.9262        1.2236\n",
      "     30        \u001b[36m0.1236\u001b[0m       0.7586        0.7904        1.2231\n",
      "     31        \u001b[36m0.1036\u001b[0m       0.7517        0.8607        1.2234\n",
      "     32        0.1108       0.7862        0.7237        1.2234\n",
      "     33        0.1239       0.7724        0.7471        1.2233\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__hidden_features=128;, score=-0.882 total time= 1.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.4059\u001b[0m       \u001b[32m0.4375\u001b[0m        \u001b[35m1.5604\u001b[0m     +  1.2818\n",
      "      2        \u001b[36m2.3076\u001b[0m       0.3958        \u001b[35m1.4238\u001b[0m     +  1.2885\n",
      "      3        \u001b[36m1.7075\u001b[0m       0.3819        \u001b[35m1.4047\u001b[0m     +  1.2890\n",
      "      4        \u001b[36m1.4821\u001b[0m       0.4306        \u001b[35m1.3426\u001b[0m     +  1.2881\n",
      "      5        \u001b[36m1.3612\u001b[0m       \u001b[32m0.4653\u001b[0m        \u001b[35m1.2940\u001b[0m     +  1.2883\n",
      "      6        \u001b[36m1.2738\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.2685\u001b[0m     +  1.2901\n",
      "      7        \u001b[36m1.2270\u001b[0m       \u001b[32m0.5694\u001b[0m        \u001b[35m1.1770\u001b[0m     +  1.2896\n",
      "      8        \u001b[36m1.1167\u001b[0m       0.5694        \u001b[35m1.1197\u001b[0m     +  1.2914\n",
      "      9        \u001b[36m1.0013\u001b[0m       0.5208        \u001b[35m1.0901\u001b[0m     +  1.2893\n",
      "     10        \u001b[36m0.9297\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.9723\u001b[0m     +  1.2887\n",
      "     11        \u001b[36m0.7965\u001b[0m       0.6597        \u001b[35m0.9043\u001b[0m     +  1.2998\n",
      "     12        \u001b[36m0.6652\u001b[0m       \u001b[32m0.6944\u001b[0m        \u001b[35m0.8587\u001b[0m     +  1.2898\n",
      "     13        \u001b[36m0.5530\u001b[0m       \u001b[32m0.7431\u001b[0m        \u001b[35m0.8008\u001b[0m     +  1.2897\n",
      "     14        \u001b[36m0.4440\u001b[0m       0.6806        0.8145        1.2884\n",
      "     15        \u001b[36m0.4106\u001b[0m       0.7014        \u001b[35m0.7746\u001b[0m     +  1.2886\n",
      "     16        \u001b[36m0.2928\u001b[0m       \u001b[32m0.7569\u001b[0m        \u001b[35m0.7318\u001b[0m     +  1.2875\n",
      "     17        \u001b[36m0.2135\u001b[0m       0.7292        0.7772        1.2891\n",
      "     18        \u001b[36m0.2084\u001b[0m       0.7431        0.7517        1.2891\n",
      "     19        \u001b[36m0.1341\u001b[0m       \u001b[32m0.7639\u001b[0m        0.7568        1.2871\n",
      "     20        \u001b[36m0.1295\u001b[0m       0.7153        0.8251        1.2843\n",
      "     21        \u001b[36m0.0852\u001b[0m       \u001b[32m0.7986\u001b[0m        \u001b[35m0.7235\u001b[0m     +  1.2864\n",
      "     22        \u001b[36m0.0640\u001b[0m       0.7500        0.7726        1.2916\n",
      "     23        \u001b[36m0.0569\u001b[0m       0.7361        0.8369        1.2858\n",
      "     24        \u001b[36m0.0410\u001b[0m       0.7917        0.7730        1.2847\n",
      "     25        0.0439       0.7500        0.8969        1.2957\n",
      "     26        \u001b[36m0.0377\u001b[0m       0.7569        0.8831        1.3207\n",
      "     27        \u001b[36m0.0350\u001b[0m       0.7778        0.9029        1.2915\n",
      "     28        \u001b[36m0.0318\u001b[0m       0.7292        0.8840        1.2866\n",
      "     29        0.0318       0.7500        0.8555        1.2900\n",
      "     30        \u001b[36m0.0206\u001b[0m       0.7222        0.9153        1.2899\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.5, module__hidden_features=256;, score=-1.120 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m5.0434\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m2.3191\u001b[0m     +  1.2848\n",
      "      2        \u001b[36m3.1442\u001b[0m       \u001b[32m0.3103\u001b[0m        \u001b[35m1.5288\u001b[0m     +  1.2909\n",
      "      3        \u001b[36m1.8696\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.4551\u001b[0m     +  1.2898\n",
      "      4        \u001b[36m1.6200\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m1.3984\u001b[0m     +  1.2927\n",
      "      5        \u001b[36m1.4148\u001b[0m       0.5241        \u001b[35m1.3808\u001b[0m     +  1.2916\n",
      "      6        \u001b[36m1.4013\u001b[0m       0.5310        \u001b[35m1.3159\u001b[0m     +  1.2903\n",
      "      7        \u001b[36m1.2729\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.2706\u001b[0m     +  1.3127\n",
      "      8        \u001b[36m1.2137\u001b[0m       0.4828        \u001b[35m1.2040\u001b[0m     +  1.2893\n",
      "      9        \u001b[36m1.1405\u001b[0m       \u001b[32m0.6207\u001b[0m        \u001b[35m1.1541\u001b[0m     +  1.2905\n",
      "     10        \u001b[36m0.9933\u001b[0m       0.5931        \u001b[35m1.0740\u001b[0m     +  1.2910\n",
      "     11        \u001b[36m0.8944\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m1.0002\u001b[0m     +  1.2904\n",
      "     12        \u001b[36m0.8129\u001b[0m       0.6690        \u001b[35m0.9553\u001b[0m     +  1.2929\n",
      "     13        \u001b[36m0.7162\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m0.8230\u001b[0m     +  1.2896\n",
      "     14        \u001b[36m0.5998\u001b[0m       0.7034        0.8231        1.2908\n",
      "     15        \u001b[36m0.4834\u001b[0m       \u001b[32m0.7172\u001b[0m        \u001b[35m0.8129\u001b[0m     +  1.2894\n",
      "     16        \u001b[36m0.4209\u001b[0m       0.6690        0.8557        1.2903\n",
      "     17        \u001b[36m0.3475\u001b[0m       \u001b[32m0.7517\u001b[0m        \u001b[35m0.7927\u001b[0m     +  1.2890\n",
      "     18        \u001b[36m0.2391\u001b[0m       0.7379        0.8409        1.2909\n",
      "     19        \u001b[36m0.2157\u001b[0m       0.7103        0.8423        1.2933\n",
      "     20        \u001b[36m0.1684\u001b[0m       0.7172        0.9522        1.2876\n",
      "     21        \u001b[36m0.1622\u001b[0m       0.7241        0.8221        1.2875\n",
      "     22        \u001b[36m0.1152\u001b[0m       0.7034        1.0025        1.2881\n",
      "     23        0.1250       \u001b[32m0.7655\u001b[0m        0.8243        1.2884\n",
      "     24        \u001b[36m0.0800\u001b[0m       0.7655        0.8963        1.2971\n",
      "     25        \u001b[36m0.0694\u001b[0m       \u001b[32m0.7862\u001b[0m        0.8606        1.2949\n",
      "     26        \u001b[36m0.0488\u001b[0m       0.7724        0.9339        1.2876\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.5, module__hidden_features=256;, score=-0.980 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.0563\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m2.0446\u001b[0m     +  1.2852\n",
      "      2        \u001b[36m3.0165\u001b[0m       \u001b[32m0.4138\u001b[0m        \u001b[35m1.4673\u001b[0m     +  1.2901\n",
      "      3        \u001b[36m1.8727\u001b[0m       \u001b[32m0.4276\u001b[0m        \u001b[35m1.4445\u001b[0m     +  1.2903\n",
      "      4        \u001b[36m1.5515\u001b[0m       0.3862        \u001b[35m1.4268\u001b[0m     +  1.2960\n",
      "      5        \u001b[36m1.4750\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.3596\u001b[0m     +  1.2909\n",
      "      6        \u001b[36m1.3922\u001b[0m       \u001b[32m0.5931\u001b[0m        \u001b[35m1.3132\u001b[0m     +  1.2932\n",
      "      7        \u001b[36m1.3047\u001b[0m       0.5862        \u001b[35m1.2533\u001b[0m     +  1.2902\n",
      "      8        \u001b[36m1.1517\u001b[0m       \u001b[32m0.6552\u001b[0m        \u001b[35m1.1362\u001b[0m     +  1.2903\n",
      "      9        \u001b[36m1.0927\u001b[0m       \u001b[32m0.6828\u001b[0m        \u001b[35m1.0859\u001b[0m     +  1.2934\n",
      "     10        \u001b[36m1.0161\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m1.0119\u001b[0m     +  1.2898\n",
      "     11        \u001b[36m0.8591\u001b[0m       0.6828        \u001b[35m0.9431\u001b[0m     +  1.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12        \u001b[36m0.7775\u001b[0m       0.6690        \u001b[35m0.8793\u001b[0m     +  1.2915\n",
      "     13        \u001b[36m0.6439\u001b[0m       \u001b[32m0.7103\u001b[0m        \u001b[35m0.7744\u001b[0m     +  1.2914\n",
      "     14        \u001b[36m0.5674\u001b[0m       \u001b[32m0.7241\u001b[0m        \u001b[35m0.7686\u001b[0m     +  1.2902\n",
      "     15        \u001b[36m0.5062\u001b[0m       0.7172        0.7952        1.2913\n",
      "     16        \u001b[36m0.3787\u001b[0m       \u001b[32m0.7379\u001b[0m        \u001b[35m0.7203\u001b[0m     +  1.2854\n",
      "     17        \u001b[36m0.2931\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.6938\u001b[0m     +  1.2901\n",
      "     18        \u001b[36m0.2389\u001b[0m       \u001b[32m0.7655\u001b[0m        0.6967        1.2908\n",
      "     19        \u001b[36m0.2158\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m0.6433\u001b[0m     +  1.2877\n",
      "     20        \u001b[36m0.1752\u001b[0m       0.7586        0.7405        1.2923\n",
      "     21        \u001b[36m0.1309\u001b[0m       0.7655        0.7983        1.2894\n",
      "     22        \u001b[36m0.1167\u001b[0m       0.7379        0.9087        1.2889\n",
      "     23        \u001b[36m0.1104\u001b[0m       \u001b[32m0.7862\u001b[0m        0.7134        1.2880\n",
      "     24        \u001b[36m0.0811\u001b[0m       0.7793        0.7893        1.2882\n",
      "     25        \u001b[36m0.0804\u001b[0m       0.7862        0.7096        1.2895\n",
      "     26        \u001b[36m0.0607\u001b[0m       0.7655        0.8548        1.2870\n",
      "     27        \u001b[36m0.0534\u001b[0m       0.7793        0.8013        1.2889\n",
      "     28        \u001b[36m0.0421\u001b[0m       0.7724        0.8754        1.2885\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.5, module__hidden_features=256;, score=-0.921 total time= 1.5min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m3.5620\u001b[0m       \u001b[32m0.1389\u001b[0m        \u001b[35m1.6438\u001b[0m     +  1.1955\n",
      "      2        \u001b[36m2.0442\u001b[0m       \u001b[32m0.2431\u001b[0m        \u001b[35m1.5968\u001b[0m     +  1.1920\n",
      "      3        \u001b[36m1.7899\u001b[0m       \u001b[32m0.3194\u001b[0m        1.6004        1.1918\n",
      "      4        \u001b[36m1.7106\u001b[0m       0.2639        \u001b[35m1.5897\u001b[0m     +  1.1910\n",
      "      5        \u001b[36m1.6880\u001b[0m       0.1875        \u001b[35m1.5886\u001b[0m     +  1.1916\n",
      "      6        \u001b[36m1.6265\u001b[0m       0.2986        \u001b[35m1.5820\u001b[0m     +  1.1919\n",
      "      7        \u001b[36m1.6221\u001b[0m       \u001b[32m0.3264\u001b[0m        1.5893        1.1930\n",
      "      8        \u001b[36m1.6180\u001b[0m       0.3194        1.5869        1.1912\n",
      "      9        \u001b[36m1.5671\u001b[0m       0.2778        \u001b[35m1.5762\u001b[0m     +  1.1904\n",
      "     10        1.5793       0.3264        1.5771        1.1918\n",
      "     11        1.6164       \u001b[32m0.3403\u001b[0m        \u001b[35m1.5748\u001b[0m     +  1.1928\n",
      "     12        1.5994       \u001b[32m0.3611\u001b[0m        1.5774        1.1922\n",
      "     13        1.6097       0.3333        1.5756        1.1908\n",
      "     14        1.5712       0.3264        \u001b[35m1.5607\u001b[0m     +  1.1913\n",
      "     15        \u001b[36m1.5654\u001b[0m       0.3056        \u001b[35m1.5580\u001b[0m     +  1.1917\n",
      "     16        1.6043       \u001b[32m0.4028\u001b[0m        1.5665        1.2045\n",
      "     17        \u001b[36m1.5628\u001b[0m       0.3750        1.5673        1.1979\n",
      "     18        \u001b[36m1.5508\u001b[0m       0.3403        \u001b[35m1.5499\u001b[0m     +  1.1912\n",
      "     19        \u001b[36m1.5444\u001b[0m       0.3819        \u001b[35m1.5472\u001b[0m     +  1.1899\n",
      "     20        1.5644       0.3750        1.5527        1.1938\n",
      "     21        \u001b[36m1.5354\u001b[0m       0.3333        1.5511        1.1890\n",
      "     22        1.5622       0.3958        \u001b[35m1.5311\u001b[0m     +  1.1916\n",
      "     23        \u001b[36m1.4839\u001b[0m       0.3958        \u001b[35m1.5022\u001b[0m     +  1.1902\n",
      "     24        \u001b[36m1.4755\u001b[0m       \u001b[32m0.4097\u001b[0m        \u001b[35m1.4810\u001b[0m     +  1.1934\n",
      "     25        \u001b[36m1.4674\u001b[0m       \u001b[32m0.4306\u001b[0m        \u001b[35m1.4775\u001b[0m     +  1.1914\n",
      "     26        \u001b[36m1.4619\u001b[0m       \u001b[32m0.4931\u001b[0m        \u001b[35m1.4668\u001b[0m     +  1.1930\n",
      "     27        \u001b[36m1.4463\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.4201\u001b[0m     +  1.1923\n",
      "     28        \u001b[36m1.3940\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m1.3975\u001b[0m     +  1.1908\n",
      "     29        \u001b[36m1.3710\u001b[0m       0.5069        \u001b[35m1.3732\u001b[0m     +  1.1905\n",
      "     30        \u001b[36m1.3701\u001b[0m       \u001b[32m0.5347\u001b[0m        \u001b[35m1.3636\u001b[0m     +  1.1898\n",
      "     31        \u001b[36m1.3509\u001b[0m       \u001b[32m0.5625\u001b[0m        \u001b[35m1.3412\u001b[0m     +  1.1900\n",
      "     32        \u001b[36m1.2708\u001b[0m       0.5069        1.3467        1.1927\n",
      "     33        \u001b[36m1.2410\u001b[0m       \u001b[32m0.5903\u001b[0m        \u001b[35m1.2951\u001b[0m     +  1.1903\n",
      "     34        1.2697       0.5417        \u001b[35m1.2855\u001b[0m     +  1.1915\n",
      "     35        1.2425       \u001b[32m0.6181\u001b[0m        \u001b[35m1.2546\u001b[0m     +  1.1921\n",
      "     36        \u001b[36m1.2217\u001b[0m       0.5903        \u001b[35m1.2349\u001b[0m     +  1.1917\n",
      "     37        \u001b[36m1.2074\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m1.1666\u001b[0m     +  1.1918\n",
      "     38        \u001b[36m1.1775\u001b[0m       0.5972        1.1794        1.1921\n",
      "     39        \u001b[36m1.1278\u001b[0m       0.6181        1.1806        1.1894\n",
      "     40        \u001b[36m1.0208\u001b[0m       0.6597        \u001b[35m1.1275\u001b[0m     +  1.1886\n",
      "     41        1.0834       0.6319        1.1324        1.1915\n",
      "     42        1.0335       0.6250        \u001b[35m1.1236\u001b[0m     +  1.1927\n",
      "     43        \u001b[36m1.0151\u001b[0m       \u001b[32m0.7014\u001b[0m        \u001b[35m1.0547\u001b[0m     +  1.1923\n",
      "     44        \u001b[36m0.9828\u001b[0m       0.6667        1.0840        1.1917\n",
      "     45        \u001b[36m0.9324\u001b[0m       0.6389        1.0902        1.1902\n",
      "     46        \u001b[36m0.9050\u001b[0m       0.6736        \u001b[35m1.0450\u001b[0m     +  1.1894\n",
      "     47        0.9407       0.6944        \u001b[35m1.0450\u001b[0m     +  1.1921\n",
      "     48        \u001b[36m0.9020\u001b[0m       0.6736        1.0551        1.1921\n",
      "     49        \u001b[36m0.8351\u001b[0m       \u001b[32m0.7083\u001b[0m        \u001b[35m1.0155\u001b[0m     +  1.1915\n",
      "     50        \u001b[36m0.8309\u001b[0m       0.6389        1.0698        1.1928\n",
      "     51        \u001b[36m0.8074\u001b[0m       \u001b[32m0.7292\u001b[0m        \u001b[35m0.9628\u001b[0m     +  1.1937\n",
      "     52        \u001b[36m0.7807\u001b[0m       0.6528        1.0649        1.1908\n",
      "     53        \u001b[36m0.7789\u001b[0m       0.6944        \u001b[35m0.9606\u001b[0m     +  1.1899\n",
      "     54        \u001b[36m0.7443\u001b[0m       0.7014        0.9692        1.1921\n",
      "     55        \u001b[36m0.7334\u001b[0m       0.6458        1.0319        1.1909\n",
      "     56        \u001b[36m0.7288\u001b[0m       0.7014        0.9773        1.1909\n",
      "     57        \u001b[36m0.7015\u001b[0m       0.6875        \u001b[35m0.9547\u001b[0m     +  1.1906\n",
      "     58        \u001b[36m0.6911\u001b[0m       0.6806        0.9707        1.1922\n",
      "     59        \u001b[36m0.6735\u001b[0m       0.6944        0.9642        1.1908\n",
      "     60        \u001b[36m0.6400\u001b[0m       0.7083        \u001b[35m0.9277\u001b[0m     +  1.1913\n",
      "     61        \u001b[36m0.6159\u001b[0m       \u001b[32m0.7361\u001b[0m        \u001b[35m0.8918\u001b[0m     +  1.1925\n",
      "     62        \u001b[36m0.6080\u001b[0m       0.6736        0.9865        1.1924\n",
      "     63        0.6534       0.6944        0.9373        1.1908\n",
      "     64        \u001b[36m0.5861\u001b[0m       0.7292        \u001b[35m0.8916\u001b[0m     +  1.1911\n",
      "     65        0.6287       0.6806        1.0665        1.1906\n",
      "     66        \u001b[36m0.5557\u001b[0m       0.6806        0.9646        1.1908\n",
      "     67        \u001b[36m0.5475\u001b[0m       0.7083        0.9892        1.1901\n",
      "     68        0.5757       0.6736        1.0195        1.1912\n",
      "     69        0.5569       0.6736        0.9744        1.1896\n",
      "     70        \u001b[36m0.5307\u001b[0m       0.7361        0.9028        1.1902\n",
      "     71        \u001b[36m0.5263\u001b[0m       0.7153        0.9364        1.1908\n",
      "     72        \u001b[36m0.5154\u001b[0m       0.6944        0.9979        1.1901\n",
      "     73        \u001b[36m0.5041\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.8627\u001b[0m     +  1.1915\n",
      "     74        \u001b[36m0.4980\u001b[0m       0.7500        \u001b[35m0.8394\u001b[0m     +  1.1927\n",
      "     75        0.5009       0.7222        0.9275        1.1929\n",
      "     76        \u001b[36m0.4922\u001b[0m       0.7292        0.9990        1.1978\n",
      "     77        0.5166       0.7083        0.8613        1.1916\n",
      "     78        0.5184       0.7292        0.9473        1.1924\n",
      "     79        \u001b[36m0.4783\u001b[0m       0.6806        1.0634        1.1920\n",
      "     80        \u001b[36m0.4396\u001b[0m       0.7153        1.0117        1.1902\n",
      "     81        0.4784       0.7292        0.9205        1.1885\n",
      "     82        0.4705       0.7361        0.8851        1.1902\n",
      "     83        0.5073       0.7083        0.9810        1.1907\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.75, module__hidden_features=64;, score=-1.012 total time= 2.2min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.2366\u001b[0m       \u001b[32m0.2207\u001b[0m        \u001b[35m1.5961\u001b[0m     +  1.1919\n",
      "      2        \u001b[36m2.0769\u001b[0m       \u001b[32m0.2483\u001b[0m        1.6013        1.1920\n",
      "      3        \u001b[36m1.8054\u001b[0m       0.2000        1.6037        1.1935\n",
      "      4        \u001b[36m1.6563\u001b[0m       \u001b[32m0.2759\u001b[0m        1.5987        1.1927\n",
      "      5        \u001b[36m1.6276\u001b[0m       0.2690        1.5986        1.1919\n",
      "      6        \u001b[36m1.6044\u001b[0m       0.2483        1.5972        1.1934\n",
      "      7        \u001b[36m1.6004\u001b[0m       0.2759        \u001b[35m1.5908\u001b[0m     +  1.1913\n",
      "      8        \u001b[36m1.5941\u001b[0m       0.2414        \u001b[35m1.5856\u001b[0m     +  1.1945\n",
      "      9        1.6026       0.2483        1.5995        1.2109\n",
      "     10        1.5942       0.2621        1.5969        1.1912\n",
      "     11        1.6025       \u001b[32m0.2828\u001b[0m        1.5948        1.1943\n",
      "     12        1.5988       0.2483        1.6003        1.1908\n",
      "     13        1.5996       0.2414        1.6014        1.1899\n",
      "     14        \u001b[36m1.5854\u001b[0m       0.2483        \u001b[35m1.5804\u001b[0m     +  1.1910\n",
      "     15        1.6042       0.2483        1.5928        1.1925\n",
      "     16        \u001b[36m1.5772\u001b[0m       0.2828        \u001b[35m1.5732\u001b[0m     +  1.1901\n",
      "     17        \u001b[36m1.5666\u001b[0m       0.2690        \u001b[35m1.5625\u001b[0m     +  1.1918\n",
      "     18        1.5828       0.2828        1.5689        1.1941\n",
      "     19        \u001b[36m1.5609\u001b[0m       0.2690        \u001b[35m1.5516\u001b[0m     +  1.1941\n",
      "     20        1.5650       0.2414        1.5579        1.1956\n",
      "     21        \u001b[36m1.5598\u001b[0m       0.2552        1.5603        1.1922\n",
      "     22        1.5725       0.2759        \u001b[35m1.5515\u001b[0m     +  1.1916\n",
      "     23        1.5616       \u001b[32m0.3310\u001b[0m        \u001b[35m1.5348\u001b[0m     +  1.1937\n",
      "     24        1.5730       0.2828        \u001b[35m1.5281\u001b[0m     +  1.1936\n",
      "     25        \u001b[36m1.5505\u001b[0m       0.2414        1.5389        1.1927\n",
      "     26        1.5718       0.2414        1.5796        1.1919\n",
      "     27        1.5507       0.2414        1.5431        1.1916\n",
      "     28        1.5742       0.2621        1.5417        1.1931\n",
      "     29        1.5550       0.2966        1.5364        1.1917\n",
      "     30        1.5731       0.2966        1.5481        1.1908\n",
      "     31        1.5550       0.2345        1.5875        1.1919\n",
      "     32        1.5636       0.2414        1.5779        1.1900\n",
      "     33        1.5515       0.2483        1.5367        1.2087\n",
      "     34        \u001b[36m1.5299\u001b[0m       0.2897        \u001b[35m1.4683\u001b[0m     +  1.2121\n",
      "     35        \u001b[36m1.5288\u001b[0m       0.2690        1.5553        1.2099\n",
      "     36        1.5452       0.2483        1.5456        1.2063\n",
      "     37        1.5416       0.2414        1.5219        1.1996\n",
      "     38        \u001b[36m1.4885\u001b[0m       0.2966        \u001b[35m1.4651\u001b[0m     +  1.1917\n",
      "     39        1.5170       0.2621        1.5025        1.1951\n",
      "     40        1.5032       0.2414        1.4952        1.1929\n",
      "     41        1.5093       0.2414        1.4931        1.2092\n",
      "     42        1.4991       0.2621        1.4784        1.2054\n",
      "     43        \u001b[36m1.4749\u001b[0m       0.2621        \u001b[35m1.4467\u001b[0m     +  1.1930\n",
      "     44        1.5067       \u001b[32m0.3379\u001b[0m        1.5062        1.1953\n",
      "     45        1.4987       0.2414        1.4915        1.1933\n",
      "     46        \u001b[36m1.4643\u001b[0m       0.2414        1.4712        1.1933\n",
      "     47        1.5168       0.2966        1.4963        1.1944\n",
      "     48        1.4842       0.3172        1.4553        1.1942\n",
      "     49        1.4933       0.2897        1.4717        1.1936\n",
      "     50        \u001b[36m1.4269\u001b[0m       0.2690        1.4589        1.1918\n",
      "     51        1.4706       \u001b[32m0.3586\u001b[0m        \u001b[35m1.4401\u001b[0m     +  1.1926\n",
      "     52        1.4339       0.2828        \u001b[35m1.4303\u001b[0m     +  1.1941\n",
      "     53        1.4290       0.3034        1.4533        1.1945\n",
      "     54        \u001b[36m1.4182\u001b[0m       0.3241        \u001b[35m1.4105\u001b[0m     +  1.1923\n",
      "     55        1.4297       0.3379        \u001b[35m1.3811\u001b[0m     +  1.1939\n",
      "     56        \u001b[36m1.3525\u001b[0m       0.3586        1.3919        1.1935\n",
      "     57        1.3974       0.3172        1.3907        1.1936\n",
      "     58        1.3852       0.3172        1.3861        1.1937\n",
      "     59        1.3840       0.3448        \u001b[35m1.3744\u001b[0m     +  1.1928\n",
      "     60        \u001b[36m1.3439\u001b[0m       0.3586        \u001b[35m1.3727\u001b[0m     +  1.1943\n",
      "     61        1.3626       0.3586        \u001b[35m1.3525\u001b[0m     +  1.2307\n",
      "     62        1.3820       \u001b[32m0.3931\u001b[0m        1.3928        1.1948\n",
      "     63        \u001b[36m1.3352\u001b[0m       0.3448        1.3566        1.1936\n",
      "     64        1.3401       0.3724        \u001b[35m1.3369\u001b[0m     +  1.1913\n",
      "     65        1.3445       0.3862        \u001b[35m1.3259\u001b[0m     +  1.2298\n",
      "     66        \u001b[36m1.3152\u001b[0m       \u001b[32m0.4000\u001b[0m        1.3261        1.2161\n",
      "     67        1.3418       0.3793        1.3348        1.2046\n",
      "     68        1.3189       \u001b[32m0.4483\u001b[0m        1.3264        1.2142\n",
      "     69        1.3161       \u001b[32m0.4759\u001b[0m        1.3620        1.2543\n",
      "     70        \u001b[36m1.2950\u001b[0m       \u001b[32m0.5172\u001b[0m        1.3318        1.2408\n",
      "     71        1.3152       0.4966        \u001b[35m1.3120\u001b[0m     +  1.2545\n",
      "     72        \u001b[36m1.2751\u001b[0m       0.4483        1.3259        1.2224\n",
      "     73        1.3057       \u001b[32m0.5241\u001b[0m        1.3205        1.2147\n",
      "     74        1.2939       0.5103        \u001b[35m1.3040\u001b[0m     +  1.2011\n",
      "     75        1.3012       0.4897        \u001b[35m1.2929\u001b[0m     +  1.2087\n",
      "     76        \u001b[36m1.2742\u001b[0m       \u001b[32m0.5655\u001b[0m        1.3032        1.1996\n",
      "     77        \u001b[36m1.2588\u001b[0m       0.5241        1.2937        1.1987\n",
      "     78        1.2670       0.5241        \u001b[35m1.2732\u001b[0m     +  1.1979\n",
      "     79        1.2679       \u001b[32m0.5724\u001b[0m        1.2945        1.1970\n",
      "     80        1.2837       0.5517        1.2947        1.1953\n",
      "     81        1.2696       0.5310        1.2823        1.1973\n",
      "     82        \u001b[36m1.2513\u001b[0m       0.5379        \u001b[35m1.2530\u001b[0m     +  1.1971\n",
      "     83        1.2593       0.5448        1.3090        1.1964\n",
      "     84        1.2607       0.5586        1.2746        1.1979\n",
      "     85        \u001b[36m1.2408\u001b[0m       0.4966        1.2842        1.1977\n",
      "     86        1.2463       0.5586        1.2767        1.1962\n",
      "     87        1.2486       0.5448        1.2619        1.1972\n",
      "     88        \u001b[36m1.1834\u001b[0m       0.5103        1.2663        1.1956\n",
      "     89        1.2147       0.5655        1.2860        1.1973\n",
      "     90        1.2129       0.5172        \u001b[35m1.2512\u001b[0m     +  1.1954\n",
      "     91        1.1913       0.5448        \u001b[35m1.2390\u001b[0m     +  1.1960\n",
      "     92        \u001b[36m1.1690\u001b[0m       0.4483        1.2964        1.1987\n",
      "     93        1.2076       0.5310        1.2646        1.1957\n",
      "     94        \u001b[36m1.1602\u001b[0m       0.5448        1.2419        1.1958\n",
      "     95        1.1986       0.5655        \u001b[35m1.2255\u001b[0m     +  1.1981\n",
      "     96        \u001b[36m1.1489\u001b[0m       0.4897        1.2442        1.1964\n",
      "     97        1.2121       0.5586        \u001b[35m1.2052\u001b[0m     +  1.1954\n",
      "     98        1.1540       0.5586        1.2145        1.1946\n",
      "     99        1.1531       0.5586        1.2467        1.1946\n",
      "    100        \u001b[36m1.1293\u001b[0m       0.5241        1.2717        1.1952\n",
      "[CV 2/3] END module__dropout=0.75, module__hidden_features=64;, score=-1.239 total time= 2.4min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.6445\u001b[0m       \u001b[32m0.2483\u001b[0m        \u001b[35m1.6533\u001b[0m     +  1.1997\n",
      "      2        \u001b[36m2.5202\u001b[0m       0.2069        \u001b[35m1.6405\u001b[0m     +  1.1960\n",
      "      3        \u001b[36m1.9105\u001b[0m       0.2276        \u001b[35m1.5966\u001b[0m     +  1.1964\n",
      "      4        \u001b[36m1.7020\u001b[0m       0.2138        \u001b[35m1.5956\u001b[0m     +  1.1965\n",
      "      5        \u001b[36m1.6544\u001b[0m       0.1931        \u001b[35m1.5927\u001b[0m     +  1.1950\n",
      "      6        \u001b[36m1.6178\u001b[0m       0.2345        1.5993        1.1954\n",
      "      7        \u001b[36m1.6160\u001b[0m       0.1793        1.6108        1.1948\n",
      "      8        \u001b[36m1.6141\u001b[0m       0.2345        1.5997        1.1944\n",
      "      9        \u001b[36m1.6104\u001b[0m       \u001b[32m0.3034\u001b[0m        \u001b[35m1.5830\u001b[0m     +  1.1944\n",
      "     10        1.6141       0.2000        1.6071        1.1962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        1.6120       0.2345        1.5988        1.1950\n",
      "     12        \u001b[36m1.5979\u001b[0m       0.2828        \u001b[35m1.5761\u001b[0m     +  1.1949\n",
      "     13        1.6122       0.2414        1.5964        1.1945\n",
      "     14        \u001b[36m1.5855\u001b[0m       0.2552        1.5929        1.1952\n",
      "     15        1.6134       0.2552        1.5963        1.2012\n",
      "     16        1.6044       0.2483        1.5965        1.1958\n",
      "     17        1.5984       \u001b[32m0.3103\u001b[0m        1.5783        1.1939\n",
      "     18        1.5997       0.3103        \u001b[35m1.5681\u001b[0m     +  1.1949\n",
      "     19        1.6000       \u001b[32m0.4483\u001b[0m        \u001b[35m1.5660\u001b[0m     +  1.1976\n",
      "     20        1.5995       0.3379        1.5890        1.1960\n",
      "     21        1.5877       \u001b[32m0.4690\u001b[0m        \u001b[35m1.5512\u001b[0m     +  1.1953\n",
      "     22        1.6042       0.4414        1.5647        1.1952\n",
      "     23        1.5874       0.4414        1.5595        1.1948\n",
      "     24        \u001b[36m1.5763\u001b[0m       0.4414        \u001b[35m1.5505\u001b[0m     +  1.1938\n",
      "     25        1.5799       0.4552        \u001b[35m1.5445\u001b[0m     +  1.1963\n",
      "     26        1.5930       0.4552        1.5501        1.1949\n",
      "     27        1.5907       0.4483        1.5508        1.1953\n",
      "     28        1.5764       0.4621        \u001b[35m1.5229\u001b[0m     +  1.1949\n",
      "     29        \u001b[36m1.5697\u001b[0m       \u001b[32m0.4897\u001b[0m        1.5292        1.1956\n",
      "     30        \u001b[36m1.5556\u001b[0m       0.4483        \u001b[35m1.5163\u001b[0m     +  1.1964\n",
      "     31        1.5765       0.4483        1.5446        1.1972\n",
      "     32        1.5727       0.4207        \u001b[35m1.5132\u001b[0m     +  1.1967\n",
      "     33        1.5846       0.4483        1.5540        1.1978\n",
      "     34        1.5733       0.4276        1.5137        1.1956\n",
      "     35        1.5657       0.4207        1.5317        1.1959\n",
      "     36        1.5815       0.4276        1.5193        1.1962\n",
      "     37        1.5562       0.4897        \u001b[35m1.5028\u001b[0m     +  1.1955\n",
      "     38        1.5670       0.4759        1.5410        1.1968\n",
      "     39        1.5800       0.4414        1.5219        1.1975\n",
      "     40        1.5832       0.4069        1.5324        1.1964\n",
      "     41        1.5892       0.3517        1.5699        1.1967\n",
      "     42        1.5561       0.3241        \u001b[35m1.4734\u001b[0m     +  1.1970\n",
      "     43        1.5897       0.3862        1.5664        1.1970\n",
      "     44        1.5799       0.4000        1.5336        1.2020\n",
      "     45        1.5820       0.4483        1.4773        1.1975\n",
      "     46        1.5622       0.4621        1.5234        1.1968\n",
      "     47        \u001b[36m1.5487\u001b[0m       0.4483        1.4997        1.1969\n",
      "     48        1.5626       0.4690        1.5084        1.1954\n",
      "     49        1.5590       0.4414        1.5147        1.1971\n",
      "     50        1.5638       0.3931        1.5393        1.1968\n",
      "     51        1.5496       0.4483        1.4860        1.1971\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.75, module__hidden_features=64;, score=-1.522 total time= 1.3min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m5.5602\u001b[0m       \u001b[32m0.1528\u001b[0m        \u001b[35m1.6397\u001b[0m     +  1.2219\n",
      "      2        \u001b[36m2.8741\u001b[0m       \u001b[32m0.2847\u001b[0m        \u001b[35m1.5770\u001b[0m     +  1.2273\n",
      "      3        \u001b[36m1.9037\u001b[0m       0.2500        \u001b[35m1.5540\u001b[0m     +  1.2286\n",
      "      4        \u001b[36m1.7256\u001b[0m       \u001b[32m0.3819\u001b[0m        1.5621        1.2278\n",
      "      5        \u001b[36m1.6200\u001b[0m       0.2986        1.5748        1.2270\n",
      "      6        \u001b[36m1.6068\u001b[0m       0.3681        1.5744        1.2267\n",
      "      7        \u001b[36m1.6054\u001b[0m       0.3542        1.5825        1.2265\n",
      "      8        \u001b[36m1.5998\u001b[0m       0.2917        1.5856        1.2274\n",
      "      9        \u001b[36m1.5788\u001b[0m       0.3125        1.5793        1.2267\n",
      "     10        1.5838       0.2917        1.5682        1.2272\n",
      "     11        1.5908       0.2917        1.5655        1.2268\n",
      "     12        \u001b[36m1.5615\u001b[0m       0.3125        \u001b[35m1.5409\u001b[0m     +  1.2273\n",
      "     13        1.5861       \u001b[32m0.3958\u001b[0m        \u001b[35m1.5264\u001b[0m     +  1.2270\n",
      "     14        \u001b[36m1.5442\u001b[0m       0.3889        1.5325        1.2266\n",
      "     15        1.5671       0.3194        1.5358        1.2263\n",
      "     16        1.5478       0.3889        \u001b[35m1.5123\u001b[0m     +  1.2252\n",
      "     17        1.5536       0.3819        \u001b[35m1.5114\u001b[0m     +  1.2265\n",
      "     18        \u001b[36m1.5334\u001b[0m       0.3125        1.5239        1.2278\n",
      "     19        \u001b[36m1.5270\u001b[0m       0.3750        1.5246        1.2264\n",
      "     20        1.5357       \u001b[32m0.4306\u001b[0m        1.5142        1.2261\n",
      "     21        1.5379       0.3889        \u001b[35m1.4999\u001b[0m     +  1.2253\n",
      "     22        1.5275       \u001b[32m0.4653\u001b[0m        \u001b[35m1.4505\u001b[0m     +  1.2254\n",
      "     23        \u001b[36m1.4933\u001b[0m       0.4375        1.4600        1.2251\n",
      "     24        1.5204       0.4514        1.4883        1.2246\n",
      "     25        \u001b[36m1.4857\u001b[0m       0.4444        1.4851        1.2246\n",
      "     26        \u001b[36m1.4837\u001b[0m       0.4375        1.4677        1.2242\n",
      "     27        1.5019       \u001b[32m0.4792\u001b[0m        \u001b[35m1.4216\u001b[0m     +  1.2244\n",
      "     28        \u001b[36m1.4773\u001b[0m       \u001b[32m0.5139\u001b[0m        1.4462        1.2295\n",
      "     29        1.4797       0.4306        1.4571        1.2276\n",
      "     30        1.4800       0.4931        1.4342        1.2276\n",
      "     31        1.4791       0.4792        1.4293        1.2284\n",
      "     32        \u001b[36m1.4260\u001b[0m       0.4931        1.4222        1.2278\n",
      "     33        \u001b[36m1.3980\u001b[0m       0.4861        \u001b[35m1.4185\u001b[0m     +  1.2268\n",
      "     34        1.4368       \u001b[32m0.5625\u001b[0m        \u001b[35m1.3828\u001b[0m     +  1.2277\n",
      "     35        \u001b[36m1.3921\u001b[0m       0.5556        1.4052        1.2281\n",
      "     36        1.3929       0.5417        \u001b[35m1.3595\u001b[0m     +  1.2257\n",
      "     37        \u001b[36m1.3906\u001b[0m       \u001b[32m0.5694\u001b[0m        1.3759        1.2283\n",
      "     38        1.3975       0.4931        1.3649        1.2276\n",
      "     39        1.4201       0.5278        1.3870        1.2275\n",
      "     40        \u001b[36m1.3388\u001b[0m       0.5417        \u001b[35m1.3433\u001b[0m     +  1.2271\n",
      "     41        1.4097       0.5278        1.3577        1.2281\n",
      "     42        \u001b[36m1.3144\u001b[0m       0.5486        1.3472        1.2277\n",
      "     43        1.3288       0.5625        1.3587        1.2269\n",
      "     44        1.3150       0.5208        1.3514        1.2272\n",
      "     45        1.3164       0.4931        \u001b[35m1.3078\u001b[0m     +  1.2269\n",
      "     46        1.3213       0.5069        1.3667        1.2269\n",
      "     47        1.3385       0.5694        1.3269        1.2259\n",
      "     48        \u001b[36m1.3034\u001b[0m       \u001b[32m0.6181\u001b[0m        \u001b[35m1.2879\u001b[0m     +  1.2241\n",
      "     49        \u001b[36m1.2666\u001b[0m       0.5139        \u001b[35m1.2870\u001b[0m     +  1.2269\n",
      "     50        1.3049       0.5139        1.4233        1.2275\n",
      "     51        1.2850       0.5625        1.2879        1.2288\n",
      "     52        \u001b[36m1.2493\u001b[0m       0.5833        1.2935        1.2272\n",
      "     53        1.2499       0.5833        \u001b[35m1.2842\u001b[0m     +  1.2254\n",
      "     54        1.2542       0.5556        \u001b[35m1.2681\u001b[0m     +  1.2284\n",
      "     55        \u001b[36m1.2455\u001b[0m       0.5486        1.2877        1.2265\n",
      "     56        \u001b[36m1.2284\u001b[0m       0.5556        1.3004        1.2251\n",
      "     57        1.2312       0.5486        1.3083        1.2262\n",
      "     58        \u001b[36m1.1949\u001b[0m       0.6042        \u001b[35m1.2492\u001b[0m     +  1.2258\n",
      "     59        1.2474       0.6111        \u001b[35m1.2401\u001b[0m     +  1.2270\n",
      "     60        1.2051       0.6111        1.2749        1.2274\n",
      "     61        \u001b[36m1.1879\u001b[0m       0.6111        \u001b[35m1.2231\u001b[0m     +  1.2272\n",
      "     62        \u001b[36m1.1809\u001b[0m       0.5903        1.2269        1.2276\n",
      "     63        \u001b[36m1.1663\u001b[0m       0.5972        1.2755        1.2261\n",
      "     64        \u001b[36m1.1385\u001b[0m       0.5694        \u001b[35m1.1967\u001b[0m     +  1.2251\n",
      "     65        1.1484       \u001b[32m0.6319\u001b[0m        1.2357        1.2273\n",
      "     66        1.1707       0.6181        1.2370        1.2260\n",
      "     67        \u001b[36m1.1274\u001b[0m       0.5972        1.2404        1.2270\n",
      "     68        \u001b[36m1.1263\u001b[0m       0.5972        \u001b[35m1.1791\u001b[0m     +  1.2262\n",
      "     69        \u001b[36m1.1065\u001b[0m       0.6042        1.2129        1.2261\n",
      "     70        \u001b[36m1.0563\u001b[0m       \u001b[32m0.6458\u001b[0m        1.2551        1.2259\n",
      "     71        1.0964       0.6319        1.2651        1.2266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     72        1.0776       0.6042        1.2101        1.2269\n",
      "     73        1.0880       0.5903        1.2793        1.2260\n",
      "     74        1.0793       0.6250        1.2255        1.2255\n",
      "     75        \u001b[36m1.0372\u001b[0m       0.6181        1.2249        1.2261\n",
      "     76        1.0726       0.6042        1.2382        1.2261\n",
      "     77        1.0421       0.6111        1.1953        1.2253\n",
      "     78        1.0526       0.5972        \u001b[35m1.1725\u001b[0m     +  1.2266\n",
      "     79        1.0640       0.6250        1.3121        1.2271\n",
      "     80        1.0539       0.6042        1.2536        1.2273\n",
      "     81        \u001b[36m1.0094\u001b[0m       0.6250        \u001b[35m1.1536\u001b[0m     +  1.2270\n",
      "     82        1.0884       0.5972        1.2405        1.2287\n",
      "     83        1.0150       0.5694        1.2481        1.2258\n",
      "     84        1.0599       0.5417        1.2754        1.2248\n",
      "     85        1.0147       0.5833        1.2429        1.2253\n",
      "     86        1.0536       0.5764        1.2712        1.2261\n",
      "     87        \u001b[36m1.0046\u001b[0m       0.5764        1.2439        1.2247\n",
      "     88        1.0625       0.5972        1.2375        1.2247\n",
      "     89        1.0247       0.5903        1.2493        1.2272\n",
      "     90        1.0214       0.6042        1.2880        1.2281\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.75, module__hidden_features=128;, score=-1.342 total time= 2.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.9347\u001b[0m       \u001b[32m0.2276\u001b[0m        \u001b[35m1.6366\u001b[0m     +  1.2263\n",
      "      2        \u001b[36m2.7012\u001b[0m       \u001b[32m0.4345\u001b[0m        \u001b[35m1.5582\u001b[0m     +  1.2273\n",
      "      3        \u001b[36m1.8172\u001b[0m       0.3241        1.5716        1.2306\n",
      "      4        \u001b[36m1.6585\u001b[0m       0.3724        1.5742        1.2274\n",
      "      5        \u001b[36m1.5935\u001b[0m       0.3379        1.5709        1.2277\n",
      "      6        1.5991       0.3448        1.5714        1.2266\n",
      "      7        \u001b[36m1.5808\u001b[0m       0.3241        1.5687        1.2253\n",
      "      8        \u001b[36m1.5767\u001b[0m       0.3793        1.5639        1.2271\n",
      "      9        \u001b[36m1.5737\u001b[0m       0.4069        \u001b[35m1.5495\u001b[0m     +  1.2265\n",
      "     10        \u001b[36m1.5642\u001b[0m       0.4069        \u001b[35m1.5423\u001b[0m     +  1.2309\n",
      "     11        \u001b[36m1.5607\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m1.5334\u001b[0m     +  1.2338\n",
      "     12        \u001b[36m1.5587\u001b[0m       0.4345        \u001b[35m1.5286\u001b[0m     +  1.2282\n",
      "     13        1.5608       0.4069        1.5448        1.2282\n",
      "     14        \u001b[36m1.5509\u001b[0m       \u001b[32m0.4759\u001b[0m        \u001b[35m1.5181\u001b[0m     +  1.2273\n",
      "     15        \u001b[36m1.5225\u001b[0m       0.4276        \u001b[35m1.5018\u001b[0m     +  1.2275\n",
      "     16        1.5355       0.4483        1.5217        1.2288\n",
      "     17        \u001b[36m1.5189\u001b[0m       0.3655        1.5250        1.2283\n",
      "     18        1.5202       0.4207        \u001b[35m1.4680\u001b[0m     +  1.2260\n",
      "     19        \u001b[36m1.5093\u001b[0m       0.4690        1.4859        1.2279\n",
      "     20        \u001b[36m1.4817\u001b[0m       0.4276        \u001b[35m1.4505\u001b[0m     +  1.2285\n",
      "     21        1.4897       0.4759        1.4630        1.2306\n",
      "     22        1.5201       \u001b[32m0.4897\u001b[0m        1.4735        1.2289\n",
      "     23        1.5026       \u001b[32m0.5448\u001b[0m        \u001b[35m1.4284\u001b[0m     +  1.2283\n",
      "     24        \u001b[36m1.4784\u001b[0m       0.4966        1.4772        1.2284\n",
      "     25        \u001b[36m1.4489\u001b[0m       0.4897        \u001b[35m1.4057\u001b[0m     +  1.2275\n",
      "     26        1.4715       \u001b[32m0.5586\u001b[0m        1.4249        1.2286\n",
      "     27        \u001b[36m1.4424\u001b[0m       0.4966        \u001b[35m1.3824\u001b[0m     +  1.2266\n",
      "     28        1.4562       0.3931        1.4127        1.2294\n",
      "     29        1.4579       \u001b[32m0.5931\u001b[0m        1.4404        1.2273\n",
      "     30        1.4559       \u001b[32m0.6621\u001b[0m        1.3948        1.2274\n",
      "     31        \u001b[36m1.4282\u001b[0m       0.6207        \u001b[35m1.3616\u001b[0m     +  1.2259\n",
      "     32        \u001b[36m1.4028\u001b[0m       0.5931        \u001b[35m1.3512\u001b[0m     +  1.2304\n",
      "     33        1.4218       0.5517        \u001b[35m1.3368\u001b[0m     +  1.2297\n",
      "     34        \u001b[36m1.3670\u001b[0m       0.5793        \u001b[35m1.3317\u001b[0m     +  1.2280\n",
      "     35        1.3798       0.5517        \u001b[35m1.2937\u001b[0m     +  1.2289\n",
      "     36        1.3896       0.5724        1.3885        1.2274\n",
      "     37        \u001b[36m1.3530\u001b[0m       0.5172        \u001b[35m1.2748\u001b[0m     +  1.2291\n",
      "     38        1.4279       0.6414        1.2938        1.2285\n",
      "     39        1.3693       0.6276        1.2752        1.2283\n",
      "     40        \u001b[36m1.3316\u001b[0m       0.5793        \u001b[35m1.2686\u001b[0m     +  1.2295\n",
      "     41        1.3930       0.6345        1.3634        1.2281\n",
      "     42        1.3500       0.6414        \u001b[35m1.2265\u001b[0m     +  1.2272\n",
      "     43        1.3371       0.6552        1.2499        1.2299\n",
      "     44        1.3385       0.6414        1.3008        1.2287\n",
      "     45        1.3349       \u001b[32m0.6897\u001b[0m        \u001b[35m1.1956\u001b[0m     +  1.2280\n",
      "     46        \u001b[36m1.2520\u001b[0m       0.5379        \u001b[35m1.1592\u001b[0m     +  1.2295\n",
      "     47        1.2629       0.6828        1.1746        1.2289\n",
      "     48        1.2693       0.6828        1.1607        1.2284\n",
      "     49        \u001b[36m1.2349\u001b[0m       0.6690        \u001b[35m1.1191\u001b[0m     +  1.2284\n",
      "     50        \u001b[36m1.1763\u001b[0m       0.6897        1.1637        1.2289\n",
      "     51        1.2057       0.6345        1.1264        1.2289\n",
      "     52        \u001b[36m1.1649\u001b[0m       \u001b[32m0.7034\u001b[0m        \u001b[35m1.0960\u001b[0m     +  1.2321\n",
      "     53        \u001b[36m1.1439\u001b[0m       0.6966        1.1569        1.2292\n",
      "     54        \u001b[36m1.1303\u001b[0m       0.6759        \u001b[35m1.0654\u001b[0m     +  1.2282\n",
      "     55        \u001b[36m1.1138\u001b[0m       0.7034        \u001b[35m1.0579\u001b[0m     +  1.2278\n",
      "     56        1.1262       0.6345        1.1270        1.2278\n",
      "     57        \u001b[36m1.0995\u001b[0m       0.6759        1.1065        1.2288\n",
      "     58        \u001b[36m1.0531\u001b[0m       0.6759        1.0784        1.2292\n",
      "     59        1.0962       0.6414        1.1071        1.2281\n",
      "     60        1.0840       0.5862        1.2054        1.2285\n",
      "     61        1.1368       0.6897        \u001b[35m1.0397\u001b[0m     +  1.2299\n",
      "     62        \u001b[36m1.0175\u001b[0m       \u001b[32m0.7379\u001b[0m        \u001b[35m0.9562\u001b[0m     +  1.2288\n",
      "     63        \u001b[36m0.9964\u001b[0m       0.7379        1.0510        1.2291\n",
      "     64        1.0483       0.7241        0.9816        1.2284\n",
      "     65        0.9987       0.7034        1.0523        1.2315\n",
      "     66        1.0249       0.7310        \u001b[35m0.9549\u001b[0m     +  1.2286\n",
      "     67        \u001b[36m0.9618\u001b[0m       0.7310        \u001b[35m0.9439\u001b[0m     +  1.2284\n",
      "     68        1.0068       0.7379        1.0022        1.2297\n",
      "     69        1.0005       0.7103        1.0412        1.2291\n",
      "     70        1.0196       0.7172        1.0788        1.2299\n",
      "     71        1.0160       0.7379        1.0079        1.2298\n",
      "     72        0.9704       \u001b[32m0.7586\u001b[0m        \u001b[35m0.9059\u001b[0m     +  1.2290\n",
      "     73        0.9638       0.7448        0.9767        1.2280\n",
      "     74        0.9786       0.7448        0.9994        1.2296\n",
      "     75        \u001b[36m0.9218\u001b[0m       0.7379        0.9064        1.2288\n",
      "     76        0.9537       0.7448        0.9434        1.2291\n",
      "     77        \u001b[36m0.9184\u001b[0m       0.7448        0.9967        1.2263\n",
      "     78        \u001b[36m0.8907\u001b[0m       0.7379        0.9471        1.2283\n",
      "     79        \u001b[36m0.8727\u001b[0m       0.7448        0.9198        1.2268\n",
      "     80        0.9101       \u001b[32m0.7655\u001b[0m        \u001b[35m0.8425\u001b[0m     +  1.2282\n",
      "     81        0.9045       0.7379        0.8792        1.2297\n",
      "     82        0.8746       0.7586        0.8940        1.2282\n",
      "     83        0.8792       0.7655        0.8502        1.2270\n",
      "     84        0.8815       0.7379        0.9423        1.2269\n",
      "     85        \u001b[36m0.8333\u001b[0m       0.7379        0.8672        1.2264\n",
      "     86        \u001b[36m0.8095\u001b[0m       0.7517        0.8584        1.2272\n",
      "     87        \u001b[36m0.7716\u001b[0m       0.7655        0.8965        1.2276\n",
      "     88        0.8365       0.7448        0.9277        1.2278\n",
      "     89        \u001b[36m0.7656\u001b[0m       0.7310        0.9092        1.2283\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.75, module__hidden_features=128;, score=-0.870 total time= 2.7min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m4.7649\u001b[0m       \u001b[32m0.2552\u001b[0m        \u001b[35m1.5618\u001b[0m     +  1.2266\n",
      "      2        \u001b[36m2.7685\u001b[0m       \u001b[32m0.3034\u001b[0m        \u001b[35m1.5477\u001b[0m     +  1.2296\n",
      "      3        \u001b[36m2.0049\u001b[0m       0.3034        \u001b[35m1.5468\u001b[0m     +  1.2292\n",
      "      4        \u001b[36m1.7592\u001b[0m       0.2828        1.5928        1.2265\n",
      "      5        \u001b[36m1.6417\u001b[0m       0.2207        1.6015        1.2267\n",
      "      6        \u001b[36m1.6241\u001b[0m       \u001b[32m0.3586\u001b[0m        1.5854        1.2277\n",
      "      7        \u001b[36m1.5920\u001b[0m       0.3310        1.5716        1.2266\n",
      "      8        1.6033       0.3103        1.5679        1.2278\n",
      "      9        1.6165       0.3379        1.5737        1.2276\n",
      "     10        \u001b[36m1.5687\u001b[0m       \u001b[32m0.3655\u001b[0m        1.5502        1.2257\n",
      "     11        1.5746       0.3172        \u001b[35m1.5423\u001b[0m     +  1.2265\n",
      "     12        \u001b[36m1.5662\u001b[0m       0.3241        1.5549        1.2286\n",
      "     13        \u001b[36m1.5590\u001b[0m       0.3586        \u001b[35m1.5257\u001b[0m     +  1.2270\n",
      "     14        \u001b[36m1.5433\u001b[0m       \u001b[32m0.4276\u001b[0m        \u001b[35m1.4981\u001b[0m     +  1.2284\n",
      "     15        \u001b[36m1.5346\u001b[0m       \u001b[32m0.4621\u001b[0m        1.5105        1.2290\n",
      "     16        1.5356       0.3655        \u001b[35m1.4838\u001b[0m     +  1.2292\n",
      "     17        \u001b[36m1.5246\u001b[0m       0.3793        1.4844        1.2296\n",
      "     18        \u001b[36m1.4903\u001b[0m       0.4207        1.4949        1.2281\n",
      "     19        \u001b[36m1.4766\u001b[0m       0.4069        \u001b[35m1.4601\u001b[0m     +  1.2286\n",
      "     20        1.4799       0.4069        \u001b[35m1.4547\u001b[0m     +  1.2272\n",
      "     21        \u001b[36m1.4583\u001b[0m       \u001b[32m0.5379\u001b[0m        \u001b[35m1.4413\u001b[0m     +  1.2307\n",
      "     22        \u001b[36m1.4402\u001b[0m       0.5172        \u001b[35m1.3995\u001b[0m     +  1.2295\n",
      "     23        \u001b[36m1.3985\u001b[0m       0.5241        \u001b[35m1.3646\u001b[0m     +  1.2302\n",
      "     24        1.4391       \u001b[32m0.5448\u001b[0m        1.3801        1.2293\n",
      "     25        \u001b[36m1.3935\u001b[0m       0.5103        \u001b[35m1.3509\u001b[0m     +  1.2294\n",
      "     26        \u001b[36m1.3367\u001b[0m       \u001b[32m0.6000\u001b[0m        1.3770        1.2283\n",
      "     27        1.3682       0.5448        \u001b[35m1.3104\u001b[0m     +  1.2271\n",
      "     28        \u001b[36m1.3206\u001b[0m       0.5724        \u001b[35m1.2962\u001b[0m     +  1.2293\n",
      "     29        \u001b[36m1.2731\u001b[0m       0.5724        \u001b[35m1.2773\u001b[0m     +  1.2278\n",
      "     30        \u001b[36m1.2535\u001b[0m       \u001b[32m0.6345\u001b[0m        \u001b[35m1.2476\u001b[0m     +  1.2293\n",
      "     31        1.3212       0.5862        1.2500        1.2273\n",
      "     32        1.3079       \u001b[32m0.6414\u001b[0m        \u001b[35m1.2204\u001b[0m     +  1.2281\n",
      "     33        \u001b[36m1.2273\u001b[0m       0.6138        \u001b[35m1.1710\u001b[0m     +  1.2277\n",
      "     34        \u001b[36m1.1914\u001b[0m       0.6207        \u001b[35m1.1538\u001b[0m     +  1.2301\n",
      "     35        \u001b[36m1.1637\u001b[0m       0.6414        \u001b[35m1.1408\u001b[0m     +  1.2299\n",
      "     36        \u001b[36m1.1547\u001b[0m       \u001b[32m0.6621\u001b[0m        1.1510        1.2369\n",
      "     37        \u001b[36m1.1164\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m1.0797\u001b[0m     +  1.2267\n",
      "     38        \u001b[36m1.0896\u001b[0m       0.6690        \u001b[35m1.0583\u001b[0m     +  1.2287\n",
      "     39        1.1115       0.6759        \u001b[35m1.0514\u001b[0m     +  1.2271\n",
      "     40        \u001b[36m1.0682\u001b[0m       0.6483        1.0684        1.2337\n",
      "     41        1.0698       0.6690        1.0721        1.2276\n",
      "     42        1.0847       0.6483        1.0564        1.2275\n",
      "     43        \u001b[36m0.9882\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.9804\u001b[0m     +  1.2270\n",
      "     44        \u001b[36m0.9627\u001b[0m       0.6690        \u001b[35m0.9673\u001b[0m     +  1.2282\n",
      "     45        \u001b[36m0.9610\u001b[0m       0.6276        \u001b[35m0.9616\u001b[0m     +  1.2279\n",
      "     46        0.9785       0.6621        0.9792        1.2272\n",
      "     47        \u001b[36m0.9404\u001b[0m       0.6759        0.9648        1.2270\n",
      "     48        \u001b[36m0.8871\u001b[0m       0.6897        \u001b[35m0.9028\u001b[0m     +  1.2248\n",
      "     49        0.9114       \u001b[32m0.7172\u001b[0m        \u001b[35m0.8859\u001b[0m     +  1.2290\n",
      "     50        0.9180       \u001b[32m0.7379\u001b[0m        0.9063        1.2303\n",
      "     51        \u001b[36m0.8557\u001b[0m       0.7034        \u001b[35m0.8823\u001b[0m     +  1.2287\n",
      "     52        \u001b[36m0.8382\u001b[0m       0.6897        \u001b[35m0.8629\u001b[0m     +  1.2296\n",
      "     53        \u001b[36m0.8362\u001b[0m       0.6828        0.8936        1.2341\n",
      "     54        \u001b[36m0.7960\u001b[0m       0.6828        \u001b[35m0.8537\u001b[0m     +  1.2288\n",
      "     55        \u001b[36m0.7928\u001b[0m       0.7241        \u001b[35m0.8141\u001b[0m     +  1.2280\n",
      "     56        \u001b[36m0.7924\u001b[0m       0.7379        0.8247        1.2310\n",
      "     57        0.7936       0.7241        0.8174        1.2272\n",
      "     58        \u001b[36m0.7503\u001b[0m       0.7241        0.8255        1.2273\n",
      "     59        0.7616       0.6897        \u001b[35m0.8070\u001b[0m     +  1.2284\n",
      "     60        \u001b[36m0.7023\u001b[0m       0.7034        0.8761        1.2302\n",
      "     61        0.7457       0.7379        \u001b[35m0.7942\u001b[0m     +  1.2276\n",
      "     62        0.7149       0.7241        \u001b[35m0.7698\u001b[0m     +  1.2337\n",
      "     63        0.7289       0.7103        0.7927        1.2283\n",
      "     64        \u001b[36m0.6441\u001b[0m       0.7241        \u001b[35m0.7689\u001b[0m     +  1.2275\n",
      "     65        0.6768       \u001b[32m0.7448\u001b[0m        0.7772        1.2320\n",
      "     66        0.6491       0.7310        0.7885        1.2279\n",
      "     67        \u001b[36m0.6399\u001b[0m       0.7241        0.8225        1.2266\n",
      "     68        \u001b[36m0.6131\u001b[0m       0.7448        \u001b[35m0.7437\u001b[0m     +  1.2264\n",
      "     69        \u001b[36m0.6033\u001b[0m       \u001b[32m0.7517\u001b[0m        \u001b[35m0.7104\u001b[0m     +  1.2282\n",
      "     70        0.6054       \u001b[32m0.7655\u001b[0m        0.7235        1.2300\n",
      "     71        \u001b[36m0.6010\u001b[0m       0.7586        0.7332        1.2288\n",
      "     72        \u001b[36m0.5717\u001b[0m       0.7448        0.7365        1.2300\n",
      "     73        0.5780       0.7517        0.7325        1.2308\n",
      "     74        0.6003       0.7448        0.7142        1.2283\n",
      "     75        \u001b[36m0.5647\u001b[0m       0.7586        0.7157        1.2277\n",
      "     76        \u001b[36m0.5643\u001b[0m       0.7655        0.7745        1.2293\n",
      "     77        \u001b[36m0.5213\u001b[0m       0.7241        0.7893        1.2288\n",
      "     78        \u001b[36m0.5144\u001b[0m       0.7586        0.7565        1.2287\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.75, module__hidden_features=128;, score=-0.957 total time= 2.6min\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m5.3492\u001b[0m       \u001b[32m0.2292\u001b[0m        \u001b[35m1.5506\u001b[0m     +  1.2819\n",
      "      2        \u001b[36m2.7684\u001b[0m       \u001b[32m0.2778\u001b[0m        \u001b[35m1.5315\u001b[0m     +  1.2964\n",
      "      3        \u001b[36m1.8685\u001b[0m       \u001b[32m0.3819\u001b[0m        1.5605        1.2935\n",
      "      4        \u001b[36m1.6079\u001b[0m       0.2708        1.5793        1.2922\n",
      "      5        \u001b[36m1.5985\u001b[0m       0.2986        1.5735        1.2921\n",
      "      6        \u001b[36m1.5908\u001b[0m       0.3056        1.5580        1.2910\n",
      "      7        \u001b[36m1.5878\u001b[0m       0.3472        1.5580        1.3550\n",
      "      8        \u001b[36m1.5868\u001b[0m       0.3194        1.5505        1.3074\n",
      "      9        \u001b[36m1.5660\u001b[0m       0.3264        1.5432        1.2917\n",
      "     10        1.5746       0.3125        1.5460        1.2924\n",
      "     11        1.5661       0.3194        1.5462        1.2921\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 1/3] END module__dropout=0.75, module__hidden_features=256;, score=-1.580 total time=  23.3s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m5.3275\u001b[0m       \u001b[32m0.2414\u001b[0m        \u001b[35m1.6145\u001b[0m     +  1.2903\n",
      "      2        \u001b[36m2.7987\u001b[0m       \u001b[32m0.3448\u001b[0m        \u001b[35m1.5318\u001b[0m     +  1.2945\n",
      "      3        \u001b[36m2.0951\u001b[0m       0.2690        1.5625        1.2943\n",
      "      4        \u001b[36m1.6927\u001b[0m       0.2897        1.5788        1.2917\n",
      "      5        \u001b[36m1.6080\u001b[0m       0.2828        1.5868        1.2946\n",
      "      6        \u001b[36m1.6028\u001b[0m       0.3241        1.5750        1.2938\n",
      "      7        \u001b[36m1.5980\u001b[0m       0.2966        1.5748        1.2930\n",
      "      8        \u001b[36m1.5919\u001b[0m       0.2966        1.5787        1.2917\n",
      "      9        1.6159       0.2966        1.5854        1.2905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m1.5861\u001b[0m       0.3103        1.5667        1.2895\n",
      "     11        1.5994       0.2897        1.5751        1.2904\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 2/3] END module__dropout=0.75, module__hidden_features=256;, score=-1.565 total time=  23.1s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m5.7813\u001b[0m       \u001b[32m0.2966\u001b[0m        \u001b[35m1.5796\u001b[0m     +  1.2873\n",
      "      2        \u001b[36m3.7352\u001b[0m       0.2690        \u001b[35m1.5465\u001b[0m     +  1.2969\n",
      "      3        \u001b[36m2.2344\u001b[0m       \u001b[32m0.3379\u001b[0m        1.5712        1.2944\n",
      "      4        \u001b[36m1.7127\u001b[0m       \u001b[32m0.3448\u001b[0m        1.5541        1.2916\n",
      "      5        \u001b[36m1.6258\u001b[0m       \u001b[32m0.3655\u001b[0m        1.5595        1.2917\n",
      "      6        \u001b[36m1.6084\u001b[0m       0.3379        1.5797        1.2902\n",
      "      7        1.6099       0.3586        1.5633        1.2917\n",
      "      8        1.6140       0.2414        1.5899        1.2901\n",
      "      9        \u001b[36m1.5758\u001b[0m       0.3172        1.5723        1.2898\n",
      "     10        1.6033       0.2552        1.5943        1.2898\n",
      "     11        1.5911       0.2483        1.5829        1.2899\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "[CV 3/3] END module__dropout=0.75, module__hidden_features=256;, score=-1.567 total time=  23.2s\n",
      "==>> loaded pretrained_models/yolox-nano.pth, epoch 294\n",
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m2.2850\u001b[0m       \u001b[32m0.3180\u001b[0m        \u001b[35m1.5627\u001b[0m     +  1.7859\n",
      "      2        \u001b[36m1.7169\u001b[0m       \u001b[32m0.3410\u001b[0m        \u001b[35m1.4957\u001b[0m     +  1.7866\n",
      "      3        \u001b[36m1.6196\u001b[0m       \u001b[32m0.5069\u001b[0m        \u001b[35m1.4755\u001b[0m     +  1.7864\n",
      "      4        \u001b[36m1.4814\u001b[0m       0.3548        \u001b[35m1.4366\u001b[0m     +  1.7893\n",
      "      5        \u001b[36m1.4381\u001b[0m       0.4194        \u001b[35m1.4033\u001b[0m     +  1.7882\n",
      "      6        \u001b[36m1.4191\u001b[0m       0.4332        \u001b[35m1.3867\u001b[0m     +  1.7862\n",
      "      7        \u001b[36m1.4099\u001b[0m       \u001b[32m0.5161\u001b[0m        \u001b[35m1.3269\u001b[0m     +  1.7906\n",
      "      8        \u001b[36m1.3543\u001b[0m       \u001b[32m0.5576\u001b[0m        \u001b[35m1.3223\u001b[0m     +  1.7890\n",
      "      9        \u001b[36m1.3167\u001b[0m       0.5023        \u001b[35m1.2455\u001b[0m     +  1.7897\n",
      "     10        \u001b[36m1.3038\u001b[0m       \u001b[32m0.6221\u001b[0m        1.2493        1.7906\n",
      "     11        \u001b[36m1.2316\u001b[0m       0.5668        \u001b[35m1.1541\u001b[0m     +  1.7865\n",
      "     12        \u001b[36m1.1601\u001b[0m       0.5668        \u001b[35m1.1143\u001b[0m     +  1.7871\n",
      "     13        \u001b[36m1.1163\u001b[0m       0.5760        \u001b[35m1.0648\u001b[0m     +  1.7900\n",
      "     14        \u001b[36m1.0893\u001b[0m       \u001b[32m0.6728\u001b[0m        1.0716        1.7896\n",
      "     15        \u001b[36m1.0527\u001b[0m       0.6037        1.1490        1.7857\n",
      "     16        1.0613       \u001b[32m0.7189\u001b[0m        \u001b[35m1.0179\u001b[0m     +  1.7850\n",
      "     17        1.0658       0.6636        \u001b[35m0.9865\u001b[0m     +  1.7876\n",
      "     18        \u001b[36m0.9272\u001b[0m       0.6959        \u001b[35m0.9019\u001b[0m     +  1.7895\n",
      "     19        \u001b[36m0.8902\u001b[0m       0.7097        \u001b[35m0.8614\u001b[0m     +  1.7892\n",
      "     20        \u001b[36m0.8278\u001b[0m       \u001b[32m0.7512\u001b[0m        \u001b[35m0.8243\u001b[0m     +  1.7884\n",
      "     21        \u001b[36m0.8044\u001b[0m       \u001b[32m0.7604\u001b[0m        \u001b[35m0.7528\u001b[0m     +  1.7908\n",
      "     22        \u001b[36m0.6514\u001b[0m       0.7604        \u001b[35m0.7237\u001b[0m     +  1.7882\n",
      "     23        \u001b[36m0.5922\u001b[0m       \u001b[32m0.7696\u001b[0m        0.7415        1.7867\n",
      "     24        0.6213       0.7512        \u001b[35m0.7018\u001b[0m     +  1.7842\n",
      "     25        \u001b[36m0.5390\u001b[0m       \u001b[32m0.7742\u001b[0m        \u001b[35m0.6818\u001b[0m     +  1.7873\n",
      "     26        \u001b[36m0.4549\u001b[0m       \u001b[32m0.7926\u001b[0m        \u001b[35m0.6519\u001b[0m     +  1.7897\n",
      "     27        \u001b[36m0.4432\u001b[0m       0.7926        \u001b[35m0.6476\u001b[0m     +  1.7888\n",
      "     28        0.4545       0.7880        \u001b[35m0.6364\u001b[0m     +  1.7913\n",
      "     29        \u001b[36m0.4123\u001b[0m       0.7604        0.6584        1.7886\n",
      "     30        \u001b[36m0.3436\u001b[0m       0.7788        \u001b[35m0.6221\u001b[0m     +  1.7865\n",
      "     31        \u001b[36m0.2677\u001b[0m       0.7696        0.6926        1.7859\n",
      "     32        0.3406       0.7926        0.6579        1.7860\n",
      "     33        0.2774       0.7742        \u001b[35m0.6140\u001b[0m     +  1.7849\n",
      "     34        0.2683       0.7696        0.6929        1.7883\n",
      "     35        \u001b[36m0.2428\u001b[0m       0.7880        \u001b[35m0.6097\u001b[0m     +  1.7866\n",
      "     36        \u001b[36m0.2104\u001b[0m       0.7834        0.6285        1.7870\n",
      "     37        \u001b[36m0.1918\u001b[0m       0.7696        0.7018        1.7858\n",
      "     38        \u001b[36m0.1720\u001b[0m       0.7650        0.7035        1.7881\n",
      "     39        \u001b[36m0.1703\u001b[0m       0.7926        0.7584        1.7868\n",
      "     40        0.1734       0.7880        0.6939        1.7875\n",
      "     41        \u001b[36m0.1577\u001b[0m       0.7604        0.6861        1.7879\n",
      "     42        \u001b[36m0.1539\u001b[0m       0.7880        0.7108        1.7873\n",
      "     43        0.1729       0.7834        0.7781        1.7862\n",
      "     44        0.1617       0.7742        0.7209        1.7838\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<function generate_model at 0x7f9b2be614c0>,\n",
       "  module__dropout=0.25,\n",
       "  module__hidden_features=128,\n",
       "  module__opt={},\n",
       "),\n",
       "             param_grid={'module__dropout': [0.25, 0.5, 0.75],\n",
       "                         'module__hidden_features': [64, 128, 256]},\n",
       "             scoring='neg_log_loss', verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Uncomment this for tuning hyperparameter and saving the best model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Hyper parameter tuning with cross validation\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    max_epochs=15,\n",
    "    lr=0.001,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    "    optimizer=torch.optim.Adam\n",
    ")\n",
    "\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False,\n",
    "               # verbose=0\n",
    "               )\n",
    "# net.fit(inp_imgs[:20], torch.as_tensor(target[:20]))  # Make sure the model works on a small set of data\n",
    "params = {\n",
    "    'lr': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# params = {\n",
    "#     'optimizer': [torch.optim.SGD, torch.optim.Adam]\n",
    "# }\n",
    "gs = GridSearchCV(net, params,\n",
    "                  # refit=False,\n",
    "                  cv=3,\n",
    "                  scoring='neg_log_loss',\n",
    "                  verbose=3, error_score='raise')\n",
    "\n",
    "gs.fit(inp_imgs, torch.as_tensor(target))\n",
    "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "# Save best model to pickel file\n",
    "with open('yolox.pkl', 'wb') as f:\n",
    "    pickle.dump(gs, f)\n",
    "    \n",
    "    \n",
    "'''\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "params={'module__hidden_features': [64, 128, 256],\n",
    "       'module__dropout': [0.25,0.5,0.75]}\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    # model,\n",
    "    module=generate_model,\n",
    "    module__opt=opt,\n",
    "    module__hidden_features=128,\n",
    "    module__dropout=0.25,\n",
    "    max_epochs=100,\n",
    "    lr=3e-4,\n",
    "    device=opt.device,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=128,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[('checkpoint', Checkpoint()),\n",
    "               ('early_stopping', EarlyStopping(patience=10))]\n",
    ")\n",
    "# net.fit(inp_imgs, torch.as_tensor(target))\n",
    "\n",
    "gs = GridSearchCV(net, params,\n",
    "                  # refit=False,\n",
    "                  cv=3,\n",
    "                  scoring='neg_log_loss',\n",
    "                  verbose=3, error_score='raise')\n",
    "\n",
    "gs.fit(inp_imgs, torch.as_tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yolox.pkl', 'wb') as f:\n",
    "    pickle.dump(gs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from pickel file\n",
    "with open('yolox.pkl', 'rb') as f:\n",
    "    gs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module__dropout': 0.5, 'module__hidden_features': 64}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 4, 4, 4, 3, 2, 2, 3, 1, 1, 4, 4, 4, 0, 0, 3, 4, 2, 3, 4]),\n",
       " array([1, 4, 4, 1, 1, 2, 2, 3, 1, 1, 2, 4, 4, 0, 0, 3, 4, 2, 3, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(inp_imgs[:20]), target[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate both models on a withheld test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "# Predicting test \n",
    "test_path = \"dataset/test/\"\n",
    "y_pred = []\n",
    "y_test = []\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "                y_pred.append(baseline_model.predict([temp]))\n",
    "                y_test.append(labelencoder.transform([subdir.replace(path, '')])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the baseline model (SVM)\n",
    "baseline_report = classification_report(y_test, y_pred, target_names=labelencoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset/test/\"\n",
    "\n",
    "# Creating Dataset\n",
    "y_test = []\n",
    "images_arrays = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for img in files:\n",
    "        temp = []\n",
    "        img = os.path.join(subdir, img)\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "        img, r = preproc(img, opt.test_size, opt.rgb_means, opt.std)\n",
    "        images_arrays.append(img)\n",
    "        y_test.append(subdir.replace(path, ''))\n",
    "\n",
    "\n",
    "test_imgs = np.zeros([len(images_arrays), 3, opt.test_size[0], opt.test_size[1]], dtype=np.float32)\n",
    "for b_i, image in enumerate(images_arrays):\n",
    "    test_imgs[b_i] = image\n",
    "\n",
    "test_imgs = test_imgs\n",
    "y_test = labelencoder.fit_transform(y_test)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gs.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.94      0.80      0.87        97\n",
      "     goddess       0.85      0.59      0.70        80\n",
      "       plank       0.84      0.90      0.87       115\n",
      "        tree       0.81      0.94      0.87        69\n",
      "    warrior2       0.76      0.90      0.82       109\n",
      "\n",
      "    accuracy                           0.83       470\n",
      "   macro avg       0.84      0.83      0.82       470\n",
      "weighted avg       0.84      0.83      0.83       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the YOLOX model\n",
    "yolox_report = classification_report(y_test, y_pred, target_names=labelencoder.classes_)\n",
    "print(yolox_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Display results on the test set for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.99      0.97      0.98       196\n",
      "     goddess       0.93      0.84      0.88       164\n",
      "       plank       0.94      0.98      0.96       225\n",
      "        tree       0.97      0.85      0.91       136\n",
      "    warrior2       0.86      0.96      0.91       238\n",
      "\n",
      "    accuracy                           0.93       959\n",
      "   macro avg       0.94      0.92      0.93       959\n",
      "weighted avg       0.93      0.93      0.93       959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### II. YOLOX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     downdog       0.91      0.87      0.89        97\n",
      "     goddess       0.76      0.84      0.80        80\n",
      "       plank       0.89      0.85      0.87       115\n",
      "        tree       0.86      0.96      0.90        69\n",
      "    warrior2       0.90      0.85      0.88       109\n",
      "\n",
      "    accuracy                           0.87       470\n",
      "   macro avg       0.87      0.87      0.87       470\n",
      "weighted avg       0.87      0.87      0.87       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yolox_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
